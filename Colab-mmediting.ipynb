{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab-mmediting.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "6giPJBPbb4JU",
        "rfbkE2oVemqf",
        "0MJLf_sWeo7z",
        "U7CdL58mer_5",
        "bY9giPLf2nak"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5REUXZI7ejq6"
      },
      "source": [
        "# Colab-mmediting with [open-mmlab/mmediting](https://github.com/open-mmlab/mmediting)\n",
        "\n",
        "My fork is located in [styler00dollar/Colab-mmediting](https://github.com/styler00dollar/Colab-mmediting)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7SvGgIHiNCF",
        "cellView": "form"
      },
      "source": [
        "#@title Check GPU\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pq-pTiD6LHx3",
        "cellView": "form"
      },
      "source": [
        "#@title Install mmediting reqruirements (installing mmcv takes quite long)\n",
        "%cd /content/\n",
        "!git clone https://github.com/styler00dollar/Colab-mmediting\n",
        "%cd Colab-mmediting\n",
        "!pip install -r requirements.txt\n",
        "!pip install -v -e .  # or \"python setup.py develop\"\n",
        "!pip install tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6giPJBPbb4JU"
      },
      "source": [
        "# Test inpainting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5To_7PcbdNm",
        "cellView": "form"
      },
      "source": [
        "#@title Download all models\n",
        "%cd /content/\n",
        "!mkdir /content/models/\n",
        "# deepfillv2\n",
        "!wget --no-check-certificate 'https://openmmlab.oss-accelerate.aliyuncs.com/mmediting/inpainting/deepfillv2/deepfillv2_256x256_8x2_places_20200619-10d15793.pth' -O /content/models/deepfillv2_256x256_8x2_places.pth\n",
        "!wget --no-check-certificate 'https://openmmlab.oss-accelerate.aliyuncs.com/mmediting/inpainting/deepfillv2/deepfillv2_256x256_8x2_celeba_20200619-c96e5f12.pth' -O /content/models/deepfillv2_256x256_8x2_celeba.pth\n",
        "# deepfillv1\n",
        "!wget --no-check-certificate 'https://openmmlab.oss-accelerate.aliyuncs.com/mmediting/inpainting/deepfillv1/deepfillv1_256x256_4x4_celeba_20200619-dd51a855.pth' -O /content/models/deepfillv1_256x256_4x4_celeba.pth\n",
        "!wget --no-check-certificate 'https://openmmlab.oss-accelerate.aliyuncs.com/mmediting/inpainting/deepfillv1/deepfillv1_256x256_8x2_places_20200619-c00a0e21.pth' -O /content/models/deepfillv1_256x256_8x2_places.pth\n",
        "# global_local\n",
        "!wget --no-check-certificate 'https://openmmlab.oss-accelerate.aliyuncs.com/mmediting/inpainting/global_local/gl_256x256_8x12_celeba_20200619-5af0493f.pth' -O /content/models/gl_256x256_8x12_celeba.pth\n",
        "!wget --no-check-certificate 'https://openmmlab.oss-accelerate.aliyuncs.com/mmediting/inpainting/global_local/gl_256x256_8x12_places_20200619-52a040a8.pth' -O /content/models/gl_256x256_8x12_places.pth\n",
        "# partial_conv\n",
        "!wget --no-check-certificate 'https://openmmlab.oss-accelerate.aliyuncs.com/mmediting/inpainting/pconv/pconv_256x256_stage2_4x2_places_20200619-1ffed0e8.pth' -O /content/models/pconv_places.pth\n",
        "!wget --no-check-certificate 'https://openmmlab.oss-accelerate.aliyuncs.com/mmediting/inpainting/pconv/pconv_256x256_stage2_4x2_celeba_20200619-860f8b95.pth' -O /content/models/pconv_celeba.pth\n",
        "%cd /content/Colab-mmediting"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGA8wwgbcIvt"
      },
      "source": [
        "Place pictures in ```/content/image.png``` and ```/content/mask.png ```. Result will be in ```/content/result.png```.\n",
        "\n",
        "Info: The mask is a black white image and the masked area is white. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXP0lp7pg7Pv",
        "cellView": "form"
      },
      "source": [
        "#@title Connect Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "print('Google Drive connected.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfbkE2oVemqf"
      },
      "source": [
        "# deepfillv2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yLpR2sBOVA-",
        "cellView": "form"
      },
      "source": [
        "#@title deepfill v2 places\n",
        "!python demo/inpainting_demo.py configs/inpainting/deepfillv2/deepfillv2_256x256_8x2_places.py /content/models/deepfillv2_256x256_8x2_places.pth /content/image.png /content/mask.png /content/result.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKR7CROqc1lP",
        "cellView": "form"
      },
      "source": [
        "#@title deepfill v2 celeba\n",
        "!python demo/inpainting_demo.py configs/inpainting/deepfillv2/deepfillv2_256x256_8x2_celeba.py /content/models/deepfillv2_256x256_8x2_celeba.pth /content/image.png /content/mask.png /content/result.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MJLf_sWeo7z"
      },
      "source": [
        "# deepfillv1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCl0oMrGc_Sw",
        "cellView": "form"
      },
      "source": [
        "#@title deepfill v1 places\n",
        "!python demo/inpainting_demo.py configs/inpainting/deepfillv1/deepfillv1_256x256_8x2_places.py /content/models/deepfillv1_256x256_8x2_places.pth /content/image.png /content/mask.png /content/result.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMMd4hB6dIOs",
        "cellView": "form"
      },
      "source": [
        "#@title deepfill v1 celeba\n",
        "!python demo/inpainting_demo.py configs/inpainting/deepfillv1/deepfillv1_256x256_4x4_celeba.py /content/models/deepfillv1_256x256_4x4_celeba.pth /content/image.png /content/mask.png /content/result.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7CdL58mer_5"
      },
      "source": [
        "# global_local"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBpo1ZQUdPCv",
        "cellView": "form"
      },
      "source": [
        "#@title global_local places\n",
        "!python demo/inpainting_demo.py configs/inpainting/global_local/gl_256x256_8x12_places.py /content/models/gl_256x256_8x12_places.pth /content/image.png /content/mask.png /content/result.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrhWrbA8dXQl",
        "cellView": "form"
      },
      "source": [
        "#@title global_local celeba\n",
        "!python demo/inpainting_demo.py configs/inpainting/global_local/gl_256x256_8x12_celeba.py /content/models/gl_256x256_8x12_celeba.pth /content/image.png /content/mask.png /content/result.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3YN4oS27F9v"
      },
      "source": [
        "# partial_conv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4X_FZSaFeu8Q"
      },
      "source": [
        "partial_conv (Does not accept any image dimension as input. ```250x250, 256x256, 500x500, 501x501, 502x502, 503x503, 504x504, 505x505, 506x506, 507x507, 508x508, 509x509, 510x510, 511x511, 512x512, 624x624, 750x750, 1000x1000, 1001x1001, 1000x1250, 1250x1250, 1256x1256 and 1512x1512``` seem to work. ```180x180, 192x192, 201x201, 325x325, 400x400, 425x425, 444x444, 480x480, 513x513, 514x514, 515x515, 516x516, 517x517, 518x518, 520x520, 524x524, 555x555, 569x569,  666x666, 724x724, 800x800, 812x812, 1500x1500 and 2000x2000``` does not work. I can't really see a pattern.)\n",
        "\n",
        "[Problematic code is here.](https://github.com/open-mmlab/mmediting/blob/master/mmedit/models/backbones/encoder_decoders/decoders/pconv_decoder.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJWS5f7RdwJK"
      },
      "source": [
        "# resize input if needed\n",
        "import cv2\n",
        "filepath = '/content/image.png'\n",
        "size = (666,666)\n",
        "\n",
        "image = cv2.imread(filepath)\n",
        "image = cv2.resize(image, size, cv2.INTER_NEAREST)\n",
        "cv2.imwrite(filepath, image)\n",
        "filepath = '/content/mask.png'\n",
        "image = cv2.imread(filepath)\n",
        "image = cv2.resize(image, size, cv2.INTER_NEAREST)\n",
        "cv2.imwrite(filepath, image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2dCAUN9dipe",
        "cellView": "form"
      },
      "source": [
        "#@title partial_conv 8x1 places\n",
        "!python demo/inpainting_demo.py configs/inpainting/partial_conv/pconv_256x256_stage1_8x1_places.py /content/models/pconv_places.pth /content/image.png /content/mask.png /content/result.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYUx3xlFdepC",
        "cellView": "form"
      },
      "source": [
        "#@title partial_conv 8x1 celeba\n",
        "!python demo/inpainting_demo.py configs/inpainting/partial_conv/pconv_256x256_stage1_8x1_celeba.py /content/models/pconv_celeba.pth /content/image.png /content/mask.png /content/result.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJ2995x-e4pc",
        "cellView": "form"
      },
      "source": [
        "#@title partial_conv 4x2 places\n",
        "!python demo/inpainting_demo.py configs/inpainting/partial_conv/pconv_256x256_stage2_4x2_places.py /content/models/pconv_places.pth /content/image.png /content/mask.png /content/result.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TglLOv2Me-OC",
        "cellView": "form"
      },
      "source": [
        "#@title partial_conv 4x2 celeba\n",
        "!python demo/inpainting_demo.py configs/inpainting/partial_conv/pconv_256x256_stage2_4x2_celeba.py /content/models/pconv_celeba.pth /content/image.png /content/mask.png /content/result.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P50PwChJxYsv"
      },
      "source": [
        "-------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY9giPLf2nak"
      },
      "source": [
        "# Training deepfillv2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLWkPZpTvg_8",
        "cellView": "form"
      },
      "source": [
        "#@title create empty folders\n",
        "!mkdir /content/train/\n",
        "!mkdir /content/val/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Il9CqQQ2mun",
        "cellView": "form"
      },
      "source": [
        "#@title custom.py (contains configuration for paths and parameter)\n",
        "%%writefile /content/Colab-mmediting/configs/inpainting/deepfillv2/custom.py\n",
        "model = dict(\n",
        "    type='TwoStageInpaintor',\n",
        "    disc_input_with_mask=True,\n",
        "    encdec=dict(\n",
        "        type='DeepFillEncoderDecoder',\n",
        "        stage1=dict(\n",
        "            type='GLEncoderDecoder',\n",
        "            encoder=dict(\n",
        "                type='DeepFillEncoder',\n",
        "                conv_type='gated_conv',\n",
        "                channel_factor=0.75,\n",
        "                padding_mode='reflect'),\n",
        "            decoder=dict(\n",
        "                type='DeepFillDecoder',\n",
        "                conv_type='gated_conv',\n",
        "                in_channels=96,\n",
        "                channel_factor=0.75,\n",
        "                out_act_cfg=dict(type='Tanh'),\n",
        "                padding_mode='reflect'),\n",
        "            dilation_neck=dict(\n",
        "                type='GLDilationNeck',\n",
        "                in_channels=96,\n",
        "                conv_type='gated_conv',\n",
        "                act_cfg=dict(type='ELU'),\n",
        "                padding_mode='reflect')),\n",
        "        stage2=dict(\n",
        "            type='DeepFillRefiner',\n",
        "            encoder_attention=dict(\n",
        "                type='DeepFillEncoder',\n",
        "                encoder_type='stage2_attention',\n",
        "                conv_type='gated_conv',\n",
        "                channel_factor=0.75,\n",
        "                padding_mode='reflect'),\n",
        "            encoder_conv=dict(\n",
        "                type='DeepFillEncoder',\n",
        "                encoder_type='stage2_conv',\n",
        "                conv_type='gated_conv',\n",
        "                channel_factor=0.75,\n",
        "                padding_mode='reflect'),\n",
        "            dilation_neck=dict(\n",
        "                type='GLDilationNeck',\n",
        "                in_channels=96,\n",
        "                conv_type='gated_conv',\n",
        "                act_cfg=dict(type='ELU'),\n",
        "                padding_mode='reflect'),\n",
        "            contextual_attention=dict(\n",
        "                type='ContextualAttentionNeck',\n",
        "                in_channels=96,\n",
        "                conv_type='gated_conv',\n",
        "                padding_mode='reflect'),\n",
        "            decoder=dict(\n",
        "                type='DeepFillDecoder',\n",
        "                in_channels=192,\n",
        "                conv_type='gated_conv',\n",
        "                out_act_cfg=dict(type='Tanh'),\n",
        "                padding_mode='reflect'))),\n",
        "    disc=dict(\n",
        "        type='MultiLayerDiscriminator',\n",
        "        in_channels=4,\n",
        "        max_channels=256,\n",
        "        fc_in_channels=None,\n",
        "        num_convs=6,\n",
        "        norm_cfg=None,\n",
        "        act_cfg=dict(type='LeakyReLU', negative_slope=0.2),\n",
        "        out_act_cfg=dict(type='LeakyReLU', negative_slope=0.2),\n",
        "        with_spectral_norm=True,\n",
        "    ),\n",
        "    stage1_loss_type=('loss_l1_hole', 'loss_l1_valid'),\n",
        "    stage2_loss_type=('loss_l1_hole', 'loss_l1_valid', 'loss_gan'),\n",
        "    loss_gan=dict(\n",
        "        type='GANLoss',\n",
        "        gan_type='hinge',\n",
        "        loss_weight=0.1,\n",
        "    ),\n",
        "    loss_l1_hole=dict(\n",
        "        type='L1Loss',\n",
        "        loss_weight=1.0,\n",
        "    ),\n",
        "    loss_l1_valid=dict(\n",
        "        type='L1Loss',\n",
        "        loss_weight=1.0,\n",
        "    ),\n",
        "    pretrained=None)\n",
        "\n",
        "train_cfg = dict(disc_step=1)\n",
        "test_cfg = dict(metrics=['l1', 'psnr', 'ssim'])\n",
        "\n",
        "dataset_type = 'ImgInpaintingDataset'\n",
        "input_shape = (256, 256)\n",
        "\n",
        "train_pipeline = [\n",
        "    dict(type='LoadImageFromFile', key='gt_img'),\n",
        "    dict(\n",
        "        type='LoadMask',\n",
        "        mask_mode='irregular',\n",
        "        mask_config=dict(\n",
        "            num_vertexes=(4, 10),\n",
        "            max_angle=6.0,\n",
        "            length_range=(20, 128),\n",
        "            brush_width=(10, 45),\n",
        "            area_ratio_range=(0.15, 0.65),\n",
        "            img_shape=input_shape)),\n",
        "    dict(\n",
        "        type='Crop',\n",
        "        keys=['gt_img'],\n",
        "        crop_size=(384, 384),\n",
        "        random_crop=True,\n",
        "    ),\n",
        "    dict(\n",
        "        type='Resize',\n",
        "        keys=['gt_img'],\n",
        "        scale=input_shape,\n",
        "        keep_ratio=False,\n",
        "    ),\n",
        "    dict(\n",
        "        type='Normalize',\n",
        "        keys=['gt_img'],\n",
        "        mean=[127.5] * 3,\n",
        "        std=[127.5] * 3,\n",
        "        to_rgb=False),\n",
        "    dict(type='GetMaskedImage'),\n",
        "    dict(\n",
        "        type='Collect',\n",
        "        keys=['gt_img', 'masked_img', 'mask'],\n",
        "        meta_keys=['gt_img_path']),\n",
        "    dict(type='ImageToTensor', keys=['gt_img', 'masked_img', 'mask'])\n",
        "]\n",
        "\n",
        "test_pipeline = train_pipeline\n",
        "\n",
        "data_root = '/content/data'\n",
        "\n",
        "data = dict(\n",
        "    samples_per_gpu=2,\n",
        "    workers_per_gpu=8,\n",
        "    val_samples_per_gpu=1,\n",
        "    val_workers_per_gpu=8,\n",
        "    drop_last=True,\n",
        "    train=dict(\n",
        "        type=dataset_type,\n",
        "        ann_file='/content/train/train.tflist',\n",
        "        data_prefix=data_root,\n",
        "        pipeline=train_pipeline,\n",
        "        test_mode=False),\n",
        "    val=dict(\n",
        "        type=dataset_type,\n",
        "        ann_file='/content/val/val.tflist',\n",
        "        data_prefix=data_root,\n",
        "        pipeline=test_pipeline,\n",
        "        test_mode=True),\n",
        "    test=dict(\n",
        "        type=dataset_type,\n",
        "        ann_file='/content/val/val.tflist',\n",
        "        data_prefix=data_root,\n",
        "        pipeline=test_pipeline,\n",
        "        test_mode=True))\n",
        "\n",
        "optimizers = dict(\n",
        "    generator=dict(type='Adam', lr=0.0001), disc=dict(type='Adam', lr=0.0001))\n",
        "\n",
        "lr_config = dict(policy='Fixed', by_epoch=False)\n",
        "\n",
        "checkpoint_config = dict(by_epoch=False, interval=1000)\n",
        "log_config = dict(\n",
        "    interval=100,\n",
        "    hooks=[\n",
        "        dict(type='TextLoggerHook', by_epoch=False),\n",
        "        dict(type='TensorboardLoggerHook'),\n",
        "        #dict(type='PaviLoggerHook', init_kwargs=dict(project='mmedit'))\n",
        "    ])\n",
        "\n",
        "visual_config = dict(\n",
        "    type='VisualizationHook',\n",
        "    output_dir='visual',\n",
        "    interval=1000,\n",
        "    res_name_list=[\n",
        "        'gt_img', 'masked_img', 'stage1_fake_res', 'stage1_fake_img',\n",
        "        'stage2_fake_res', 'stage2_fake_img', 'fake_gt_local'\n",
        "    ],\n",
        ")\n",
        "\n",
        "evaluation = dict(interval=50000)\n",
        "\n",
        "total_iters = 1000003\n",
        "dist_params = dict(backend='nccl')\n",
        "log_level = 'INFO'\n",
        "work_dir = './work_dirs/test_pggan'\n",
        "load_from = None\n",
        "resume_from = None\n",
        "workflow = [('train', 10000)]\n",
        "exp_name = 'deepfillv2_256x256_8x2_places'\n",
        "find_unused_parameters = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQogZKR72tWD",
        "cellView": "form"
      },
      "source": [
        "#@title Train (works on pytorch 1.6)\n",
        "%cd /content/Colab-mmediting\n",
        "!python tools/train.py configs/inpainting/deepfillv2/custom.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BC1jaAYRrQ_"
      },
      "source": [
        "# Training deepfillv2 with Differentiable Augmentation and/or Mosaic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFxQzJvxwM3s",
        "cellView": "form"
      },
      "source": [
        "#@title create empty folders\n",
        "!mkdir /content/train/\n",
        "!mkdir /content/val/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QGZQbnGl0Zc"
      },
      "source": [
        "--------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX1GN-VgpeGB"
      },
      "source": [
        "[Experimental] Custom loss functions for deepfill v2: HFENLoss (high frequency error norm), ElasticLoss, RelativeL1, L1CosineSim, ClipL1, FFTloss, OFLoss (Overflow loss), GPLoss (Gradient Profile (GP) loss), CPLoss (Color Profile (CP) loss) and Contextual_Loss. Currently no weight value in config and can be added in the ```two_stage.py``` file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwuKxWB4h-sb"
      },
      "source": [
        "Inpainting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLc8bnP6v4-s",
        "cellView": "form"
      },
      "source": [
        "#@title two_stage.py (diffaug + new loss)\n",
        "%%writefile /content/Colab-mmediting/mmedit/models/inpaintors/two_stage.py\n",
        "\n",
        "from .vic.loss import CharbonnierLoss, GANLoss, GradientPenaltyLoss, HFENLoss, TVLoss, GradientLoss, ElasticLoss, RelativeL1, L1CosineSim, ClipL1, MaskedL1Loss, MultiscalePixelLoss, FFTloss, OFLoss, L1_regularization, ColorLoss, AverageLoss, GPLoss, CPLoss, SPL_ComputeWithTrace, SPLoss, Contextual_Loss\n",
        "from .vic.filters import *\n",
        "from .vic.colors import *\n",
        "from .vic.discriminators import *\n",
        "from .diffaug import *\n",
        "\n",
        "import os.path as osp\n",
        "from pathlib import Path\n",
        "\n",
        "import mmcv\n",
        "import torch\n",
        "from mmedit.core import tensor2img\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from ..common.model_utils import set_requires_grad\n",
        "from ..registry import MODELS\n",
        "from .one_stage import OneStageInpaintor\n",
        "\n",
        "#from DiffAugment_pytorch import DiffAugment\n",
        "\n",
        "@MODELS.register_module()\n",
        "class TwoStageInpaintor(OneStageInpaintor):\n",
        "    \"\"\"Two-Stage Inpaintor.\n",
        "\n",
        "    Currently, we support these loss types in each of two stage inpaintors:\n",
        "    ['loss_gan', 'loss_l1_hole', 'loss_l1_valid', 'loss_composed_percep',\\\n",
        "     'loss_out_percep', 'loss_tv']\n",
        "    The `stage1_loss_type` and `stage2_loss_type` should be chosen from these\n",
        "    loss types.\n",
        "\n",
        "    Args:\n",
        "        stage1_loss_type (tuple[str]): Contains the loss names used in the\n",
        "            first stage model.\n",
        "        stage2_loss_type (tuple[str]): Contains the loss names used in the\n",
        "            second stage model.\n",
        "        input_with_ones (bool): Whether to concatenate an extra ones tensor in\n",
        "            input. Default: True.\n",
        "        disc_input_with_mask (bool): Whether to add mask as input in\n",
        "            discriminator. Default: False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 *args,\n",
        "                 stage1_loss_type=('loss_l1_hole', ),\n",
        "                 stage2_loss_type=('loss_l1_hole', 'loss_gan'),\n",
        "                 input_with_ones=True,\n",
        "                 disc_input_with_mask=False,\n",
        "                 **kwargs):\n",
        "        super(TwoStageInpaintor, self).__init__(*args, **kwargs)\n",
        "\n",
        "        self.stage1_loss_type = stage1_loss_type\n",
        "        self.stage2_loss_type = stage2_loss_type\n",
        "        self.input_with_ones = input_with_ones\n",
        "        self.disc_input_with_mask = disc_input_with_mask\n",
        "        self.eval_with_metrics = ('metrics' in self.test_cfg) and (\n",
        "            self.test_cfg['metrics'] is not None)\n",
        "\n",
        "        # new loss\n",
        "        \n",
        "        # #l_hfen_type = CharbonnierLoss() # nn.L1Loss(), nn.MSELoss(), CharbonnierLoss(), ElasticLoss(), RelativeL1(), L1CosineSim()\n",
        "        l_hfen_type = CharbonnierLoss()\n",
        "        self.HFENLoss = HFENLoss(loss_f=l_hfen_type, kernel='log', kernel_size=15, sigma = 2.5, norm = False)\n",
        "\n",
        "        self.ElasticLoss = ElasticLoss(a=0.2, reduction='mean')\n",
        "\n",
        "        self.RelativeL1 = RelativeL1(eps=.01, reduction='mean')\n",
        "\n",
        "        self.L1CosineSim = L1CosineSim(loss_lambda=5, reduction='mean')\n",
        "\n",
        "        self.ClipL1 = ClipL1(clip_min=0.0, clip_max=10.0)\n",
        "\n",
        "        self.FFTloss = FFTloss(loss_f = torch.nn.L1Loss, reduction='mean')\n",
        "\n",
        "        self.OFLoss = OFLoss()\n",
        "\n",
        "        self.GPLoss = GPLoss(trace=False, spl_denorm=False)\n",
        "\n",
        "        self.CPLoss = CPLoss(rgb=True, yuv=True, yuvgrad=True, trace=False, spl_denorm=False, yuv_denorm=False)\n",
        "\n",
        "        layers_weights = {'conv_1_1': 1.0, 'conv_3_2': 1.0}\n",
        "        self.Contextual_Loss = Contextual_Loss(layers_weights, crop_quarter=False, max_1d_size=100, \n",
        "            distance_type = 'cosine', b=1.0, band_width=0.5, \n",
        "            use_vgg = True, net = 'vgg19', calc_type = 'regular')\n",
        "\n",
        "\n",
        "\n",
        "    def forward_test(self,\n",
        "                     masked_img,\n",
        "                     mask,\n",
        "                     save_image=False,\n",
        "                     save_path=None,\n",
        "                     iteration=None,\n",
        "                     **kwargs):\n",
        "        \"\"\"Forward function for testing.\n",
        "\n",
        "        Args:\n",
        "            masked_img (torch.Tensor): Tensor with shape of (n, 3, h, w).\n",
        "            mask (torch.Tensor): Tensor with shape of (n, 1, h, w).\n",
        "            save_image (bool, optional): If True, results will be saved as\n",
        "                image. Defaults to False.\n",
        "            save_path (str, optional): If given a valid str, the reuslts will\n",
        "                be saved in this path. Defaults to None.\n",
        "            iteration (int, optional): Iteration number. Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            dict: Contain output results and eval metrics (if have).\n",
        "        \"\"\"\n",
        "        if self.input_with_ones:\n",
        "            tmp_ones = torch.ones_like(mask)\n",
        "            input_x = torch.cat([masked_img, tmp_ones, mask], dim=1)\n",
        "        else:\n",
        "            input_x = torch.cat([masked_img, mask], dim=1)\n",
        "        stage1_fake_res, stage2_fake_res = self.generator(input_x)\n",
        "        fake_img = stage2_fake_res * mask + masked_img * (1. - mask)\n",
        "        output = dict()\n",
        "        eval_results = {}\n",
        "        if self.eval_with_metrics:\n",
        "            gt_img = kwargs['gt_img']\n",
        "            data_dict = dict(\n",
        "                gt_img=gt_img, fake_res=stage2_fake_res, mask=mask)\n",
        "            for metric_name in self.test_cfg['metrics']:\n",
        "                if metric_name in ['ssim', 'psnr']:\n",
        "                    eval_results[metric_name] = self._eval_metrics[\n",
        "                        metric_name](tensor2img(fake_img, min_max=(-1, 1)),\n",
        "                                     tensor2img(gt_img, min_max=(-1, 1)))\n",
        "                else:\n",
        "                    eval_results[metric_name] = self._eval_metrics[\n",
        "                        metric_name]()(data_dict).item()\n",
        "            output['eval_results'] = eval_results\n",
        "        else:\n",
        "            output['stage1_fake_res'] = stage1_fake_res\n",
        "            output['stage2_fake_res'] = stage2_fake_res\n",
        "            output['fake_res'] = stage2_fake_res\n",
        "            output['fake_img'] = fake_img\n",
        "\n",
        "        output['meta'] = None if 'meta' not in kwargs else kwargs['meta'][0]\n",
        "\n",
        "        if save_image:\n",
        "            assert save_image and save_path is not None, (\n",
        "                'Save path should be given')\n",
        "            assert output['meta'] is not None, (\n",
        "                'Meta information should be given to save image.')\n",
        "\n",
        "            tmp_filename = output['meta']['gt_img_path']\n",
        "            filestem = Path(tmp_filename).stem\n",
        "            if iteration is not None:\n",
        "                filename = f'{filestem}_{iteration}.png'\n",
        "            else:\n",
        "                filename = f'{filestem}.png'\n",
        "            mmcv.mkdir_or_exist(save_path)\n",
        "            img_list = [kwargs['gt_img']] if 'gt_img' in kwargs else []\n",
        "            img_list.extend([\n",
        "                masked_img,\n",
        "                mask.expand_as(masked_img), stage1_fake_res, stage2_fake_res,\n",
        "                fake_img\n",
        "            ])\n",
        "            img = torch.cat(img_list, dim=3).cpu()\n",
        "            self.save_visualization(img, osp.join(save_path, filename))\n",
        "            output['save_img_path'] = osp.abspath(\n",
        "                osp.join(save_path, filename))\n",
        "\n",
        "        return output\n",
        "\n",
        "    def save_visualization(self, img, filename):\n",
        "        \"\"\"Save visualization results.\n",
        "\n",
        "        Args:\n",
        "            img (torch.Tensor): Tensor with shape of (n, 3, h, w).\n",
        "            filename (str): Path to save visualization.\n",
        "        \"\"\"\n",
        "        if self.test_cfg.get('img_rerange', True):\n",
        "            img = (img + 1) / 2\n",
        "        if self.test_cfg.get('img_bgr2rgb', True):\n",
        "            img = img[:, [2, 1, 0], ...]\n",
        "        save_image(img, filename, nrow=1, padding=0)\n",
        "\n",
        "    def two_stage_loss(self, stage1_data, stage2_data, data_batch):\n",
        "        \"\"\"Calculate two-stage loss.\n",
        "\n",
        "        Args:\n",
        "            stage1_data (dict): Contain stage1 results.\n",
        "            stage2_data (dict): Contain stage2 results.\n",
        "            data_batch (dict): Contain data needed to calculate loss.\n",
        "\n",
        "        Returns:\n",
        "            dict: Contain losses with name.\n",
        "        \"\"\"\n",
        "        gt = data_batch['gt_img']\n",
        "        mask = data_batch['mask']\n",
        "        masked_img = data_batch['masked_img']\n",
        "\n",
        "        loss = dict()\n",
        "        results = dict(\n",
        "            gt_img=gt.cpu(), mask=mask.cpu(), masked_img=masked_img.cpu())\n",
        "        # calculate losses for stage1\n",
        "        if self.stage1_loss_type is not None:\n",
        "            fake_res = stage1_data['fake_res']\n",
        "            fake_img = stage1_data['fake_img']\n",
        "            for type_key in self.stage1_loss_type:\n",
        "                tmp_loss = self.calculate_loss_with_type(\n",
        "                    type_key, fake_res, fake_img, gt, mask, prefix='stage1_')\n",
        "                loss.update(tmp_loss)\n",
        "\n",
        "        results.update(\n",
        "            dict(\n",
        "                stage1_fake_res=stage1_data['fake_res'].cpu(),\n",
        "                stage1_fake_img=stage1_data['fake_img'].cpu()))\n",
        "\n",
        "        if self.stage2_loss_type is not None:\n",
        "            fake_res = stage2_data['fake_res']\n",
        "            fake_img = stage2_data['fake_img']\n",
        "            for type_key in self.stage2_loss_type:\n",
        "                tmp_loss = self.calculate_loss_with_type(\n",
        "                    type_key, fake_res, fake_img, gt, mask, prefix='stage2_')\n",
        "                loss.update(tmp_loss)\n",
        "        results.update(\n",
        "            dict(\n",
        "                stage2_fake_res=stage2_data['fake_res'].cpu(),\n",
        "                stage2_fake_img=stage2_data['fake_img'].cpu()))\n",
        "\n",
        "        return results, loss\n",
        "\n",
        "    def calculate_loss_with_type(self,\n",
        "                                 loss_type,\n",
        "                                 fake_res,\n",
        "                                 fake_img,\n",
        "                                 gt,\n",
        "                                 mask,\n",
        "                                 prefix='stage1_'):\n",
        "        \"\"\"Calculate multiple types of losses.\n",
        "\n",
        "        Args:\n",
        "            loss_type (str): Type of the loss.\n",
        "            fake_res (torch.Tensor): Direct results from model.\n",
        "            fake_img (torch.Tensor): Composited results from model.\n",
        "            gt (torch.Tensor): Ground-truth tensor.\n",
        "            mask (torch.Tensor): Mask tensor.\n",
        "            prefix (str, optional): Prefix for loss name.\n",
        "                Defaults to 'stage1_'.\n",
        "\n",
        "        Returns:\n",
        "            dict: Contain loss value with its name.\n",
        "        \"\"\"\n",
        "        loss_dict = dict()\n",
        "        if loss_type == 'loss_gan':\n",
        "            if self.disc_input_with_mask:\n",
        "                disc_input_x = torch.cat([fake_img, mask], dim=1)\n",
        "            else:\n",
        "                disc_input_x = fake_img\n",
        "            g_fake_pred = self.disc(disc_input_x)\n",
        "            #############################################################\n",
        "            #loss_g_fake = self.loss_gan(g_fake_pred, True, is_disc=False)\n",
        "            #loss_g_fake = (DiffAugment(g_fake_pred, policy=policy)) #DiffAug\n",
        "\n",
        "            #alternativ:\n",
        "            g_fake_pred = DiffAugment(g_fake_pred, policy=policy)\n",
        "            loss_g_fake = self.loss_gan(g_fake_pred, True, is_disc=False)\n",
        "            ##############################################################\n",
        "            loss_dict[prefix + 'loss_g_fake'] = loss_g_fake\n",
        "        elif 'percep' in loss_type:\n",
        "            loss_pecep, loss_style = self.loss_percep(fake_img, gt)\n",
        "            if loss_pecep is not None:\n",
        "                loss_dict[prefix + loss_type] = loss_pecep\n",
        "            if loss_style is not None:\n",
        "                loss_dict[prefix + loss_type[:-6] + 'style'] = loss_style\n",
        "        elif 'tv' in loss_type:\n",
        "            loss_tv = self.loss_tv(fake_img, mask=mask)\n",
        "            loss_dict[prefix + loss_type] = loss_tv\n",
        "        elif 'l1' in loss_type:\n",
        "            weight = 1. - mask if 'valid' in loss_type else mask\n",
        "            loss_l1 = getattr(self, loss_type)(fake_res, gt, weight=weight)\n",
        "            loss_dict[prefix + loss_type] = loss_l1\n",
        "        # new\n",
        "        elif 'HFEN' in loss_type:\n",
        "            loss_hfen = self.HFENLoss(fake_img, gt)\n",
        "            loss_dict[prefix + loss_type] = loss_hfen\n",
        "        elif 'Elastic' in loss_type:\n",
        "            loss_elastic = self.ElasticLoss(fake_img, gt)\n",
        "            loss_dict[prefix + loss_type] = loss_elastic\n",
        "        elif 'RelativeL1' in loss_type:\n",
        "            loss_relativel1 = self.RelativeL1(fake_img, gt)\n",
        "            loss_dict[prefix + loss_type] = loss_relativel1\n",
        "        elif 'L1CosineSim' in loss_type:\n",
        "            loss_l1cosinesim = self.L1CosineSim(fake_img, gt)\n",
        "            loss_dict[prefix + loss_type] = loss_l1cosinesim\n",
        "        elif 'ClipL1' in loss_type:\n",
        "            loss_clipl1 = self.ClipL1(fake_img, gt)\n",
        "            loss_dict[prefix + loss_type] = loss_clipl1\n",
        "        elif 'FFT' in loss_type:\n",
        "            loss_fft = self.FFTloss(fake_img, gt)\n",
        "            loss_dict[prefix + loss_type] = loss_fft\n",
        "        elif 'OF' in loss_type:\n",
        "            loss_of = self.OFloss(fake_img)\n",
        "            loss_dict[prefix + loss_type] = loss_of\n",
        "        elif 'GP' in loss_type:\n",
        "            loss_gp = self.GPloss(fake_img, gt)\n",
        "            loss_dict[prefix + loss_type] = loss_gp\n",
        "        elif 'CP' in loss_type:\n",
        "            loss_cp = self.CPloss(fake_img, gt)\n",
        "            loss_dict[prefix + loss_type] = loss_cp\n",
        "        else:\n",
        "            raise NotImplementedError(\n",
        "                f'Please check your loss type {loss_type}'\n",
        "                f' and the config dict in init function. '\n",
        "                f'We cannot find the related loss function.')\n",
        "\n",
        "        return loss_dict\n",
        "\n",
        "    def train_step(self, data_batch, optimizer):\n",
        "        \"\"\"Train step function.\n",
        "\n",
        "        In this function, the inpaintor will finish the train step following\n",
        "        the pipeline:\n",
        "\n",
        "            1. get fake res/image\n",
        "            2. optimize discriminator (if have)\n",
        "            3. optimize generator\n",
        "\n",
        "        If `self.train_cfg.disc_step > 1`, the train step will contain multiple\n",
        "        iterations for optimizing discriminator with different input data and\n",
        "        only one iteration for optimizing gerator after `disc_step` iterations\n",
        "        for discriminator.\n",
        "\n",
        "        Args:\n",
        "            data_batch (torch.Tensor): Batch of data as input.\n",
        "            optimizer (dict[torch.optim.Optimizer]): Dict with optimizers for\n",
        "                generator and discriminator (if have).\n",
        "\n",
        "        Returns:\n",
        "            dict: Dict with loss, information for logger, the number of \\\n",
        "                samples and results for visualization.\n",
        "        \"\"\"\n",
        "        log_vars = {}\n",
        "\n",
        "        gt_img = data_batch['gt_img']\n",
        "        mask = data_batch['mask']\n",
        "        masked_img = data_batch['masked_img']\n",
        "\n",
        "        # get common output from encdec\n",
        "        if self.input_with_ones:\n",
        "            tmp_ones = torch.ones_like(mask)\n",
        "            input_x = torch.cat([masked_img, tmp_ones, mask], dim=1)\n",
        "        else:\n",
        "            input_x = torch.cat([masked_img, mask], dim=1)\n",
        "        stage1_fake_res, stage2_fake_res = self.generator(input_x)\n",
        "        stage1_fake_img = masked_img * (1. - mask) + stage1_fake_res * mask\n",
        "        stage2_fake_img = masked_img * (1. - mask) + stage2_fake_res * mask\n",
        "\n",
        "        # discriminator training step\n",
        "        # In this version, we only use the results from the second stage to\n",
        "        # train discriminators, which is a commonly used setting. This can be\n",
        "        # easily modified to your custom training schedule.\n",
        "        if self.train_cfg.disc_step > 0:\n",
        "            set_requires_grad(self.disc, True)\n",
        "            if self.disc_input_with_mask:\n",
        "                disc_input_x = torch.cat([stage2_fake_img.detach(), mask],\n",
        "                                         dim=1)\n",
        "            else:\n",
        "                disc_input_x = stage2_fake_img.detach()\n",
        "            disc_losses = self.forward_train_d(disc_input_x, False, is_disc=True)\n",
        "            loss_disc, log_vars_d = self.parse_losses(disc_losses)\n",
        "            log_vars.update(log_vars_d)\n",
        "            optimizer['disc'].zero_grad()\n",
        "            loss_disc.backward()\n",
        "\n",
        "            if self.disc_input_with_mask:\n",
        "                disc_input_x = torch.cat([gt_img, mask], dim=1)\n",
        "            else:\n",
        "                disc_input_x = gt_img\n",
        "            disc_losses = self.forward_train_d(disc_input_x, True, is_disc=True)\n",
        "            loss_disc, log_vars_d = self.parse_losses(disc_losses)\n",
        "            log_vars.update(log_vars_d)\n",
        "            loss_disc.backward()\n",
        "\n",
        "            if self.with_gp_loss:\n",
        "                # gradient penalty loss should not be used with mask as input\n",
        "                assert not self.disc_input_with_mask\n",
        "                loss_d_gp = self.loss_gp(self.disc, gt_img, stage2_fake_img, mask=mask)\n",
        "                loss_disc, log_vars_d = self.parse_losses(dict(loss_gp=loss_d_gp))\n",
        "                log_vars.update(log_vars_d)\n",
        "                loss_disc.backward()\n",
        "\n",
        "            optimizer['disc'].step()\n",
        "\n",
        "            self.disc_step_count = (self.disc_step_count +\n",
        "                                    1) % self.train_cfg.disc_step\n",
        "            if self.disc_step_count != 0:\n",
        "                # results contain the data for visualization\n",
        "                results = dict(\n",
        "                    gt_img=gt_img.cpu(),\n",
        "                    masked_img=masked_img.cpu(),\n",
        "                    fake_res=stage2_fake_res.cpu(),\n",
        "                    fake_img=stage2_fake_img.cpu())\n",
        "                outputs = dict(\n",
        "                    log_vars=log_vars,\n",
        "                    num_samples=len(data_batch['gt_img'].data),\n",
        "                    results=results)\n",
        "\n",
        "                return outputs\n",
        "\n",
        "        # prepare stage1 results and stage2 results dict for calculating losses\n",
        "        stage1_results = dict(\n",
        "            fake_res=stage1_fake_res, fake_img=stage1_fake_img)\n",
        "        stage2_results = dict(\n",
        "            fake_res=stage2_fake_res, fake_img=stage2_fake_img)\n",
        "\n",
        "        # generator (encdec) and refiner training step, results contain the\n",
        "        # data for visualization\n",
        "        if self.with_gan:\n",
        "            set_requires_grad(self.disc, False)\n",
        "        results, two_stage_losses = self.two_stage_loss(stage1_results, stage2_results, data_batch)\n",
        "        loss_two_stage, log_vars_two_stage = self.parse_losses(two_stage_losses)\n",
        "        log_vars.update(log_vars_two_stage)\n",
        "        optimizer['generator'].zero_grad()\n",
        "        loss_two_stage.backward()\n",
        "        optimizer['generator'].step()\n",
        "\n",
        "        outputs = dict(\n",
        "            log_vars=log_vars,\n",
        "            num_samples=len(data_batch['gt_img'].data),\n",
        "            results=results)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# generator\n",
        "two_stage.py / train_step()\n",
        "v\n",
        "two_stage.py / two_stage_loss()\n",
        "v\n",
        "two_stage.py / calculate_loss_with_type()\n",
        "v\n",
        "two_stage.py / loss_g_fake = self.loss_gan(g_fake_pred, True, is_disc=False)\n",
        "\n",
        "# discriminator\n",
        "two_stage.py / train_step()\n",
        "v\n",
        "two_stage.py -> one_stage.py / forward_train_d()\n",
        "v\n",
        "one_stage.py / loss = dict(real_loss=loss_) if is_real else dict(fake_loss=loss_)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvJjL1S3vjD0",
        "cellView": "form"
      },
      "source": [
        "#@title one_stage.py (new loss, todo: adding loss functions to ```def generator_loss```)\n",
        "%%writefile /content/Colab-mmediting/mmedit/models/inpaintors/one_stage.py\n",
        "\n",
        "\n",
        "from .vic.loss import CharbonnierLoss, GANLoss, GradientPenaltyLoss, HFENLoss, TVLoss, GradientLoss, ElasticLoss, RelativeL1, L1CosineSim, ClipL1, MaskedL1Loss, MultiscalePixelLoss, FFTloss, OFLoss, L1_regularization, ColorLoss, AverageLoss, GPLoss, CPLoss, SPL_ComputeWithTrace, SPLoss, Contextual_Loss\n",
        "from .vic.filters import *\n",
        "from .vic.colors import *\n",
        "from .vic.discriminators import *\n",
        "\n",
        "import os.path as osp\n",
        "from pathlib import Path\n",
        "\n",
        "import mmcv\n",
        "import torch\n",
        "from mmcv.runner import auto_fp16\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from mmedit.core import L1Evaluation, psnr, ssim, tensor2img\n",
        "from ..base import BaseModel\n",
        "from ..builder import build_backbone, build_component, build_loss\n",
        "from ..common import set_requires_grad\n",
        "from ..registry import MODELS\n",
        "\n",
        "\n",
        "@MODELS.register_module()\n",
        "class OneStageInpaintor(BaseModel):\n",
        "    \"\"\"Standard one-stage inpaintor with commonly used losses.\n",
        "\n",
        "    An inpaintor must contain an encoder-decoder style generator to\n",
        "    inpaint masked regions. A discriminator will be adopted when\n",
        "    adversarial training is needed.\n",
        "\n",
        "    In this class, we provide a common interface for inpaintors.\n",
        "    For other inpaintors, only some funcs may be modified to fit the\n",
        "    input style or training schedule.\n",
        "\n",
        "    Args:\n",
        "        generator (dict): Config for encoder-decoder style generator.\n",
        "        disc (dict): Config for discriminator.\n",
        "        loss_gan (dict): Config for adversarial loss.\n",
        "        loss_gp (dict): Config for gradient penalty loss.\n",
        "        loss_disc_shift (dict): Config for discriminator shift loss.\n",
        "        loss_composed_percep (dict): Config for perceptural and style loss with\n",
        "            composed image as input.\n",
        "        loss_out_percep (dict): Config for perceptural and style loss with\n",
        "            direct output as input.\n",
        "        loss_l1_hole (dict): Config for l1 loss in the hole.\n",
        "        loss_l1_valid (dict): Config for l1 loss in the valid region.\n",
        "        loss_tv (dict): Config for total variation loss.\n",
        "        train_cfg (dict): Configs for training scheduler. `disc_step` must be\n",
        "            contained for indicates the discriminator updating steps in each\n",
        "            training step.\n",
        "        test_cfg (dict): Configs for testing scheduler.\n",
        "        pretrained (str): Path for pretrained model. Default None.\n",
        "    \"\"\"\n",
        "    _eval_metrics = dict(l1=L1Evaluation, psnr=psnr, ssim=ssim)\n",
        "\n",
        "    def __init__(self,\n",
        "                 encdec,\n",
        "                 disc=None,\n",
        "                 loss_gan=None,\n",
        "                 loss_gp=None,\n",
        "                 loss_disc_shift=None,\n",
        "                 loss_composed_percep=None,\n",
        "                 loss_out_percep=False,\n",
        "                 loss_l1_hole=None,\n",
        "                 loss_l1_valid=None,\n",
        "                 loss_tv=None,\n",
        "                 train_cfg=None,\n",
        "                 test_cfg=None,\n",
        "                 pretrained=None):\n",
        "        super(OneStageInpaintor, self).__init__()\n",
        "        self.with_l1_hole_loss = loss_l1_hole is not None\n",
        "        self.with_l1_valid_loss = loss_l1_valid is not None\n",
        "        self.with_tv_loss = loss_tv is not None\n",
        "        self.with_composed_percep_loss = loss_composed_percep is not None\n",
        "        self.with_out_percep_loss = loss_out_percep\n",
        "        self.with_gan = disc is not None and loss_gan is not None\n",
        "        self.with_gp_loss = loss_gp is not None\n",
        "        self.with_disc_shift_loss = loss_disc_shift is not None\n",
        "        self.is_train = train_cfg is not None\n",
        "        self.train_cfg = train_cfg\n",
        "        self.test_cfg = test_cfg\n",
        "        self.eval_with_metrics = ('metrics' in self.test_cfg) and (\n",
        "            self.test_cfg['metrics'] is not None)\n",
        "\n",
        "        self.generator = build_backbone(encdec)\n",
        "\n",
        "        # support fp16\n",
        "        self.fp16_enabled = False\n",
        "\n",
        "        # build loss modules\n",
        "        if self.with_gan:\n",
        "            self.disc = build_component(disc)\n",
        "            self.loss_gan = build_loss(loss_gan)\n",
        "\n",
        "        if self.with_l1_hole_loss:\n",
        "            self.loss_l1_hole = build_loss(loss_l1_hole)\n",
        "\n",
        "        if self.with_l1_valid_loss:\n",
        "            self.loss_l1_valid = build_loss(loss_l1_valid)\n",
        "\n",
        "        if self.with_composed_percep_loss:\n",
        "            self.loss_percep = build_loss(loss_composed_percep)\n",
        "\n",
        "        if self.with_gp_loss:\n",
        "            self.loss_gp = build_loss(loss_gp)\n",
        "\n",
        "        if self.with_disc_shift_loss:\n",
        "            self.loss_disc_shift = build_loss(loss_disc_shift)\n",
        "\n",
        "        if self.with_tv_loss:\n",
        "            self.loss_tv = build_loss(loss_tv)\n",
        "\n",
        "        self.disc_step_count = 0\n",
        "        self.init_weights(pretrained=pretrained)\n",
        "\n",
        "        # new loss\n",
        "\n",
        "        # #l_hfen_type = CharbonnierLoss() # nn.L1Loss(), nn.MSELoss(), CharbonnierLoss(), ElasticLoss(), RelativeL1(), L1CosineSim()\n",
        "        l_hfen_type = L1CosineSim()\n",
        "        self.HFENLoss = HFENLoss(loss_f=l_hfen_type, kernel='log', kernel_size=15, sigma = 2.5, norm = False)\n",
        "\n",
        "        self.ElasticLoss = ElasticLoss(a=0.2, reduction='mean')\n",
        "\n",
        "        self.RelativeL1 = RelativeL1(eps=.01, reduction='mean')\n",
        "\n",
        "        self.L1CosineSim = L1CosineSim(loss_lambda=5, reduction='mean')\n",
        "\n",
        "        self.ClipL1 = ClipL1(clip_min=0.0, clip_max=10.0)\n",
        "\n",
        "        self.FFTloss = FFTloss(loss_f = torch.nn.L1Loss, reduction='mean')\n",
        "\n",
        "        self.OFLoss = OFLoss()\n",
        "\n",
        "        self.GPLoss = GPLoss(trace=False, spl_denorm=False)\n",
        "\n",
        "        self.CPLoss = CPLoss(rgb=True, yuv=True, yuvgrad=True, trace=False, spl_denorm=False, yuv_denorm=False)\n",
        "\n",
        "        layers_weights = {'conv_1_1': 1.0, 'conv_3_2': 1.0}\n",
        "        self.Contextual_Loss = Contextual_Loss(layers_weights, crop_quarter=False, max_1d_size=100,\n",
        "            distance_type = 'cosine', b=1.0, band_width=0.5,\n",
        "            use_vgg = True, net = 'vgg19', calc_type = 'regular')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def init_weights(self, pretrained=None):\n",
        "        \"\"\"Init weights for models.\n",
        "\n",
        "        Args:\n",
        "            pretrained (str, optional): Path for pretrained weights. If given\n",
        "                None, pretrained weights will not be loaded. Defaults to None.\n",
        "        \"\"\"\n",
        "        self.generator.init_weights(pretrained=pretrained)\n",
        "        if self.with_gan:\n",
        "            self.disc.init_weights(pretrained=pretrained)\n",
        "\n",
        "    @auto_fp16(apply_to=('masked_img', 'mask'))\n",
        "    def forward(self, masked_img, mask, test_mode=True, **kwargs):\n",
        "        \"\"\"Forward function.\n",
        "\n",
        "        Args:\n",
        "            masked_img (torch.Tensor): Image with hole as input.\n",
        "            mask (torch.Tensor): Mask as input.\n",
        "            test_mode (bool, optional): Whether use testing mode.\n",
        "                Defaults to True.\n",
        "\n",
        "        Returns:\n",
        "            dict: Dict contains output results.\n",
        "        \"\"\"\n",
        "        if not test_mode:\n",
        "            return self.forward_train(masked_img, mask, **kwargs)\n",
        "        else:\n",
        "            return self.forward_test(masked_img, mask, **kwargs)\n",
        "\n",
        "    def forward_train(self, *args, **kwargs):\n",
        "        \"\"\"Forward function for training.\n",
        "\n",
        "        In this version, we do not use this interface.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError('This interface should not be used in '\n",
        "                                  'current training schedule. Please use '\n",
        "                                  '`train_step` for training.')\n",
        "\n",
        "    def forward_train_d(self, data_batch, is_real, is_disc):\n",
        "        \"\"\"Forward function in discriminator training step.\n",
        "\n",
        "        In this function, we compute the prediction for each data batch (real\n",
        "        or fake). Meanwhile, the standard gan loss will be computed with\n",
        "        several proposed losses fro stable training.\n",
        "\n",
        "        Args:\n",
        "            data (torch.Tensor): Batch of real data or fake data.\n",
        "            is_real (bool): If True, the gan loss will regard this batch as\n",
        "                real data. Otherwise, the gan loss will regard this batch as\n",
        "                fake data.\n",
        "            is_disc (bool): If True, this function is called in discriminator\n",
        "                training step. Otherwise, this function is called in generator\n",
        "                training step. This will help us to compute different types of\n",
        "                adversarial loss, like LSGAN.\n",
        "\n",
        "        Returns:\n",
        "            dict: Contains the loss items computed in this function.\n",
        "        \"\"\"\n",
        "        pred = self.disc(data_batch)\n",
        "        loss_ = self.loss_gan(pred, is_real, is_disc)\n",
        "\n",
        "        loss = dict(real_loss=loss_) if is_real else dict(fake_loss=loss_)\n",
        "\n",
        "        if self.with_disc_shift_loss:\n",
        "            loss_d_shift = self.loss_disc_shift(loss_)\n",
        "            # 0.5 for average the fake and real data\n",
        "            loss.update(loss_disc_shift=loss_d_shift * 0.5)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def generator_loss(self, fake_res, fake_img, data_batch):\n",
        "        \"\"\"Forward function in generator training step.\n",
        "\n",
        "        In this function, we mainly compute the loss items for generator with\n",
        "        the given (fake_res, fake_img). In general, the `fake_res` is the\n",
        "        direct output of the generator and the `fake_img` is the composition of\n",
        "        direct output and ground-truth image.\n",
        "\n",
        "        Args:\n",
        "            fake_res (torch.Tensor): Direct output of the generator.\n",
        "            fake_img (torch.Tensor): Composition of `fake_res` and\n",
        "                ground-truth image.\n",
        "            data_batch (dict): Contain other elements for computing losses.\n",
        "\n",
        "        Returns:\n",
        "            tuple(dict): Dict contains the results computed within this \\\n",
        "                function for visualization and dict contains the loss items \\\n",
        "                computed in this function.\n",
        "        \"\"\"\n",
        "        gt = data_batch['gt_img']\n",
        "        mask = data_batch['mask']\n",
        "        masked_img = data_batch['masked_img']\n",
        "\n",
        "        loss = dict()\n",
        "\n",
        "        if self.with_gan:\n",
        "            g_fake_pred = self.disc(fake_img)\n",
        "            loss_g_fake = self.loss_gan(g_fake_pred, True, is_disc=False)\n",
        "            loss['loss_g_fake'] = loss_g_fake\n",
        "\n",
        "        if self.with_l1_hole_loss:\n",
        "            loss_l1_hole = self.loss_l1_hole(fake_res, gt, weight=mask)\n",
        "            loss['loss_l1_hole'] = loss_l1_hole\n",
        "\n",
        "        if self.with_l1_valid_loss:\n",
        "            loss_loss_l1_valid = self.loss_l1_valid(\n",
        "                fake_res, gt, weight=1. - mask)\n",
        "            loss['loss_l1_valid'] = loss_loss_l1_valid\n",
        "\n",
        "        if self.with_composed_percep_loss:\n",
        "            loss_pecep, loss_style = self.loss_percep(fake_img, gt)\n",
        "            if loss_pecep is not None:\n",
        "                loss['loss_composed_percep'] = loss_pecep\n",
        "            if loss_style is not None:\n",
        "                loss['loss_composed_style'] = loss_style\n",
        "\n",
        "        if self.with_out_percep_loss:\n",
        "            loss_out_percep, loss_out_style = self.loss_percep(fake_res, gt)\n",
        "            if loss_out_percep is not None:\n",
        "                loss['loss_out_percep'] = loss_out_percep\n",
        "            if loss_out_style is not None:\n",
        "                loss['loss_out_style'] = loss_out_style\n",
        "\n",
        "        if self.with_tv_loss:\n",
        "            loss_tv = self.loss_tv(fake_img, mask=mask)\n",
        "            loss['loss_tv'] = loss_tv\n",
        "\n",
        "        res = dict(\n",
        "            gt_img=gt.cpu(),\n",
        "            masked_img=masked_img.cpu(),\n",
        "            fake_res=fake_res.cpu(),\n",
        "            fake_img=fake_img.cpu())\n",
        "\n",
        "        return res, loss\n",
        "\n",
        "    def forward_test(self,\n",
        "                     masked_img,\n",
        "                     mask,\n",
        "                     save_image=False,\n",
        "                     save_path=None,\n",
        "                     iteration=None,\n",
        "                     **kwargs):\n",
        "        \"\"\"Forward function for testing.\n",
        "\n",
        "        Args:\n",
        "            masked_img (torch.Tensor): Tensor with shape of (n, 3, h, w).\n",
        "            mask (torch.Tensor): Tensor with shape of (n, 1, h, w).\n",
        "            save_image (bool, optional): If True, results will be saved as\n",
        "                image. Defaults to False.\n",
        "            save_path (str, optional): If given a valid str, the reuslts will\n",
        "                be saved in this path. Defaults to None.\n",
        "            iteration (int, optional): Iteration number. Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            dict: Contain output results and eval metrics (if have).\n",
        "        \"\"\"\n",
        "        input_x = torch.cat([masked_img, mask], dim=1)\n",
        "        fake_res = self.generator(input_x)\n",
        "        fake_img = fake_res * mask + masked_img * (1. - mask)\n",
        "\n",
        "        output = dict()\n",
        "        eval_results = {}\n",
        "        if self.eval_with_metrics:\n",
        "            gt_img = kwargs['gt_img']\n",
        "            data_dict = dict(gt_img=gt_img, fake_res=fake_res, mask=mask)\n",
        "            for metric_name in self.test_cfg['metrics']:\n",
        "                if metric_name in ['ssim', 'psnr']:\n",
        "                    eval_results[metric_name] = self._eval_metrics[\n",
        "                        metric_name](tensor2img(fake_img, min_max=(-1, 1)),\n",
        "                                     tensor2img(gt_img, min_max=(-1, 1)))\n",
        "                else:\n",
        "                    eval_results[metric_name] = self._eval_metrics[\n",
        "                        metric_name]()(data_dict).item()\n",
        "            output['eval_results'] = eval_results\n",
        "        else:\n",
        "            output['fake_res'] = fake_res\n",
        "            output['fake_img'] = fake_img\n",
        "\n",
        "        output['meta'] = None if 'meta' not in kwargs else kwargs['meta'][0]\n",
        "\n",
        "        if save_image:\n",
        "            assert save_image and save_path is not None, (\n",
        "                'Save path should been given')\n",
        "            assert output['meta'] is not None, (\n",
        "                'Meta information should be given to save image.')\n",
        "\n",
        "            tmp_filename = output['meta']['gt_img_path']\n",
        "            filestem = Path(tmp_filename).stem\n",
        "            if iteration is not None:\n",
        "                filename = f'{filestem}_{iteration}.png'\n",
        "            else:\n",
        "                filename = f'{filestem}.png'\n",
        "            mmcv.mkdir_or_exist(save_path)\n",
        "            img_list = [kwargs['gt_img']] if 'gt_img' in kwargs else []\n",
        "            img_list.extend(\n",
        "                [masked_img,\n",
        "                 mask.expand_as(masked_img), fake_res, fake_img])\n",
        "            img = torch.cat(img_list, dim=3).cpu()\n",
        "            self.save_visualization(img, osp.join(save_path, filename))\n",
        "            output['save_img_path'] = osp.abspath(\n",
        "                osp.join(save_path, filename))\n",
        "\n",
        "        return output\n",
        "\n",
        "    def save_visualization(self, img, filename):\n",
        "        \"\"\"Save visualization results.\n",
        "\n",
        "        Args:\n",
        "            img (torch.Tensor): Tensor with shape of (n, 3, h, w).\n",
        "            filename (str): Path to save visualization.\n",
        "        \"\"\"\n",
        "        if self.test_cfg.get('img_rerange', True):\n",
        "            img = (img + 1) / 2\n",
        "        if self.test_cfg.get('img_bgr2rgb', True):\n",
        "            img = img[:, [2, 1, 0], ...]\n",
        "        save_image(img, filename, nrow=1, padding=0)\n",
        "\n",
        "    def train_step(self, data_batch, optimizer):\n",
        "        \"\"\"Train step function.\n",
        "\n",
        "        In this function, the inpaintor will finish the train step following\n",
        "        the pipeline:\n",
        "\n",
        "            1. get fake res/image\n",
        "            2. optimize discriminator (if have)\n",
        "            3. optimize generator\n",
        "\n",
        "        If `self.train_cfg.disc_step > 1`, the train step will contain multiple\n",
        "        iterations for optimizing discriminator with different input data and\n",
        "        only one iteration for optimizing gerator after `disc_step` iterations\n",
        "        for discriminator.\n",
        "\n",
        "        Args:\n",
        "            data_batch (torch.Tensor): Batch of data as input.\n",
        "            optimizer (dict[torch.optim.Optimizer]): Dict with optimizers for\n",
        "                generator and discriminator (if have).\n",
        "\n",
        "        Returns:\n",
        "            dict: Dict with loss, information for logger, the number of \\\n",
        "                samples and results for visualization.\n",
        "        \"\"\"\n",
        "        log_vars = {}\n",
        "\n",
        "        gt_img = data_batch['gt_img']\n",
        "        mask = data_batch['mask']\n",
        "        masked_img = data_batch['masked_img']\n",
        "\n",
        "        # get common output from encdec\n",
        "        input_x = torch.cat([masked_img, mask], dim=1)\n",
        "        fake_res = self.generator(input_x)\n",
        "        fake_img = gt_img * (1. - mask) + fake_res * mask\n",
        "\n",
        "        # discriminator training step\n",
        "        if self.train_cfg.disc_step > 0:\n",
        "            set_requires_grad(self.disc, True)\n",
        "            disc_losses = self.forward_train_d(\n",
        "                fake_img.detach(), False, is_disc=True)\n",
        "            loss_disc, log_vars_d = self.parse_losses(disc_losses)\n",
        "            log_vars.update(log_vars_d)\n",
        "            optimizer['disc'].zero_grad()\n",
        "            loss_disc.backward()\n",
        "\n",
        "            disc_losses = self.forward_train_d(gt_img, True, is_disc=True)\n",
        "            loss_disc, log_vars_d = self.parse_losses(disc_losses)\n",
        "            log_vars.update(log_vars_d)\n",
        "            loss_disc.backward()\n",
        "\n",
        "            if self.with_gp_loss:\n",
        "                loss_d_gp = self.loss_gp(\n",
        "                    self.disc, gt_img, fake_img, mask=mask)\n",
        "                loss_disc, log_vars_d = self.parse_losses(\n",
        "                    dict(loss_gp=loss_d_gp))\n",
        "                log_vars.update(log_vars_d)\n",
        "                loss_disc.backward()\n",
        "\n",
        "            optimizer['disc'].step()\n",
        "\n",
        "            self.disc_step_count = (self.disc_step_count +\n",
        "                                    1) % self.train_cfg.disc_step\n",
        "            if self.disc_step_count != 0:\n",
        "                # results contain the data for visualization\n",
        "                results = dict(\n",
        "                    gt_img=gt_img.cpu(),\n",
        "                    masked_img=masked_img.cpu(),\n",
        "                    fake_res=fake_res.cpu(),\n",
        "                    fake_img=fake_img.cpu())\n",
        "                outputs = dict(\n",
        "                    log_vars=log_vars,\n",
        "                    num_samples=len(data_batch['gt_img'].data),\n",
        "                    results=results)\n",
        "\n",
        "                return outputs\n",
        "\n",
        "        # generator (encdec) training step, results contain the data\n",
        "        # for visualization\n",
        "        if self.with_gan:\n",
        "            set_requires_grad(self.disc, False)\n",
        "        results, g_losses = self.generator_loss(fake_res, fake_img, data_batch)\n",
        "        loss_g, log_vars_g = self.parse_losses(g_losses)\n",
        "        log_vars.update(log_vars_g)\n",
        "        optimizer['generator'].zero_grad()\n",
        "        loss_g.backward()\n",
        "        optimizer['generator'].step()\n",
        "\n",
        "        outputs = dict(\n",
        "            log_vars=log_vars,\n",
        "            num_samples=len(data_batch['gt_img'].data),\n",
        "            results=results)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def val_step(self, data_batch, **kwargs):\n",
        "        \"\"\"Forward function for evaluation.\n",
        "\n",
        "        Args:\n",
        "            data_batch (dict): Contain data for forward.\n",
        "\n",
        "        Returns:\n",
        "            dict: Contain the results from model.\n",
        "        \"\"\"\n",
        "        output = self.forward_test(**data_batch, **kwargs)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def forward_dummy(self, x):\n",
        "        \"\"\"Forward dummy function for getting flops.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor with shape of (n, c, h, w).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Results tensor with shape of (n, 3, h, w).\n",
        "        \"\"\"\n",
        "        res = self.generator(x)\n",
        "\n",
        "        return res\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6ZZHv8RiAtB"
      },
      "source": [
        "Mosaic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6oEPylXiJfA",
        "cellView": "form"
      },
      "source": [
        "#@title two_stage.py (diffaug + new loss)\n",
        "%%writefile /content/Colab-mmediting/mmedit/models/inpaintors/two_stage.py\n",
        "\n",
        "from vic.loss import CharbonnierLoss, GANLoss, GradientPenaltyLoss, HFENLoss, TVLoss, GradientLoss, ElasticLoss, RelativeL1, L1CosineSim, ClipL1, MaskedL1Loss, MultiscalePixelLoss, FFTloss, OFLoss, L1_regularization, ColorLoss, AverageLoss, GPLoss, CPLoss, SPL_ComputeWithTrace, SPLoss, Contextual_Loss\n",
        "from vic.filters import *\n",
        "from vic.colors import *\n",
        "from vic.discriminators import *\n",
        "from diffaug import *\n",
        "\n",
        "from torchvision.utils import save_image\n",
        "import os.path as osp\n",
        "from pathlib import Path\n",
        "\n",
        "import mmcv\n",
        "import torch\n",
        "from mmedit.core import tensor2img\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from ..common.model_utils import set_requires_grad\n",
        "from ..registry import MODELS\n",
        "from .one_stage import OneStageInpaintor\n",
        "\n",
        "#from DiffAugment_pytorch import DiffAugment\n",
        "\n",
        "@MODELS.register_module()\n",
        "class TwoStageInpaintor(OneStageInpaintor):\n",
        "    \"\"\"Two-Stage Inpaintor.\n",
        "\n",
        "    Currently, we support these loss types in each of two stage inpaintors:\n",
        "    ['loss_gan', 'loss_l1_hole', 'loss_l1_valid', 'loss_composed_percep',\\\n",
        "     'loss_out_percep', 'loss_tv']\n",
        "    The `stage1_loss_type` and `stage2_loss_type` should be chosen from these\n",
        "    loss types.\n",
        "\n",
        "    Args:\n",
        "        stage1_loss_type (tuple[str]): Contains the loss names used in the\n",
        "            first stage model.\n",
        "        stage2_loss_type (tuple[str]): Contains the loss names used in the\n",
        "            second stage model.\n",
        "        input_with_ones (bool): Whether to concatenate an extra ones tensor in\n",
        "            input. Default: True.\n",
        "        disc_input_with_mask (bool): Whether to add mask as input in\n",
        "            discriminator. Default: False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 *args,\n",
        "                 stage1_loss_type=('loss_l1_hole', ),\n",
        "                 stage2_loss_type=('loss_l1_hole', 'loss_gan'),\n",
        "                 input_with_ones=True,\n",
        "                 disc_input_with_mask=False,\n",
        "                 **kwargs):\n",
        "        super(TwoStageInpaintor, self).__init__(*args, **kwargs)\n",
        "\n",
        "        self.stage1_loss_type = stage1_loss_type\n",
        "        self.stage2_loss_type = stage2_loss_type\n",
        "        self.input_with_ones = input_with_ones\n",
        "        self.disc_input_with_mask = disc_input_with_mask\n",
        "        self.eval_with_metrics = ('metrics' in self.test_cfg) and (\n",
        "            self.test_cfg['metrics'] is not None)\n",
        "\n",
        "        # new loss\n",
        "        \n",
        "        # #l_hfen_type = CharbonnierLoss() # nn.L1Loss(), nn.MSELoss(), CharbonnierLoss(), ElasticLoss(), RelativeL1(), L1CosineSim()\n",
        "        l_hfen_type = L1CosineSim()\n",
        "        self.HFENLoss = HFENLoss(loss_f=l_hfen_type, kernel='log', kernel_size=15, sigma = 2.5, norm = False)\n",
        "\n",
        "        self.ElasticLoss = ElasticLoss(a=0.2, reduction='mean')\n",
        "\n",
        "        self.RelativeL1 = RelativeL1(eps=.01, reduction='mean')\n",
        "\n",
        "        self.L1CosineSim = L1CosineSim(loss_lambda=5, reduction='mean')\n",
        "\n",
        "        self.ClipL1 = ClipL1(clip_min=0.0, clip_max=10.0)\n",
        "\n",
        "        self.FFTloss = FFTloss(loss_f = torch.nn.L1Loss, reduction='mean')\n",
        "\n",
        "        self.OFLoss = OFLoss()\n",
        "\n",
        "        self.GPLoss = GPLoss(trace=False, spl_denorm=False)\n",
        "\n",
        "        self.CPLoss = CPLoss(rgb=True, yuv=True, yuvgrad=True, trace=False, spl_denorm=False, yuv_denorm=False)\n",
        "\n",
        "        layers_weights = {'conv_1_1': 1.0, 'conv_3_2': 1.0}\n",
        "        self.Contextual_Loss = Contextual_Loss(layers_weights, crop_quarter=False, max_1d_size=100, \n",
        "            distance_type = 'cosine', b=1.0, band_width=0.5, \n",
        "            use_vgg = True, net = 'vgg19', calc_type = 'regular')\n",
        "\n",
        "\n",
        "\n",
        "    def forward_test(self,\n",
        "                     masked_img,\n",
        "                     mask,\n",
        "                     save_image=False,\n",
        "                     save_path=None,\n",
        "                     iteration=None,\n",
        "                     **kwargs):\n",
        "        \"\"\"Forward function for testing.\n",
        "\n",
        "        Args:\n",
        "            masked_img (torch.Tensor): Tensor with shape of (n, 3, h, w).\n",
        "            mask (torch.Tensor): Tensor with shape of (n, 1, h, w).\n",
        "            save_image (bool, optional): If True, results will be saved as\n",
        "                image. Defaults to False.\n",
        "            save_path (str, optional): If given a valid str, the reuslts will\n",
        "                be saved in this path. Defaults to None.\n",
        "            iteration (int, optional): Iteration number. Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            dict: Contain output results and eval metrics (if have).\n",
        "        \"\"\"\n",
        "        if self.input_with_ones:\n",
        "            tmp_ones = torch.ones_like(mask)\n",
        "            input_x = torch.cat([masked_img, tmp_ones, mask], dim=1)\n",
        "        else:\n",
        "            input_x = torch.cat([masked_img, mask], dim=1)\n",
        "        stage1_fake_res, stage2_fake_res = self.generator(input_x)\n",
        "        fake_img = stage2_fake_res * mask + masked_img * (1. - mask)\n",
        "        output = dict()\n",
        "        eval_results = {}\n",
        "        if self.eval_with_metrics:\n",
        "            gt_img = kwargs['gt_img']\n",
        "            data_dict = dict(\n",
        "                gt_img=gt_img, fake_res=stage2_fake_res, mask=mask)\n",
        "            for metric_name in self.test_cfg['metrics']:\n",
        "                if metric_name in ['ssim', 'psnr']:\n",
        "                    eval_results[metric_name] = self._eval_metrics[\n",
        "                        metric_name](tensor2img(fake_img, min_max=(-1, 1)),\n",
        "                                     tensor2img(gt_img, min_max=(-1, 1)))\n",
        "                else:\n",
        "                    eval_results[metric_name] = self._eval_metrics[\n",
        "                        metric_name]()(data_dict).item()\n",
        "            output['eval_results'] = eval_results\n",
        "        else:\n",
        "            output['stage1_fake_res'] = stage1_fake_res\n",
        "            output['stage2_fake_res'] = stage2_fake_res\n",
        "            output['fake_res'] = stage2_fake_res\n",
        "            output['fake_img'] = fake_img\n",
        "\n",
        "        output['meta'] = None if 'meta' not in kwargs else kwargs['meta'][0]\n",
        "\n",
        "        if save_image:\n",
        "            assert save_image and save_path is not None, (\n",
        "                'Save path should be given')\n",
        "            assert output['meta'] is not None, (\n",
        "                'Meta information should be given to save image.')\n",
        "\n",
        "            tmp_filename = output['meta']['gt_img_path']\n",
        "            filestem = Path(tmp_filename).stem\n",
        "            if iteration is not None:\n",
        "                filename = f'{filestem}_{iteration}.png'\n",
        "            else:\n",
        "                filename = f'{filestem}.png'\n",
        "            mmcv.mkdir_or_exist(save_path)\n",
        "            img_list = [kwargs['gt_img']] if 'gt_img' in kwargs else []\n",
        "            img_list.extend([\n",
        "                masked_img,\n",
        "                mask.expand_as(masked_img), stage1_fake_res, stage2_fake_res,\n",
        "                fake_img\n",
        "            ])\n",
        "            img = torch.cat(img_list, dim=3).cpu()\n",
        "            self.save_visualization(img, osp.join(save_path, filename))\n",
        "            output['save_img_path'] = osp.abspath(\n",
        "                osp.join(save_path, filename))\n",
        "\n",
        "        return output\n",
        "\n",
        "    def save_visualization(self, img, filename):\n",
        "        \"\"\"Save visualization results.\n",
        "\n",
        "        Args:\n",
        "            img (torch.Tensor): Tensor with shape of (n, 3, h, w).\n",
        "            filename (str): Path to save visualization.\n",
        "        \"\"\"\n",
        "        if self.test_cfg.get('img_rerange', True):\n",
        "            img = (img + 1) / 2\n",
        "        if self.test_cfg.get('img_bgr2rgb', True):\n",
        "            img = img[:, [2, 1, 0], ...]\n",
        "        save_image(img, filename, nrow=1, padding=0)\n",
        "\n",
        "    def two_stage_loss(self, stage1_data, stage2_data, data_batch):\n",
        "        \"\"\"Calculate two-stage loss.\n",
        "\n",
        "        Args:\n",
        "            stage1_data (dict): Contain stage1 results.\n",
        "            stage2_data (dict): Contain stage2 results.\n",
        "            data_batch (dict): Contain data needed to calculate loss.\n",
        "\n",
        "        Returns:\n",
        "            dict: Contain losses with name.\n",
        "        \"\"\"\n",
        "        gt = data_batch['gt_img']\n",
        "        mask = data_batch['mask']\n",
        "        masked_img = data_batch['masked_img']\n",
        "\n",
        "        loss = dict()\n",
        "        results = dict(\n",
        "            gt_img=gt.cpu(), mask=mask.cpu(), masked_img=masked_img.cpu())\n",
        "        # calculate losses for stage1\n",
        "        if self.stage1_loss_type is not None:\n",
        "            fake_res = stage1_data['fake_res']\n",
        "            fake_img = stage1_data['fake_img']\n",
        "            for type_key in self.stage1_loss_type:\n",
        "                tmp_loss = self.calculate_loss_with_type(\n",
        "                    type_key, fake_res, fake_img, gt, mask, prefix='stage1_')\n",
        "                loss.update(tmp_loss)\n",
        "\n",
        "        results.update(\n",
        "            dict(\n",
        "                stage1_fake_res=stage1_data['fake_res'].cpu(),\n",
        "                stage1_fake_img=stage1_data['fake_img'].cpu()))\n",
        "\n",
        "        if self.stage2_loss_type is not None:\n",
        "            fake_res = stage2_data['fake_res']\n",
        "            fake_img = stage2_data['fake_img']\n",
        "            for type_key in self.stage2_loss_type:\n",
        "                tmp_loss = self.calculate_loss_with_type(\n",
        "                    type_key, fake_res, fake_img, gt, mask, prefix='stage2_')\n",
        "                loss.update(tmp_loss)\n",
        "        results.update(\n",
        "            dict(\n",
        "                stage2_fake_res=stage2_data['fake_res'].cpu(),\n",
        "                stage2_fake_img=stage2_data['fake_img'].cpu()))\n",
        "\n",
        "        return results, loss\n",
        "\n",
        "    def calculate_loss_with_type(self,\n",
        "                                 loss_type,\n",
        "                                 fake_res,\n",
        "                                 fake_img,\n",
        "                                 gt,\n",
        "                                 mask,\n",
        "                                 prefix='stage1_'):\n",
        "        \"\"\"Calculate multiple types of losses.\n",
        "\n",
        "        Args:\n",
        "            loss_type (str): Type of the loss.\n",
        "            fake_res (torch.Tensor): Direct results from model.\n",
        "            fake_img (torch.Tensor): Composited results from model.\n",
        "            gt (torch.Tensor): Ground-truth tensor.\n",
        "            mask (torch.Tensor): Mask tensor.\n",
        "            prefix (str, optional): Prefix for loss name.\n",
        "                Defaults to 'stage1_'.\n",
        "\n",
        "        Returns:\n",
        "            dict: Contain loss value with its name.\n",
        "        \"\"\"\n",
        "        loss_dict = dict()\n",
        "        if loss_type == 'loss_gan':\n",
        "            if self.disc_input_with_mask:\n",
        "                disc_input_x = torch.cat([fake_img, mask], dim=1)\n",
        "            else:\n",
        "                disc_input_x = fake_img\n",
        "            g_fake_pred = self.disc(disc_input_x)\n",
        "            #############################################################\n",
        "            #loss_g_fake = self.loss_gan(g_fake_pred, True, is_disc=False)\n",
        "            #loss_g_fake = (DiffAugment(g_fake_pred, policy=policy)) #DiffAug\n",
        "\n",
        "            #alternativ:\n",
        "            g_fake_pred = DiffAugment(g_fake_pred, policy=policy)\n",
        "            loss_g_fake = self.loss_gan(g_fake_pred, True, is_disc=False)\n",
        "            ##############################################################\n",
        "            loss_dict[prefix + 'loss_g_fake'] = loss_g_fake\n",
        "        elif 'percep' in loss_type:\n",
        "            loss_pecep, loss_style = self.loss_percep(fake_img, gt)\n",
        "            if loss_pecep is not None:\n",
        "                loss_dict[prefix + loss_type] = loss_pecep\n",
        "            if loss_style is not None:\n",
        "                loss_dict[prefix + loss_type[:-6] + 'style'] = loss_style\n",
        "        elif 'tv' in loss_type:\n",
        "            loss_tv = self.loss_tv(fake_img, mask=mask)\n",
        "            loss_dict[prefix + loss_type] = loss_tv\n",
        "        elif 'l1' in loss_type:\n",
        "            weight = 1. - mask if 'valid' in loss_type else mask\n",
        "            loss_l1 = getattr(self, loss_type)(fake_res, gt, weight=weight)\n",
        "            loss_dict[prefix + loss_type] = loss_l1\n",
        "        # new\n",
        "        elif 'HFEN' in loss_type:\n",
        "            loss_hfen = self.HFENLoss(fake_img, gt)\n",
        "            loss_dict[prefix + loss_type] = loss_hfen\n",
        "        elif 'Elastic' in loss_type:\n",
        "            loss_elastic = self.ElasticLoss(fake_img, gt)\n",
        "            loss_dict[prefix + loss_type] = loss_elastic\n",
        "        elif 'RelativeL1' in loss_type:\n",
        "            loss_relativel1 = self.RelativeL1(fake_img, gt)\n",
        "            loss_dict[prefix + loss_type] = loss_relativel1\n",
        "        elif 'L1CosineSim' in loss_type:\n",
        "            loss_l1cosinesim = self.L1CosineSim(fake_img, gt)\n",
        "            loss_dict[prefix + loss_type] = loss_l1cosinesim\n",
        "        elif 'ClipL1' in loss_type:\n",
        "            loss_clipl1 = self.ClipL1(fake_img, gt)\n",
        "            loss_dict[prefix + loss_type] = loss_clipl1\n",
        "        elif 'FFT' in loss_type:\n",
        "            loss_fft = self.FFTloss(fake_img, gt)\n",
        "            loss_dict[prefix + loss_type] = loss_fft\n",
        "        elif 'OF' in loss_type:\n",
        "            loss_of = self.FFTloss(fake_img)\n",
        "            loss_dict[prefix + loss_type] = loss_of\n",
        "        elif 'GP' in loss_type:\n",
        "            loss_gp = self.FFTloss(fake_img, gt)\n",
        "            loss_dict[prefix + loss_type] = loss_gp\n",
        "        elif 'CP' in loss_type:\n",
        "            loss_cp = self.FFTloss(fake_img, gt)\n",
        "            loss_dict[prefix + loss_type] = loss_cp\n",
        "        else:\n",
        "            raise NotImplementedError(\n",
        "                f'Please check your loss type {loss_type}'\n",
        "                f' and the config dict in init function. '\n",
        "                f'We cannot find the related loss function.')\n",
        "\n",
        "        return loss_dict\n",
        "\n",
        "    def train_step(self, data_batch, optimizer):\n",
        "        \"\"\"Train step function.\n",
        "\n",
        "        In this function, the inpaintor will finish the train step following\n",
        "        the pipeline:\n",
        "\n",
        "            1. get fake res/image\n",
        "            2. optimize discriminator (if have)\n",
        "            3. optimize generator\n",
        "\n",
        "        If `self.train_cfg.disc_step > 1`, the train step will contain multiple\n",
        "        iterations for optimizing discriminator with different input data and\n",
        "        only one iteration for optimizing gerator after `disc_step` iterations\n",
        "        for discriminator.\n",
        "\n",
        "        Args:\n",
        "            data_batch (torch.Tensor): Batch of data as input.\n",
        "            optimizer (dict[torch.optim.Optimizer]): Dict with optimizers for\n",
        "                generator and discriminator (if have).\n",
        "\n",
        "        Returns:\n",
        "            dict: Dict with loss, information for logger, the number of \\\n",
        "                samples and results for visualization.\n",
        "        \"\"\"\n",
        "        log_vars = {}\n",
        "\n",
        "        gt_img = data_batch['gt_img']\n",
        "        mask = data_batch['mask']\n",
        "        masked_img = data_batch['masked_img']\n",
        "\n",
        "        img_size = 256\n",
        "        MOSAIC_MIN = 0.03\n",
        "        MOSAIC_MID = 0.10\n",
        "        MOSAIC_MAX = 0.2\n",
        "\n",
        "        mosaic_size = int(random.triangular(int(min(img_size*MOSAIC_MIN, img_size*MOSAIC_MIN)), int(min(img_size*MOSAIC_MID, img_size*MOSAIC_MID)), int(min(img_size*MOSAIC_MAX, img_size*MOSAIC_MAX))))\n",
        "        images_mosaic = torch.nn.functional.interpolate(gt_img, size=(mosaic_size, mosaic_size), mode='nearest')\n",
        "        images_mosaic = torch.nn.functional.interpolate(images_mosaic, size=(img_size, img_size), mode='nearest')\n",
        "        #masked = (img * (1 - mask).float()) + (images_mosaic * (mask).float())\n",
        "        masked_img = (images_mosaic * (1 - mask).float()) + (gt_img * (mask).float())\n",
        "\n",
        "        # get common output from encdec\n",
        "        if self.input_with_ones:\n",
        "            tmp_ones = torch.ones_like(mask)\n",
        "            input_x = torch.cat([masked_img, tmp_ones, mask], dim=1)\n",
        "        else:\n",
        "            input_x = torch.cat([masked_img, mask], dim=1)\n",
        "\n",
        "        stage1_fake_res, stage2_fake_res = self.generator(input_x)\n",
        "        stage1_fake_img = masked_img * (1. - mask) + stage1_fake_res * mask\n",
        "        stage2_fake_img = masked_img * (1. - mask) + stage2_fake_res * mask\n",
        "\n",
        "        # discriminator training step\n",
        "        # In this version, we only use the results from the second stage to\n",
        "        # train discriminators, which is a commonly used setting. This can be\n",
        "        # easily modified to your custom training schedule.\n",
        "        if self.train_cfg.disc_step > 0:\n",
        "            set_requires_grad(self.disc, True)\n",
        "            if self.disc_input_with_mask:\n",
        "                disc_input_x = torch.cat([stage2_fake_img.detach(), mask],\n",
        "                                         dim=1)\n",
        "            else:\n",
        "                disc_input_x = stage2_fake_img.detach()\n",
        "            disc_losses = self.forward_train_d(disc_input_x, False, is_disc=True)\n",
        "            loss_disc, log_vars_d = self.parse_losses(disc_losses)\n",
        "            log_vars.update(log_vars_d)\n",
        "            optimizer['disc'].zero_grad()\n",
        "            loss_disc.backward()\n",
        "\n",
        "            if self.disc_input_with_mask:\n",
        "                disc_input_x = torch.cat([gt_img, mask], dim=1)\n",
        "            else:\n",
        "                disc_input_x = gt_img\n",
        "            disc_losses = self.forward_train_d(disc_input_x, True, is_disc=True)\n",
        "            loss_disc, log_vars_d = self.parse_losses(disc_losses)\n",
        "            log_vars.update(log_vars_d)\n",
        "            loss_disc.backward()\n",
        "\n",
        "            if self.with_gp_loss:\n",
        "                # gradient penalty loss should not be used with mask as input\n",
        "                assert not self.disc_input_with_mask\n",
        "                loss_d_gp = self.loss_gp(self.disc, gt_img, stage2_fake_img, mask=mask)\n",
        "                loss_disc, log_vars_d = self.parse_losses(dict(loss_gp=loss_d_gp))\n",
        "                log_vars.update(log_vars_d)\n",
        "                loss_disc.backward()\n",
        "\n",
        "            optimizer['disc'].step()\n",
        "\n",
        "            self.disc_step_count = (self.disc_step_count +\n",
        "                                    1) % self.train_cfg.disc_step\n",
        "            if self.disc_step_count != 0:\n",
        "                # results contain the data for visualization\n",
        "                results = dict(\n",
        "                    gt_img=gt_img.cpu(),\n",
        "                    masked_img=masked_img.cpu(),\n",
        "                    fake_res=stage2_fake_res.cpu(),\n",
        "                    fake_img=stage2_fake_img.cpu())\n",
        "                outputs = dict(\n",
        "                    log_vars=log_vars,\n",
        "                    num_samples=len(data_batch['gt_img'].data),\n",
        "                    results=results)\n",
        "\n",
        "                return outputs\n",
        "\n",
        "        # prepare stage1 results and stage2 results dict for calculating losses\n",
        "        stage1_results = dict(\n",
        "            fake_res=stage1_fake_res, fake_img=stage1_fake_img)\n",
        "        stage2_results = dict(\n",
        "            fake_res=stage2_fake_res, fake_img=stage2_fake_img)\n",
        "\n",
        "        # generator (encdec) and refiner training step, results contain the\n",
        "        # data for visualization\n",
        "        if self.with_gan:\n",
        "            set_requires_grad(self.disc, False)\n",
        "        results, two_stage_losses = self.two_stage_loss(stage1_results, stage2_results, data_batch)\n",
        "        loss_two_stage, log_vars_two_stage = self.parse_losses(two_stage_losses)\n",
        "        log_vars.update(log_vars_two_stage)\n",
        "        optimizer['generator'].zero_grad()\n",
        "        loss_two_stage.backward()\n",
        "        optimizer['generator'].step()\n",
        "\n",
        "        outputs = dict(\n",
        "            log_vars=log_vars,\n",
        "            num_samples=len(data_batch['gt_img'].data),\n",
        "            results=results)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# generator\n",
        "two_stage.py / train_step()\n",
        "v\n",
        "two_stage.py / two_stage_loss()\n",
        "v\n",
        "two_stage.py / calculate_loss_with_type()\n",
        "v\n",
        "two_stage.py / loss_g_fake = self.loss_gan(g_fake_pred, True, is_disc=False)\n",
        "\n",
        "# discriminator\n",
        "two_stage.py / train_step()\n",
        "v\n",
        "two_stage.py -> one_stage.py / forward_train_d()\n",
        "v\n",
        "one_stage.py / loss = dict(real_loss=loss_) if is_real else dict(fake_loss=loss_)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2o6GZM6zlyet"
      },
      "source": [
        "--------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10eWTHaHlZaS",
        "cellView": "form"
      },
      "source": [
        "#@title custom.py (new loss in configs) (deepfillv2)\n",
        "%%writefile /content/Colab-mmediting/configs/inpainting/deepfillv2/custom.py\n",
        "model = dict(\n",
        "    type='TwoStageInpaintor',\n",
        "    disc_input_with_mask=True,\n",
        "    encdec=dict(\n",
        "        type='DeepFillEncoderDecoder',\n",
        "        stage1=dict(\n",
        "            type='GLEncoderDecoder',\n",
        "            encoder=dict(\n",
        "                type='DeepFillEncoder',\n",
        "                conv_type='gated_conv',\n",
        "                channel_factor=0.75,\n",
        "                padding_mode='reflect'),\n",
        "            decoder=dict(\n",
        "                type='DeepFillDecoder',\n",
        "                conv_type='gated_conv',\n",
        "                in_channels=96,\n",
        "                channel_factor=0.75,\n",
        "                out_act_cfg=dict(type='Tanh'),\n",
        "                padding_mode='reflect'),\n",
        "            dilation_neck=dict(\n",
        "                type='GLDilationNeck',\n",
        "                in_channels=96,\n",
        "                conv_type='gated_conv',\n",
        "                act_cfg=dict(type='ELU'),\n",
        "                padding_mode='reflect')),\n",
        "        stage2=dict(\n",
        "            type='DeepFillRefiner',\n",
        "            encoder_attention=dict(\n",
        "                type='DeepFillEncoder',\n",
        "                encoder_type='stage2_attention',\n",
        "                conv_type='gated_conv',\n",
        "                channel_factor=0.75,\n",
        "                padding_mode='reflect'),\n",
        "            encoder_conv=dict(\n",
        "                type='DeepFillEncoder',\n",
        "                encoder_type='stage2_conv',\n",
        "                conv_type='gated_conv',\n",
        "                channel_factor=0.75,\n",
        "                padding_mode='reflect'),\n",
        "            dilation_neck=dict(\n",
        "                type='GLDilationNeck',\n",
        "                in_channels=96,\n",
        "                conv_type='gated_conv',\n",
        "                act_cfg=dict(type='ELU'),\n",
        "                padding_mode='reflect'),\n",
        "            contextual_attention=dict(\n",
        "                type='ContextualAttentionNeck',\n",
        "                in_channels=96,\n",
        "                conv_type='gated_conv',\n",
        "                padding_mode='reflect'),\n",
        "            decoder=dict(\n",
        "                type='DeepFillDecoder',\n",
        "                in_channels=192,\n",
        "                conv_type='gated_conv',\n",
        "                out_act_cfg=dict(type='Tanh'),\n",
        "                padding_mode='reflect'))),\n",
        "    disc=dict(\n",
        "        # 'GLDiscs', 'ModifiedVGG', 'MultiLayerDiscriminator', 'DeepFillv1Discriminators', 'PatchDiscriminator'\n",
        "        type='MultiLayerDiscriminator',\n",
        "        in_channels=4,\n",
        "        max_channels=256,\n",
        "        fc_in_channels=None,\n",
        "        num_convs=6,\n",
        "        norm_cfg=None,\n",
        "        act_cfg=dict(type='LeakyReLU', negative_slope=0.2),\n",
        "        out_act_cfg=dict(type='LeakyReLU', negative_slope=0.2),\n",
        "        with_spectral_norm=True,\n",
        "    ),\n",
        "    # loss_gan, percep, tv, l1, HFEN, Elastic, RelativeL1, L1CosineSim, ClipL1, FFT, OF, GP, CP\n",
        "    # loss_l1_hole, loss_l1_valid\n",
        "\n",
        "    # default\n",
        "    # stage1_loss_type=('loss_l1_hole', 'loss_l1_valid'),\n",
        "    # stage2_loss_type=('loss_l1_hole', 'loss_l1_valid', 'loss_gan'),\n",
        "    stage1_loss_type=('loss_l1_hole', 'loss_l1_valid'),\n",
        "    stage2_loss_type=('loss_l1_hole', 'loss_l1_valid', 'loss_gan', 'HFEN'),\n",
        "    loss_gan=dict(\n",
        "        type='GANLoss',\n",
        "        gan_type='hinge',\n",
        "        loss_weight=0.1,\n",
        "    ),\n",
        "    loss_l1_hole=dict(\n",
        "        type='L1Loss',\n",
        "        loss_weight=1.0,\n",
        "    ),\n",
        "    loss_l1_valid=dict(\n",
        "        type='L1Loss',\n",
        "        loss_weight=1.0,\n",
        "    ),\n",
        "    pretrained=None)\n",
        "\n",
        "train_cfg = dict(disc_step=1)\n",
        "test_cfg = dict(metrics=['l1', 'psnr', 'ssim'])\n",
        "\n",
        "dataset_type = 'ImgInpaintingDataset'\n",
        "input_shape = (256, 256)\n",
        "\n",
        "train_pipeline = [\n",
        "    dict(type='LoadImageFromFile', key='gt_img'),\n",
        "    dict(\n",
        "        type='LoadMask',\n",
        "        mask_mode='irregular',\n",
        "        mask_config=dict(\n",
        "            num_vertexes=(4, 10),\n",
        "            max_angle=6.0,\n",
        "            length_range=(20, 128),\n",
        "            brush_width=(10, 45),\n",
        "            area_ratio_range=(0.15, 0.65),\n",
        "            img_shape=input_shape)),\n",
        "    dict(\n",
        "        type='Crop',\n",
        "        keys=['gt_img'],\n",
        "        crop_size=(384, 384),\n",
        "        random_crop=True,\n",
        "    ),\n",
        "    dict(\n",
        "        type='Resize',\n",
        "        keys=['gt_img'],\n",
        "        scale=input_shape,\n",
        "        keep_ratio=False,\n",
        "    ),\n",
        "    dict(\n",
        "        type='Normalize',\n",
        "        keys=['gt_img'],\n",
        "        mean=[127.5] * 3,\n",
        "        std=[127.5] * 3,\n",
        "        to_rgb=False),\n",
        "    dict(type='GetMaskedImage'),\n",
        "    dict(\n",
        "        type='Collect',\n",
        "        keys=['gt_img', 'masked_img', 'mask'],\n",
        "        meta_keys=['gt_img_path']),\n",
        "    dict(type='ImageToTensor', keys=['gt_img', 'masked_img', 'mask'])\n",
        "]\n",
        "\n",
        "test_pipeline = train_pipeline\n",
        "\n",
        "data_root = '/content/data'\n",
        "\n",
        "data = dict(\n",
        "    samples_per_gpu=2,\n",
        "    workers_per_gpu=8,\n",
        "    val_samples_per_gpu=1,\n",
        "    val_workers_per_gpu=8,\n",
        "    drop_last=True,\n",
        "    train=dict(\n",
        "        type=dataset_type,\n",
        "        ann_file='/content/train/train.tflist',\n",
        "        data_prefix=data_root,\n",
        "        pipeline=train_pipeline,\n",
        "        test_mode=False),\n",
        "    val=dict(\n",
        "        type=dataset_type,\n",
        "        ann_file='/content/val/val.tflist',\n",
        "        data_prefix=data_root,\n",
        "        pipeline=test_pipeline,\n",
        "        test_mode=True),\n",
        "    test=dict(\n",
        "        type=dataset_type,\n",
        "        ann_file='/content/val/val.tflist',\n",
        "        data_prefix=data_root,\n",
        "        pipeline=test_pipeline,\n",
        "        test_mode=True))\n",
        "\n",
        "optimizers = dict(\n",
        "    generator=dict(type='Adam', lr=0.0001), disc=dict(type='Adam', lr=0.0001))\n",
        "\n",
        "lr_config = dict(policy='Fixed', by_epoch=False)\n",
        "\n",
        "checkpoint_config = dict(by_epoch=False, interval=1000)\n",
        "log_config = dict(\n",
        "    interval=100,\n",
        "    hooks=[\n",
        "        dict(type='TextLoggerHook', by_epoch=False),\n",
        "        dict(type='TensorboardLoggerHook'),\n",
        "        #dict(type='PaviLoggerHook', init_kwargs=dict(project='mmedit')) # not possible to install\n",
        "    ])\n",
        "\n",
        "visual_config = dict(\n",
        "    type='VisualizationHook',\n",
        "    output_dir='visual',\n",
        "    interval=1000,\n",
        "    res_name_list=[\n",
        "        'gt_img', 'masked_img', 'stage1_fake_res', 'stage1_fake_img',\n",
        "        'stage2_fake_res', 'stage2_fake_img', 'fake_gt_local'\n",
        "    ],\n",
        ")\n",
        "\n",
        "evaluation = dict(interval=50000)\n",
        "\n",
        "total_iters = 1000003\n",
        "dist_params = dict(backend='nccl')\n",
        "log_level = 'INFO'\n",
        "work_dir = './work_dirs/test_pggan'\n",
        "load_from = None\n",
        "resume_from = None\n",
        "workflow = [('train', 10000)]\n",
        "exp_name = 'deepfillv2_256x256_8x2_places'\n",
        "find_unused_parameters = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFEY1_skGs-W",
        "cellView": "form"
      },
      "source": [
        "#@title custom.py (new loss) (pconv)\n",
        "%%writefile /content/Colab-mmediting/configs/inpainting/partial_conv/custom.py\n",
        "\n",
        "\n",
        "model = dict(\n",
        "    type='PConvInpaintor',\n",
        "    encdec=dict(\n",
        "        type='PConvEncoderDecoder',\n",
        "        encoder=dict(\n",
        "            type='PConvEncoder',\n",
        "            norm_cfg=dict(type='SyncBN', requires_grad=False),\n",
        "            norm_eval=True),\n",
        "        decoder=dict(type='PConvDecoder', norm_cfg=dict(type='SyncBN'))),\n",
        "    disc=None,\n",
        "    loss_composed_percep=dict(\n",
        "        type='PerceptualLoss',\n",
        "        vgg_type='vgg16',\n",
        "        layer_weights={\n",
        "            '4': 1.,\n",
        "            '9': 1.,\n",
        "            '16': 1.,\n",
        "        },\n",
        "        perceptual_weight=0.05,\n",
        "        style_weight=120,\n",
        "        pretrained=('torchvision://vgg16')),\n",
        "    loss_out_percep=True,\n",
        "    loss_l1_hole=dict(\n",
        "        type='L1Loss',\n",
        "        loss_weight=6.,\n",
        "    ),\n",
        "    loss_l1_valid=dict(\n",
        "        type='L1Loss',\n",
        "        loss_weight=1.,\n",
        "    ),\n",
        "    loss_tv=dict(\n",
        "        type='MaskedTVLoss',\n",
        "        loss_weight=0.1,\n",
        "    ),\n",
        "\n",
        "    #loss_composed_percep=dict(\n",
        "    #type='PerceptualLoss',\n",
        "    #layer_weights={'0': 1.},\n",
        "    #perceptual_weight=0.1,\n",
        "    #style_weight=0,\n",
        "    #),\n",
        "\n",
        "    #loss_out_percep=True,\n",
        "\n",
        "    #loss_gan=dict(\n",
        "    #    type='GANLoss',\n",
        "    #    gan_type='hinge',\n",
        "    #    loss_weight=0.1,\n",
        "    #),\n",
        "\n",
        "    #loss_gp=dict(type='GradientPenaltyLoss', loss_weight=10.),\n",
        "\n",
        "\n",
        "    #loss_disc_shift=dict(type='DiscShiftLoss', loss_weight=0.001),\n",
        "\n",
        "    pretrained=None)\n",
        "\n",
        "train_cfg = dict(disc_step=0)\n",
        "test_cfg = dict(metrics=['l1', 'psnr', 'ssim'])\n",
        "\n",
        "dataset_type = 'ImgInpaintingDataset'\n",
        "input_shape = (256, 256)\n",
        "\n",
        "train_pipeline = [\n",
        "    dict(type='LoadImageFromFile', key='gt_img'),\n",
        "    dict(\n",
        "        type='LoadMask',\n",
        "        mask_mode='irregular',\n",
        "        mask_config=dict(\n",
        "            num_vertexes=(4, 10),\n",
        "            max_angle=6.0,\n",
        "            length_range=(20, 128),\n",
        "            brush_width=(10, 45),\n",
        "            area_ratio_range=(0.15, 0.65),\n",
        "            img_shape=input_shape)),\n",
        "    dict(\n",
        "        type='Crop',\n",
        "        keys=['gt_img'],\n",
        "        crop_size=(384, 384),\n",
        "        random_crop=True,\n",
        "    ),\n",
        "    dict(\n",
        "        type='Resize',\n",
        "        keys=['gt_img'],\n",
        "        scale=input_shape,\n",
        "        keep_ratio=False,\n",
        "    ),\n",
        "    dict(\n",
        "        type='Normalize',\n",
        "        keys=['gt_img'],\n",
        "        mean=[127.5] * 3,\n",
        "        std=[127.5] * 3,\n",
        "        to_rgb=False),\n",
        "    dict(type='GetMaskedImage'),\n",
        "    dict(\n",
        "        type='Collect',\n",
        "        keys=['gt_img', 'masked_img', 'mask'],\n",
        "        meta_keys=['gt_img_path']),\n",
        "    dict(type='ImageToTensor', keys=['gt_img', 'masked_img', 'mask'])\n",
        "]\n",
        "\n",
        "test_pipeline = train_pipeline\n",
        "\n",
        "data_root = './data/CelebA-HQ/'\n",
        "\n",
        "data = dict(\n",
        "    samples_per_gpu=1,\n",
        "    workers_per_gpu=1,\n",
        "    val_samples_per_gpu=1,\n",
        "    val_workers_per_gpu=1,\n",
        "    train_drop_last=True,\n",
        "    val_drop_last=True,\n",
        "    train=dict(\n",
        "        type=dataset_type,\n",
        "        ann_file='/content/train/train.tflist',\n",
        "        data_prefix=data_root,\n",
        "        pipeline=train_pipeline,\n",
        "        test_mode=False),\n",
        "    val=dict(\n",
        "        type=dataset_type,\n",
        "        ann_file='/content/val/val.tflist',\n",
        "        data_prefix=data_root,\n",
        "        pipeline=test_pipeline,\n",
        "        test_mode=True),\n",
        "    test=dict(\n",
        "        type=dataset_type,\n",
        "        ann_file='/content/val/val.tflist',\n",
        "        data_prefix=data_root,\n",
        "        pipeline=test_pipeline,\n",
        "        test_mode=True))\n",
        "\n",
        "optimizers = dict(generator=dict(type='Adam',\n",
        "                                 lr=0.00005))  # second stage training\n",
        "\n",
        "lr_config = dict(policy='Fixed', by_epoch=False)\n",
        "\n",
        "checkpoint_config = dict(by_epoch=False, interval=50000)\n",
        "log_config = dict(\n",
        "    interval=100,\n",
        "    hooks=[\n",
        "        dict(type='TextLoggerHook', ),\n",
        "        dict(type='TensorboardLoggerHook'),\n",
        "        #dict(type='PaviLoggerHook', init_kwargs=dict(project='mmedit'))\n",
        "    ])\n",
        "\n",
        "visual_config = dict(\n",
        "    type='VisualizationHook',\n",
        "    output_dir='visual',\n",
        "    interval=1000,\n",
        "    res_name_list=['gt_img', 'masked_img', 'fake_res', 'fake_img'],\n",
        ")\n",
        "\n",
        "evaluation = dict(\n",
        "    interval=50000,\n",
        "    metric_dict=dict(l1=dict()),\n",
        ")\n",
        "\n",
        "total_iters = 300002\n",
        "dist_params = dict(backend='nccl')\n",
        "log_level = 'INFO'\n",
        "work_dir = './work_dirs/test_pggan'\n",
        "load_from = None\n",
        "resume_from = None\n",
        "workflow = [('train', 10000)]\n",
        "exp_name = 'pconv_256x256_stage2_4x2_celeba'\n",
        "find_unused_parameters = False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyYsI4ssRqp4",
        "cellView": "form"
      },
      "source": [
        "#@title Train (deepfillv2)\n",
        "%cd /content/Colab-mmediting\n",
        "!python tools/train.py configs/inpainting/deepfillv2/custom.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V5fdFUkEvVE",
        "cellView": "form"
      },
      "source": [
        "#@title Train pconv (not working)\n",
        "%cd /content/Colab-mmediting\n",
        "!python tools/train.py configs/inpainting/partial_conv/custom.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3rY96332zAA"
      },
      "source": [
        "# Test trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3biQzN2PzwZP"
      },
      "source": [
        "!python demo/inpainting_demo.py \"configs/inpainting/deepfillv2/custom.py\" \\\n",
        "\"/content/mmediting/work_dirs/test_pggan/iter_XXXXX.pth\" /content/image.png /content/mask.png /content/result.png"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
