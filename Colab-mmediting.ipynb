{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab-mmediting.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5REUXZI7ejq6"
      },
      "source": [
        "# Colab-mmediting with [open-mmlab/mmediting](https://github.com/open-mmlab/mmediting)\n",
        "\n",
        "My fork is located in [styler00dollar/Colab-mmediting](https://github.com/styler00dollar/Colab-mmediting)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7SvGgIHiNCF",
        "cellView": "form"
      },
      "source": [
        "#@title Check GPU\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pq-pTiD6LHx3",
        "cellView": "form"
      },
      "source": [
        "#@title Install mmediting reqruirements (installing mmcv takes quite long)\n",
        "%cd /content/\n",
        "!git clone https://github.com/styler00dollar/Colab-mmediting\n",
        "%cd Colab-mmediting\n",
        "!pip install -r requirements.txt\n",
        "!pip install -v -e .  # or \"python setup.py develop\"\n",
        "!pip install tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6giPJBPbb4JU"
      },
      "source": [
        "# Test inpainting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5To_7PcbdNm",
        "cellView": "form"
      },
      "source": [
        "#@title Download all models\n",
        "%cd /content/\n",
        "!mkdir /content/models/\n",
        "# deepfillv2\n",
        "!wget --no-check-certificate 'https://openmmlab.oss-accelerate.aliyuncs.com/mmediting/inpainting/deepfillv2/deepfillv2_256x256_8x2_places_20200619-10d15793.pth' -O /content/models/deepfillv2_256x256_8x2_places.pth\n",
        "!wget --no-check-certificate 'https://openmmlab.oss-accelerate.aliyuncs.com/mmediting/inpainting/deepfillv2/deepfillv2_256x256_8x2_celeba_20200619-c96e5f12.pth' -O /content/models/deepfillv2_256x256_8x2_celeba.pth\n",
        "# deepfillv1\n",
        "!wget --no-check-certificate 'https://openmmlab.oss-accelerate.aliyuncs.com/mmediting/inpainting/deepfillv1/deepfillv1_256x256_4x4_celeba_20200619-dd51a855.pth' -O /content/models/deepfillv1_256x256_4x4_celeba.pth\n",
        "!wget --no-check-certificate 'https://openmmlab.oss-accelerate.aliyuncs.com/mmediting/inpainting/deepfillv1/deepfillv1_256x256_8x2_places_20200619-c00a0e21.pth' -O /content/models/deepfillv1_256x256_8x2_places.pth\n",
        "# global_local\n",
        "!wget --no-check-certificate 'https://openmmlab.oss-accelerate.aliyuncs.com/mmediting/inpainting/global_local/gl_256x256_8x12_celeba_20200619-5af0493f.pth' -O /content/models/gl_256x256_8x12_celeba.pth\n",
        "!wget --no-check-certificate 'https://openmmlab.oss-accelerate.aliyuncs.com/mmediting/inpainting/global_local/gl_256x256_8x12_places_20200619-52a040a8.pth' -O /content/models/gl_256x256_8x12_places.pth\n",
        "# partial_conv\n",
        "!wget --no-check-certificate 'https://openmmlab.oss-accelerate.aliyuncs.com/mmediting/inpainting/pconv/pconv_256x256_stage2_4x2_places_20200619-1ffed0e8.pth' -O /content/models/pconv_places.pth\n",
        "!wget --no-check-certificate 'https://openmmlab.oss-accelerate.aliyuncs.com/mmediting/inpainting/pconv/pconv_256x256_stage2_4x2_celeba_20200619-860f8b95.pth' -O /content/models/pconv_celeba.pth\n",
        "%cd /content/Colab-mmediting"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGA8wwgbcIvt"
      },
      "source": [
        "Place pictures in ```/content/image.png``` and ```/content/mask.png ```. Result will be in ```/content/result.png```.\n",
        "\n",
        "Info: The mask is a black white image and the masked area is white. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXP0lp7pg7Pv",
        "cellView": "form"
      },
      "source": [
        "#@title Connect Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "print('Google Drive connected.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfbkE2oVemqf"
      },
      "source": [
        "# deepfillv2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yLpR2sBOVA-",
        "cellView": "form"
      },
      "source": [
        "#@title deepfill v2 places\n",
        "!python demo/inpainting_demo.py configs/inpainting/deepfillv2/deepfillv2_256x256_8x2_places.py /content/models/deepfillv2_256x256_8x2_places.pth /content/image.png /content/mask.png /content/result.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKR7CROqc1lP",
        "cellView": "form"
      },
      "source": [
        "#@title deepfill v2 celeba\n",
        "!python demo/inpainting_demo.py configs/inpainting/deepfillv2/deepfillv2_256x256_8x2_celeba.py /content/models/deepfillv2_256x256_8x2_celeba.pth /content/image.png /content/mask.png /content/result.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MJLf_sWeo7z"
      },
      "source": [
        "# deepfillv1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCl0oMrGc_Sw",
        "cellView": "form"
      },
      "source": [
        "#@title deepfill v1 places\n",
        "!python demo/inpainting_demo.py configs/inpainting/deepfillv1/deepfillv1_256x256_8x2_places.py /content/models/deepfillv1_256x256_8x2_places.pth /content/image.png /content/mask.png /content/result.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMMd4hB6dIOs",
        "cellView": "form"
      },
      "source": [
        "#@title deepfill v1 celeba\n",
        "!python demo/inpainting_demo.py configs/inpainting/deepfillv1/deepfillv1_256x256_4x4_celeba.py /content/models/deepfillv1_256x256_4x4_celeba.pth /content/image.png /content/mask.png /content/result.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7CdL58mer_5"
      },
      "source": [
        "# global_local"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBpo1ZQUdPCv",
        "cellView": "form"
      },
      "source": [
        "#@title global_local places\n",
        "!python demo/inpainting_demo.py configs/inpainting/global_local/gl_256x256_8x12_places.py /content/models/gl_256x256_8x12_places.pth /content/image.png /content/mask.png /content/result.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrhWrbA8dXQl",
        "cellView": "form"
      },
      "source": [
        "#@title global_local celeba\n",
        "!python demo/inpainting_demo.py configs/inpainting/global_local/gl_256x256_8x12_celeba.py /content/models/gl_256x256_8x12_celeba.pth /content/image.png /content/mask.png /content/result.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3YN4oS27F9v"
      },
      "source": [
        "# partial_conv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4X_FZSaFeu8Q"
      },
      "source": [
        "partial_conv (Does not accept any image dimension as input. ```250x250, 256x256, 500x500, 501x501, 502x502, 503x503, 504x504, 505x505, 506x506, 507x507, 508x508, 509x509, 510x510, 511x511, 512x512, 624x624, 750x750, 1000x1000, 1001x1001, 1000x1250, 1250x1250, 1256x1256 and 1512x1512``` seem to work. ```180x180, 192x192, 201x201, 325x325, 400x400, 425x425, 444x444, 480x480, 513x513, 514x514, 515x515, 516x516, 517x517, 518x518, 520x520, 524x524, 555x555, 569x569,  666x666, 724x724, 800x800, 812x812, 1500x1500 and 2000x2000``` does not work. I can't really see a pattern.)\n",
        "\n",
        "[Problematic code is here.](https://github.com/open-mmlab/mmediting/blob/master/mmedit/models/backbones/encoder_decoders/decoders/pconv_decoder.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJWS5f7RdwJK"
      },
      "source": [
        "# resize input if needed\n",
        "import cv2\n",
        "filepath = '/content/image.png'\n",
        "size = (666,666)\n",
        "\n",
        "image = cv2.imread(filepath)\n",
        "image = cv2.resize(image, size, cv2.INTER_NEAREST)\n",
        "cv2.imwrite(filepath, image)\n",
        "filepath = '/content/mask.png'\n",
        "image = cv2.imread(filepath)\n",
        "image = cv2.resize(image, size, cv2.INTER_NEAREST)\n",
        "cv2.imwrite(filepath, image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2dCAUN9dipe",
        "cellView": "form"
      },
      "source": [
        "#@title partial_conv 8x1 places\n",
        "!python demo/inpainting_demo.py configs/inpainting/partial_conv/pconv_256x256_stage1_8x1_places.py /content/models/pconv_places.pth /content/image.png /content/mask.png /content/result.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYUx3xlFdepC",
        "cellView": "form"
      },
      "source": [
        "#@title partial_conv 8x1 celeba\n",
        "!python demo/inpainting_demo.py configs/inpainting/partial_conv/pconv_256x256_stage1_8x1_celeba.py /content/models/pconv_celeba.pth /content/image.png /content/mask.png /content/result.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJ2995x-e4pc",
        "cellView": "form"
      },
      "source": [
        "#@title partial_conv 4x2 places\n",
        "!python demo/inpainting_demo.py configs/inpainting/partial_conv/pconv_256x256_stage2_4x2_places.py /content/models/pconv_places.pth /content/image.png /content/mask.png /content/result.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TglLOv2Me-OC",
        "cellView": "form"
      },
      "source": [
        "#@title partial_conv 4x2 celeba\n",
        "!python demo/inpainting_demo.py configs/inpainting/partial_conv/pconv_256x256_stage2_4x2_celeba.py /content/models/pconv_celeba.pth /content/image.png /content/mask.png /content/result.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY9giPLf2nak"
      },
      "source": [
        "# Training deepfillv2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLWkPZpTvg_8",
        "cellView": "both"
      },
      "source": [
        "#@title create empty folders\n",
        "!mkdir /content/train/\n",
        "!mkdir /content/val/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Il9CqQQ2mun",
        "cellView": "form"
      },
      "source": [
        "#@title custom.py (contains configuration for paths and parameter)\n",
        "%%writefile /content/Colab-mmediting/configs/inpainting/deepfillv2/custom.py\n",
        "model = dict(\n",
        "    type='TwoStageInpaintor',\n",
        "    disc_input_with_mask=True,\n",
        "    encdec=dict(\n",
        "        type='DeepFillEncoderDecoder',\n",
        "        stage1=dict(\n",
        "            type='GLEncoderDecoder',\n",
        "            encoder=dict(\n",
        "                type='DeepFillEncoder',\n",
        "                conv_type='gated_conv',\n",
        "                channel_factor=0.75,\n",
        "                padding_mode='reflect'),\n",
        "            decoder=dict(\n",
        "                type='DeepFillDecoder',\n",
        "                conv_type='gated_conv',\n",
        "                in_channels=96,\n",
        "                channel_factor=0.75,\n",
        "                out_act_cfg=dict(type='Tanh'),\n",
        "                padding_mode='reflect'),\n",
        "            dilation_neck=dict(\n",
        "                type='GLDilationNeck',\n",
        "                in_channels=96,\n",
        "                conv_type='gated_conv',\n",
        "                act_cfg=dict(type='ELU'),\n",
        "                padding_mode='reflect')),\n",
        "        stage2=dict(\n",
        "            type='DeepFillRefiner',\n",
        "            encoder_attention=dict(\n",
        "                type='DeepFillEncoder',\n",
        "                encoder_type='stage2_attention',\n",
        "                conv_type='gated_conv',\n",
        "                channel_factor=0.75,\n",
        "                padding_mode='reflect'),\n",
        "            encoder_conv=dict(\n",
        "                type='DeepFillEncoder',\n",
        "                encoder_type='stage2_conv',\n",
        "                conv_type='gated_conv',\n",
        "                channel_factor=0.75,\n",
        "                padding_mode='reflect'),\n",
        "            dilation_neck=dict(\n",
        "                type='GLDilationNeck',\n",
        "                in_channels=96,\n",
        "                conv_type='gated_conv',\n",
        "                act_cfg=dict(type='ELU'),\n",
        "                padding_mode='reflect'),\n",
        "            contextual_attention=dict(\n",
        "                type='ContextualAttentionNeck',\n",
        "                in_channels=96,\n",
        "                conv_type='gated_conv',\n",
        "                padding_mode='reflect'),\n",
        "            decoder=dict(\n",
        "                type='DeepFillDecoder',\n",
        "                in_channels=192,\n",
        "                conv_type='gated_conv',\n",
        "                out_act_cfg=dict(type='Tanh'),\n",
        "                padding_mode='reflect'))),\n",
        "    disc=dict(\n",
        "        type='MultiLayerDiscriminator',\n",
        "        in_channels=4,\n",
        "        max_channels=256,\n",
        "        fc_in_channels=None,\n",
        "        num_convs=6,\n",
        "        norm_cfg=None,\n",
        "        act_cfg=dict(type='LeakyReLU', negative_slope=0.2),\n",
        "        out_act_cfg=dict(type='LeakyReLU', negative_slope=0.2),\n",
        "        with_spectral_norm=True,\n",
        "    ),\n",
        "    stage1_loss_type=('loss_l1_hole', 'loss_l1_valid'),\n",
        "    stage2_loss_type=('loss_l1_hole', 'loss_l1_valid', 'loss_gan'),\n",
        "    loss_gan=dict(\n",
        "        type='GANLoss',\n",
        "        gan_type='hinge',\n",
        "        loss_weight=0.1,\n",
        "    ),\n",
        "    loss_l1_hole=dict(\n",
        "        type='L1Loss',\n",
        "        loss_weight=1.0,\n",
        "    ),\n",
        "    loss_l1_valid=dict(\n",
        "        type='L1Loss',\n",
        "        loss_weight=1.0,\n",
        "    ),\n",
        "    pretrained=None)\n",
        "\n",
        "train_cfg = dict(disc_step=1)\n",
        "test_cfg = dict(metrics=['l1', 'psnr', 'ssim'])\n",
        "\n",
        "dataset_type = 'ImgInpaintingDataset'\n",
        "input_shape = (256, 256)\n",
        "\n",
        "train_pipeline = [\n",
        "    dict(type='LoadImageFromFile', key='gt_img'),\n",
        "    dict(\n",
        "        type='LoadMask',\n",
        "        mask_mode='irregular',\n",
        "        mask_config=dict(\n",
        "            num_vertexes=(4, 10),\n",
        "            max_angle=6.0,\n",
        "            length_range=(20, 128),\n",
        "            brush_width=(10, 45),\n",
        "            area_ratio_range=(0.15, 0.65),\n",
        "            img_shape=input_shape)),\n",
        "    dict(\n",
        "        type='Crop',\n",
        "        keys=['gt_img'],\n",
        "        crop_size=(384, 384),\n",
        "        random_crop=True,\n",
        "    ),\n",
        "    dict(\n",
        "        type='Resize',\n",
        "        keys=['gt_img'],\n",
        "        scale=input_shape,\n",
        "        keep_ratio=False,\n",
        "    ),\n",
        "    dict(\n",
        "        type='Normalize',\n",
        "        keys=['gt_img'],\n",
        "        mean=[127.5] * 3,\n",
        "        std=[127.5] * 3,\n",
        "        to_rgb=False),\n",
        "    dict(type='GetMaskedImage'),\n",
        "    dict(\n",
        "        type='Collect',\n",
        "        keys=['gt_img', 'masked_img', 'mask'],\n",
        "        meta_keys=['gt_img_path']),\n",
        "    dict(type='ImageToTensor', keys=['gt_img', 'masked_img', 'mask'])\n",
        "]\n",
        "\n",
        "test_pipeline = train_pipeline\n",
        "\n",
        "data_root = '/content/data'\n",
        "\n",
        "data = dict(\n",
        "    samples_per_gpu=2,\n",
        "    workers_per_gpu=8,\n",
        "    val_samples_per_gpu=1,\n",
        "    val_workers_per_gpu=8,\n",
        "    drop_last=True,\n",
        "    train=dict(\n",
        "        type=dataset_type,\n",
        "        ann_file='/content/train/train.tflist',\n",
        "        data_prefix=data_root,\n",
        "        pipeline=train_pipeline,\n",
        "        test_mode=False),\n",
        "    val=dict(\n",
        "        type=dataset_type,\n",
        "        ann_file='/content/val/val.tflist',\n",
        "        data_prefix=data_root,\n",
        "        pipeline=test_pipeline,\n",
        "        test_mode=True),\n",
        "    test=dict(\n",
        "        type=dataset_type,\n",
        "        ann_file='/content/val/val.tflist',\n",
        "        data_prefix=data_root,\n",
        "        pipeline=test_pipeline,\n",
        "        test_mode=True))\n",
        "\n",
        "optimizers = dict(\n",
        "    generator=dict(type='Adam', lr=0.0001), disc=dict(type='Adam', lr=0.0001))\n",
        "\n",
        "lr_config = dict(policy='Fixed', by_epoch=False)\n",
        "\n",
        "checkpoint_config = dict(by_epoch=False, interval=1000)\n",
        "log_config = dict(\n",
        "    interval=100,\n",
        "    hooks=[\n",
        "        dict(type='TextLoggerHook', by_epoch=False),\n",
        "        dict(type='TensorboardLoggerHook'),\n",
        "        #dict(type='PaviLoggerHook', init_kwargs=dict(project='mmedit'))\n",
        "    ])\n",
        "\n",
        "visual_config = dict(\n",
        "    type='VisualizationHook',\n",
        "    output_dir='visual',\n",
        "    interval=1000,\n",
        "    res_name_list=[\n",
        "        'gt_img', 'masked_img', 'stage1_fake_res', 'stage1_fake_img',\n",
        "        'stage2_fake_res', 'stage2_fake_img', 'fake_gt_local'\n",
        "    ],\n",
        ")\n",
        "\n",
        "evaluation = dict(interval=50000)\n",
        "\n",
        "total_iters = 1000003\n",
        "dist_params = dict(backend='nccl')\n",
        "log_level = 'INFO'\n",
        "work_dir = './work_dirs/test_pggan'\n",
        "load_from = None\n",
        "resume_from = None\n",
        "workflow = [('train', 10000)]\n",
        "exp_name = 'deepfillv2_256x256_8x2_places'\n",
        "find_unused_parameters = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-dU6DgA-Nqx",
        "cellView": "form"
      },
      "source": [
        "#@title removing env info, since it creates a problem (train.py)\n",
        "%%writefile /content/Colab-mmediting/tools/train.py\n",
        "import argparse\n",
        "import copy\n",
        "import os\n",
        "import os.path as osp\n",
        "import time\n",
        "\n",
        "import mmcv\n",
        "import torch\n",
        "from mmcv import Config\n",
        "from mmcv.runner import init_dist\n",
        "\n",
        "from mmedit import __version__\n",
        "from mmedit.apis import set_random_seed, train_model\n",
        "from mmedit.datasets import build_dataset\n",
        "from mmedit.models import build_model\n",
        "from mmedit.utils import collect_env, get_root_logger\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description='Train an editor')\n",
        "    parser.add_argument('config', help='train config file path')\n",
        "    parser.add_argument('--work-dir', help='the dir to save logs and models')\n",
        "    parser.add_argument(\n",
        "        '--resume-from', help='the checkpoint file to resume from')\n",
        "    parser.add_argument(\n",
        "        '--no-validate',\n",
        "        action='store_true',\n",
        "        help='whether not to evaluate the checkpoint during training')\n",
        "    parser.add_argument(\n",
        "        '--gpus',\n",
        "        type=int,\n",
        "        default=1,\n",
        "        help='number of gpus to use '\n",
        "        '(only applicable to non-distributed training)')\n",
        "    parser.add_argument('--seed', type=int, default=None, help='random seed')\n",
        "    parser.add_argument(\n",
        "        '--deterministic',\n",
        "        action='store_true',\n",
        "        help='whether to set deterministic options for CUDNN backend.')\n",
        "    parser.add_argument(\n",
        "        '--launcher',\n",
        "        choices=['none', 'pytorch', 'slurm', 'mpi'],\n",
        "        default='none',\n",
        "        help='job launcher')\n",
        "    parser.add_argument('--local_rank', type=int, default=0)\n",
        "    parser.add_argument(\n",
        "        '--autoscale-lr',\n",
        "        action='store_true',\n",
        "        help='automatically scale lr with the number of gpus')\n",
        "    args = parser.parse_args()\n",
        "    if 'LOCAL_RANK' not in os.environ:\n",
        "        os.environ['LOCAL_RANK'] = str(args.local_rank)\n",
        "\n",
        "    return args\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "\n",
        "    cfg = Config.fromfile(args.config)\n",
        "    # set cudnn_benchmark\n",
        "    if cfg.get('cudnn_benchmark', False):\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "    # update configs according to CLI args\n",
        "    if args.work_dir is not None:\n",
        "        cfg.work_dir = args.work_dir\n",
        "    if args.resume_from is not None:\n",
        "        cfg.resume_from = args.resume_from\n",
        "    cfg.gpus = args.gpus\n",
        "\n",
        "    if args.autoscale_lr:\n",
        "        # apply the linear scaling rule (https://arxiv.org/abs/1706.02677)\n",
        "        cfg.optimizer['lr'] = cfg.optimizer['lr'] * cfg.gpus / 8\n",
        "\n",
        "    # init distributed env first, since logger depends on the dist info.\n",
        "    if args.launcher == 'none':\n",
        "        distributed = False\n",
        "    else:\n",
        "        distributed = True\n",
        "        init_dist(args.launcher, **cfg.dist_params)\n",
        "\n",
        "    # create work_dir\n",
        "    mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
        "    # init the logger before other steps\n",
        "    timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())\n",
        "    log_file = osp.join(cfg.work_dir, f'{timestamp}.log')\n",
        "    logger = get_root_logger(log_file=log_file, log_level=cfg.log_level)\n",
        "\n",
        "    # log env info\n",
        "    #env_info_dict = collect_env()\n",
        "    #env_info = '\\n'.join([f'{k}: {v}' for k, v in env_info_dict.items()])\n",
        "    #dash_line = '-' * 60 + '\\n'\n",
        "    #logger.info('Environment info:\\n' + dash_line + env_info + '\\n' +\n",
        "    #            dash_line)\n",
        "\n",
        "    # log some basic info\n",
        "    logger.info('Distributed training: {}'.format(distributed))\n",
        "    logger.info('mmedit Version: {}'.format(__version__))\n",
        "    logger.info('Config:\\n{}'.format(cfg.text))\n",
        "\n",
        "    # set random seeds\n",
        "    if args.seed is not None:\n",
        "        logger.info('Set random seed to {}, deterministic: {}'.format(\n",
        "            args.seed, args.deterministic))\n",
        "        set_random_seed(args.seed, deterministic=args.deterministic)\n",
        "    cfg.seed = args.seed\n",
        "\n",
        "    model = build_model(\n",
        "        cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
        "\n",
        "    datasets = [build_dataset(cfg.data.train)]\n",
        "    if len(cfg.workflow) == 2:\n",
        "        val_dataset = copy.deepcopy(cfg.data.val)\n",
        "        val_dataset.pipeline = cfg.data.train.pipeline\n",
        "        datasets.append(build_dataset(val_dataset))\n",
        "    if cfg.checkpoint_config is not None:\n",
        "        # save version, config file content and class names in\n",
        "        # checkpoints as meta data\n",
        "        cfg.checkpoint_config.meta = dict(\n",
        "            mmedit_version=__version__,\n",
        "            config=cfg.text,\n",
        "        )\n",
        "\n",
        "    # meta information\n",
        "    meta = dict()\n",
        "    if cfg.get('exp_name', None) is None:\n",
        "        cfg['exp_name'] = osp.splitext(osp.basename(cfg.work_dir))[0]\n",
        "    meta['exp_name'] = cfg.exp_name\n",
        "    meta['mmedit Version'] = __version__\n",
        "    meta['seed'] = args.seed\n",
        "    #meta['env_info'] = env_info\n",
        "\n",
        "    # add an attribute for visualization convenience\n",
        "    train_model(\n",
        "        model,\n",
        "        datasets,\n",
        "        cfg,\n",
        "        distributed=distributed,\n",
        "        validate=(not args.no_validate),\n",
        "        timestamp=timestamp,\n",
        "        meta=meta)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQogZKR72tWD",
        "cellView": "form"
      },
      "source": [
        "#@title Train (works on pytorch 1.6, despite the claims in the official repo)\n",
        "%cd /content/Colab-mmediting\n",
        "!python tools/train.py configs/inpainting/deepfillv2/custom.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BC1jaAYRrQ_"
      },
      "source": [
        "# Training deepfillv2 with Differentiable Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "negr7_QTTeLP",
        "cellView": "form"
      },
      "source": [
        "#@title one_stage.py (adding differentiable augmentation for deepfillv2)\n",
        "%%writefile /content/Colab-mmediting/mmedit/models/inpaintors/one_stage.py\n",
        "# Differentiable Augmentation for Data-Efficient GAN Training\n",
        "# Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han\n",
        "# https://arxiv.org/pdf/2006.10738\n",
        "\n",
        "from mmcv.runner import auto_fp16 \n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "policy = 'color,translation,cutout'\n",
        "\n",
        "def DiffAugment(x, policy='', channels_first=True):\n",
        "    if policy:\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 3, 1, 2)\n",
        "        for p in policy.split(','):\n",
        "            for f in AUGMENT_FNS[p]:\n",
        "                x = f(x)\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 2, 3, 1)\n",
        "        x = x.contiguous()\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_brightness(x):\n",
        "    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_saturation(x):\n",
        "    x_mean = x.mean(dim=1, keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_contrast(x):\n",
        "    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_translation(x, ratio=0.125):\n",
        "    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
        "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
        "    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
        "    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_cutout(x, ratio=0.5):\n",
        "    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
        "    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
        "    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
        "    mask[grid_batch, grid_x, grid_y] = 0\n",
        "    x = x * mask.unsqueeze(1)\n",
        "    return x\n",
        "\n",
        "\n",
        "AUGMENT_FNS = {\n",
        "    'color': [rand_brightness, rand_saturation, rand_contrast],\n",
        "    'translation': [rand_translation],\n",
        "    'cutout': [rand_cutout],\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import os.path as osp\n",
        "from pathlib import Path\n",
        "\n",
        "import mmcv\n",
        "import torch\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from mmedit.core import L1Evaluation, psnr, ssim, tensor2img\n",
        "from ..base import BaseModel\n",
        "from ..builder import build_backbone, build_component, build_loss\n",
        "from ..common import set_requires_grad\n",
        "from ..registry import MODELS\n",
        "\n",
        "\n",
        "@MODELS.register_module()\n",
        "class OneStageInpaintor(BaseModel):\n",
        "    \"\"\"Standard one-stage inpaintor with commonly used losses.\n",
        "\n",
        "    An inpaintor must contain an encoder-decoder style generator to\n",
        "    inpaint masked regions. A discriminator will be adopted when\n",
        "    adversarial training is needed.\n",
        "\n",
        "    In this class, we provide a common interface for inpaintors.\n",
        "    For other inpaintors, only some funcs may be modified to fit the\n",
        "    input style or training schedule.\n",
        "\n",
        "    Args:\n",
        "        generator (dict): Config for encoder-decoder style generator.\n",
        "        disc (dict): Config for discriminator.\n",
        "        loss_gan (dict): Config for adversarial loss.\n",
        "        loss_gp (dict): Config for gradient penalty loss.\n",
        "        loss_disc_shift (dict): Config for discriminator shift loss.\n",
        "        loss_composed_percep (dict): Config for perceptural and style loss with\n",
        "            composed image as input.\n",
        "        loss_out_percep (dict): Config for perceptural and style loss with\n",
        "            direct output as input.\n",
        "        loss_l1_hole (dict): Config for l1 loss in the hole.\n",
        "        loss_l1_valid (dict): Config for l1 loss in the valid region.\n",
        "        loss_tv (dict): Config for total variation loss.\n",
        "        train_cfg (dict): Configs for training scheduler. `disc_step` must be\n",
        "            contained for indicates the discriminator updating steps in each\n",
        "            training step.\n",
        "        test_cfg (dict): Configs for testing scheduler.\n",
        "        pretrained (str): Path for pretrained model. Default None.\n",
        "    \"\"\"\n",
        "    _eval_metrics = dict(l1=L1Evaluation, psnr=psnr, ssim=ssim)\n",
        "\n",
        "    def __init__(self,\n",
        "                 encdec,\n",
        "                 disc=None,\n",
        "                 loss_gan=None,\n",
        "                 loss_gp=None,\n",
        "                 loss_disc_shift=None,\n",
        "                 loss_composed_percep=None,\n",
        "                 loss_out_percep=False,\n",
        "                 loss_l1_hole=None,\n",
        "                 loss_l1_valid=None,\n",
        "                 loss_tv=None,\n",
        "                 train_cfg=None,\n",
        "                 test_cfg=None,\n",
        "                 pretrained=None):\n",
        "        super(OneStageInpaintor, self).__init__()\n",
        "        self.with_l1_hole_loss = loss_l1_hole is not None\n",
        "        self.with_l1_valid_loss = loss_l1_valid is not None\n",
        "        self.with_tv_loss = loss_tv is not None\n",
        "        self.with_composed_percep_loss = loss_composed_percep is not None\n",
        "        self.with_out_percep_loss = loss_out_percep\n",
        "        self.with_gan = disc is not None and loss_gan is not None\n",
        "        self.with_gp_loss = loss_gp is not None\n",
        "        self.with_disc_shift_loss = loss_disc_shift is not None\n",
        "        self.is_train = train_cfg is not None\n",
        "        self.train_cfg = train_cfg\n",
        "        self.test_cfg = test_cfg\n",
        "        self.eval_with_metrics = ('metrics' in self.test_cfg) and (\n",
        "            self.test_cfg['metrics'] is not None)\n",
        "\n",
        "        self.generator = build_backbone(encdec)\n",
        "\n",
        "        self.fp16_enabled = False \n",
        "\n",
        "        # build loss modules\n",
        "        if self.with_gan:\n",
        "            self.disc = build_component(disc)\n",
        "            self.loss_gan = build_loss(loss_gan)\n",
        "\n",
        "        if self.with_l1_hole_loss:\n",
        "            self.loss_l1_hole = build_loss(loss_l1_hole)\n",
        "\n",
        "        if self.with_l1_valid_loss:\n",
        "            self.loss_l1_valid = build_loss(loss_l1_valid)\n",
        "\n",
        "        if self.with_composed_percep_loss:\n",
        "            self.loss_percep = build_loss(loss_composed_percep)\n",
        "\n",
        "        if self.with_gp_loss:\n",
        "            self.loss_gp = build_loss(loss_gp)\n",
        "\n",
        "        if self.with_disc_shift_loss:\n",
        "            self.loss_disc_shift = build_loss(loss_disc_shift)\n",
        "\n",
        "        if self.with_tv_loss:\n",
        "            self.loss_tv = build_loss(loss_tv)\n",
        "\n",
        "        self.disc_step_count = 0\n",
        "        self.init_weights(pretrained=pretrained)\n",
        "\n",
        "    def init_weights(self, pretrained=None):\n",
        "        \"\"\"Init weights for models.\n",
        "\n",
        "        Args:\n",
        "            pretrained (str, optional): Path for pretrained weights. If given\n",
        "                None, pretrained weights will not be loaded. Defaults to None.\n",
        "        \"\"\"\n",
        "        self.generator.init_weights(pretrained=pretrained)\n",
        "        if self.with_gan:\n",
        "            self.disc.init_weights(pretrained=pretrained)\n",
        "\n",
        "    @auto_fp16(apply_to=('masked_img', 'mask')) \n",
        "    def forward(self, masked_img, mask, test_mode=True, **kwargs):\n",
        "        \"\"\"Forward function.\n",
        "\n",
        "        Args:\n",
        "            masked_img (torch.Tensor): Image with hole as input.\n",
        "            mask (torch.Tensor): Mask as input.\n",
        "            test_mode (bool, optional): Whether use testing mode.\n",
        "                Defaults to True.\n",
        "\n",
        "        Returns:\n",
        "            dict: Dict contains output results.\n",
        "        \"\"\"\n",
        "        if not test_mode:\n",
        "            return self.forward_train(masked_img, mask, **kwargs)\n",
        "        else:\n",
        "            return self.forward_test(masked_img, mask, **kwargs)\n",
        "\n",
        "    def forward_train(self, *args, **kwargs):\n",
        "        \"\"\"Forward function for training.\n",
        "\n",
        "        In this version, we do not use this interface.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError('This interface should not be used in '\n",
        "                                  'current training schedule. Please use '\n",
        "                                  '`train_step` for training.')\n",
        "\n",
        "    def forward_train_d(self, data_batch, is_real, is_disc):\n",
        "        \"\"\"Forward function in discriminator training step.\n",
        "\n",
        "        In this function, we compute the prediction for each data batch (real\n",
        "        or fake). Meanwhile, the standard gan loss will be computed with\n",
        "        several proposed losses fro stable training.\n",
        "\n",
        "        Args:\n",
        "            data (torch.Tensor): Batch of real data or fake data.\n",
        "            is_real (bool): If True, the gan loss will regard this batch as\n",
        "                real data. Otherwise, the gan loss will regard this batch as\n",
        "                fake data.\n",
        "            is_disc (bool): If True, this function is called in discriminator\n",
        "                training step. Otherwise, this function is called in generator\n",
        "                training step. This will help us to compute different types of\n",
        "                adversarial loss, like LSGAN.\n",
        "\n",
        "        Returns:\n",
        "            dict: Contains the loss items computed in this function.\n",
        "        \"\"\"\n",
        "        pred = self.disc(data_batch)\n",
        "        ##############################################################\n",
        "        #loss_ = self.loss_gan(pred, is_real, is_disc)\n",
        "        #loss_ = (DiffAugment(pred, policy=policy)) #DiffAug\n",
        "\n",
        "        #alternativ:\n",
        "        pred = (DiffAugment(pred, policy=policy))\n",
        "        loss_ = self.loss_gan(pred, is_real, is_disc)\n",
        "\n",
        "        loss = dict(real_loss=loss_) if is_real else dict(fake_loss=loss_)\n",
        "        ##############################################################\n",
        "        if self.with_disc_shift_loss:\n",
        "            loss_d_shift = self.loss_disc_shift(loss_)\n",
        "            # 0.5 for average the fake and real data\n",
        "            loss.update(loss_disc_shift=loss_d_shift * 0.5)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def generator_loss(self, fake_res, fake_img, data_batch):\n",
        "        \"\"\"Forward function in generator training step.\n",
        "\n",
        "        In this function, we mainly compute the loss items for generator with\n",
        "        the given (fake_res, fake_img). In general, the `fake_res` is the\n",
        "        direct output of the generator and the `fake_img` is the composition of\n",
        "        direct output and ground-truth image.\n",
        "\n",
        "        Args:\n",
        "            fake_res (torch.Tensor): Direct output of the generator.\n",
        "            fake_img (torch.Tensor): Composition of `fake_res` and\n",
        "                ground-truth image.\n",
        "            data_batch (dict): Contain other elements for computing losses.\n",
        "\n",
        "        Returns:\n",
        "            tuple(dict): Dict contains the results computed within this \\\n",
        "                function for visualization and dict contains the loss items \\\n",
        "                computed in this function.\n",
        "        \"\"\"\n",
        "        gt = data_batch['gt_img']\n",
        "        mask = data_batch['mask']\n",
        "        masked_img = data_batch['masked_img']\n",
        "\n",
        "        loss = dict()\n",
        "\n",
        "        if self.with_gan:\n",
        "            g_fake_pred = self.disc(fake_img)\n",
        "            loss_g_fake = self.loss_gan(g_fake_pred, True, is_disc=False)\n",
        "            loss['loss_g_fake'] = loss_g_fake\n",
        "\n",
        "        if self.with_l1_hole_loss:\n",
        "            loss_l1_hole = self.loss_l1_hole(fake_res, gt, weight=mask)\n",
        "            loss['loss_l1_hole'] = loss_l1_hole\n",
        "\n",
        "        if self.with_l1_valid_loss:\n",
        "            loss_loss_l1_valid = self.loss_l1_valid(\n",
        "                fake_res, gt, weight=1. - mask)\n",
        "            loss['loss_l1_valid'] = loss_loss_l1_valid\n",
        "\n",
        "        if self.with_composed_percep_loss:\n",
        "            loss_pecep, loss_style = self.loss_percep(fake_img, gt)\n",
        "            if loss_pecep is not None:\n",
        "                loss['loss_composed_percep'] = loss_pecep\n",
        "            if loss_style is not None:\n",
        "                loss['loss_composed_style'] = loss_style\n",
        "\n",
        "        if self.with_out_percep_loss:\n",
        "            loss_out_percep, loss_out_style = self.loss_percep(fake_res, gt)\n",
        "            if loss_out_percep is not None:\n",
        "                loss['loss_out_percep'] = loss_out_percep\n",
        "            if loss_out_style is not None:\n",
        "                loss['loss_out_style'] = loss_out_style\n",
        "\n",
        "        if self.with_tv_loss:\n",
        "            loss_tv = self.loss_tv(fake_img, mask=mask)\n",
        "            loss['loss_tv'] = loss_tv\n",
        "\n",
        "        res = dict(\n",
        "            gt_img=gt.cpu(),\n",
        "            masked_img=masked_img.cpu(),\n",
        "            fake_res=fake_res.cpu(),\n",
        "            fake_img=fake_img.cpu())\n",
        "\n",
        "        return res, loss\n",
        "\n",
        "    def forward_test(self,\n",
        "                     masked_img,\n",
        "                     mask,\n",
        "                     save_image=False,\n",
        "                     save_path=None,\n",
        "                     iteration=None,\n",
        "                     **kwargs):\n",
        "        \"\"\"Forward function for testing.\n",
        "\n",
        "        Args:\n",
        "            masked_img (torch.Tensor): Tensor with shape of (n, 3, h, w).\n",
        "            mask (torch.Tensor): Tensor with shape of (n, 1, h, w).\n",
        "            save_image (bool, optional): If True, results will be saved as\n",
        "                image. Defaults to False.\n",
        "            save_path (str, optional): If given a valid str, the reuslts will\n",
        "                be saved in this path. Defaults to None.\n",
        "            iteration (int, optional): Iteration number. Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            dict: Contain output results and eval metrics (if have).\n",
        "        \"\"\"\n",
        "        input_x = torch.cat([masked_img, mask], dim=1)\n",
        "        fake_res = self.generator(input_x)\n",
        "        fake_img = fake_res * mask + masked_img * (1. - mask)\n",
        "\n",
        "        output = dict()\n",
        "        eval_results = {}\n",
        "        if self.eval_with_metrics:\n",
        "            gt_img = kwargs['gt_img']\n",
        "            data_dict = dict(gt_img=gt_img, fake_res=fake_res, mask=mask)\n",
        "            for metric_name in self.test_cfg['metrics']:\n",
        "                if metric_name in ['ssim', 'psnr']:\n",
        "                    eval_results[metric_name] = self._eval_metrics[\n",
        "                        metric_name](tensor2img(fake_img, min_max=(-1, 1)),\n",
        "                                     tensor2img(gt_img, min_max=(-1, 1)))\n",
        "                else:\n",
        "                    eval_results[metric_name] = self._eval_metrics[\n",
        "                        metric_name]()(data_dict).item()\n",
        "            output['eval_results'] = eval_results\n",
        "        else:\n",
        "            output['fake_res'] = fake_res\n",
        "            output['fake_img'] = fake_img\n",
        "\n",
        "        output['meta'] = None if 'meta' not in kwargs else kwargs['meta'][0]\n",
        "\n",
        "        if save_image:\n",
        "            assert save_image and save_path is not None, (\n",
        "                'Save path should been given')\n",
        "            assert output['meta'] is not None, (\n",
        "                'Meta information should be given to save image.')\n",
        "\n",
        "            tmp_filename = output['meta']['gt_img_path']\n",
        "            filestem = Path(tmp_filename).stem\n",
        "            if iteration is not None:\n",
        "                filename = f'{filestem}_{iteration}.png'\n",
        "            else:\n",
        "                filename = f'{filestem}.png'\n",
        "            mmcv.mkdir_or_exist(save_path)\n",
        "            img_list = [kwargs['gt_img']] if 'gt_img' in kwargs else []\n",
        "            img_list.extend(\n",
        "                [masked_img,\n",
        "                 mask.expand_as(masked_img), fake_res, fake_img])\n",
        "            img = torch.cat(img_list, dim=3).cpu()\n",
        "            self.save_visualization(img, osp.join(save_path, filename))\n",
        "            output['save_img_path'] = osp.abspath(\n",
        "                osp.join(save_path, filename))\n",
        "\n",
        "        return output\n",
        "\n",
        "    def save_visualization(self, img, filename):\n",
        "        \"\"\"Save visualization results.\n",
        "\n",
        "        Args:\n",
        "            img (torch.Tensor): Tensor with shape of (n, 3, h, w).\n",
        "            filename (str): Path to save visualization.\n",
        "        \"\"\"\n",
        "        if self.test_cfg.get('img_rerange', True):\n",
        "            img = (img + 1) / 2\n",
        "        if self.test_cfg.get('img_bgr2rgb', True):\n",
        "            img = img[:, [2, 1, 0], ...]\n",
        "        save_image(img, filename, nrow=1, padding=0)\n",
        "\n",
        "    def train_step(self, data_batch, optimizer):\n",
        "        \"\"\"Train step function.\n",
        "\n",
        "        In this function, the inpaintor will finish the train step following\n",
        "        the pipeline:\n",
        "\n",
        "            1. get fake res/image\n",
        "            2. optimize discriminator (if have)\n",
        "            3. optimize generator\n",
        "\n",
        "        If `self.train_cfg.disc_step > 1`, the train step will contain multiple\n",
        "        iterations for optimizing discriminator with different input data and\n",
        "        only one iteration for optimizing gerator after `disc_step` iterations\n",
        "        for discriminator.\n",
        "\n",
        "        Args:\n",
        "            data_batch (torch.Tensor): Batch of data as input.\n",
        "            optimizer (dict[torch.optim.Optimizer]): Dict with optimizers for\n",
        "                generator and discriminator (if have).\n",
        "\n",
        "        Returns:\n",
        "            dict: Dict with loss, information for logger, the number of \\\n",
        "                samples and results for visualization.\n",
        "        \"\"\"\n",
        "        log_vars = {}\n",
        "\n",
        "        gt_img = data_batch['gt_img']\n",
        "        mask = data_batch['mask']\n",
        "        masked_img = data_batch['masked_img']\n",
        "\n",
        "        # get common output from encdec\n",
        "        input_x = torch.cat([masked_img, mask], dim=1)\n",
        "        fake_res = self.generator(input_x)\n",
        "        fake_img = gt_img * (1. - mask) + fake_res * mask\n",
        "\n",
        "        # discriminator training step\n",
        "        if self.train_cfg.disc_step > 0:\n",
        "            set_requires_grad(self.disc, True)\n",
        "            disc_losses = self.forward_train_d(\n",
        "                fake_img.detach(), False, is_disc=True)\n",
        "            loss_disc, log_vars_d = self.parse_losses(disc_losses)\n",
        "            log_vars.update(log_vars_d)\n",
        "            optimizer['disc'].zero_grad()\n",
        "            loss_disc.backward()\n",
        "\n",
        "            disc_losses = self.forward_train_d(gt_img, True, is_disc=True)\n",
        "            loss_disc, log_vars_d = self.parse_losses(disc_losses)\n",
        "            log_vars.update(log_vars_d)\n",
        "            loss_disc.backward()\n",
        "\n",
        "            if self.with_gp_loss:\n",
        "                loss_d_gp = self.loss_gp(\n",
        "                    self.disc, gt_img, fake_img, mask=mask)\n",
        "                loss_disc, log_vars_d = self.parse_losses(\n",
        "                    dict(loss_gp=loss_d_gp))\n",
        "                log_vars.update(log_vars_d)\n",
        "                loss_disc.backward()\n",
        "\n",
        "            optimizer['disc'].step()\n",
        "\n",
        "            self.disc_step_count = (self.disc_step_count +\n",
        "                                    1) % self.train_cfg.disc_step\n",
        "            if self.disc_step_count != 0:\n",
        "                # results contain the data for visualization\n",
        "                results = dict(\n",
        "                    gt_img=gt_img.cpu(),\n",
        "                    masked_img=masked_img.cpu(),\n",
        "                    fake_res=fake_res.cpu(),\n",
        "                    fake_img=fake_img.cpu())\n",
        "                outputs = dict(\n",
        "                    log_vars=log_vars,\n",
        "                    num_samples=len(data_batch['gt_img'].data),\n",
        "                    results=results)\n",
        "\n",
        "                return outputs\n",
        "\n",
        "        # generator (encdec) training step, results contain the data\n",
        "        # for visualization\n",
        "        if self.with_gan:\n",
        "            set_requires_grad(self.disc, False)\n",
        "        results, g_losses = self.generator_loss(fake_res, fake_img, data_batch)\n",
        "        loss_g, log_vars_g = self.parse_losses(g_losses)\n",
        "        log_vars.update(log_vars_g)\n",
        "        optimizer['generator'].zero_grad()\n",
        "        loss_g.backward()\n",
        "        optimizer['generator'].step()\n",
        "\n",
        "        outputs = dict(\n",
        "            log_vars=log_vars,\n",
        "            num_samples=len(data_batch['gt_img'].data),\n",
        "            results=results)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def val_step(self, data_batch, **kwargs):\n",
        "        \"\"\"Forward function for evaluation.\n",
        "\n",
        "        Args:\n",
        "            data_batch (dict): Contain data for forward.\n",
        "\n",
        "        Returns:\n",
        "            dict: Contain the results from model.\n",
        "        \"\"\"\n",
        "        output = self.forward_test(**data_batch, **kwargs)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def forward_dummy(self, x):\n",
        "        \"\"\"Forward dummy function for getting flops.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor with shape of (n, c, h, w).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Results tensor with shape of (n, 3, h, w).\n",
        "        \"\"\"\n",
        "        res = self.generator(x)\n",
        "\n",
        "        return res\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWkTD21NTJKX",
        "cellView": "form"
      },
      "source": [
        "#@title two_stage.py (adding differentiable augmentation for deepfillv2)\n",
        "%%writefile /content/Colab-mmediting/mmedit/models/inpaintors/two_stage.py\n",
        "#two_stage.py\n",
        "\n",
        "# Differentiable Augmentation for Data-Efficient GAN Training\n",
        "# Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han\n",
        "# https://arxiv.org/pdf/2006.10738\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "policy = 'color,translation,cutout'\n",
        "\n",
        "def DiffAugment(x, policy='', channels_first=True):\n",
        "    if policy:\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 3, 1, 2)\n",
        "        for p in policy.split(','):\n",
        "            for f in AUGMENT_FNS[p]:\n",
        "                x = f(x)\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 2, 3, 1)\n",
        "        x = x.contiguous()\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_brightness(x):\n",
        "    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_saturation(x):\n",
        "    x_mean = x.mean(dim=1, keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_contrast(x):\n",
        "    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_translation(x, ratio=0.125):\n",
        "    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
        "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
        "    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
        "    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_cutout(x, ratio=0.5):\n",
        "    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
        "    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
        "    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
        "    mask[grid_batch, grid_x, grid_y] = 0\n",
        "    x = x * mask.unsqueeze(1)\n",
        "    return x\n",
        "\n",
        "\n",
        "AUGMENT_FNS = {\n",
        "    'color': [rand_brightness, rand_saturation, rand_contrast],\n",
        "    'translation': [rand_translation],\n",
        "    'cutout': [rand_cutout],\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import os.path as osp\n",
        "from pathlib import Path\n",
        "\n",
        "import mmcv\n",
        "import torch\n",
        "from mmedit.core import tensor2img\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from ..common.model_utils import set_requires_grad\n",
        "from ..registry import MODELS\n",
        "from .one_stage import OneStageInpaintor\n",
        "\n",
        "#from DiffAugment_pytorch import DiffAugment\n",
        "\n",
        "@MODELS.register_module()\n",
        "class TwoStageInpaintor(OneStageInpaintor):\n",
        "    \"\"\"Two-Stage Inpaintor.\n",
        "\n",
        "    Currently, we support these loss types in each of two stage inpaintors:\n",
        "    ['loss_gan', 'loss_l1_hole', 'loss_l1_valid', 'loss_composed_percep',\\\n",
        "     'loss_out_percep', 'loss_tv']\n",
        "    The `stage1_loss_type` and `stage2_loss_type` should be chosen from these\n",
        "    loss types.\n",
        "\n",
        "    Args:\n",
        "        stage1_loss_type (tuple[str]): Contains the loss names used in the\n",
        "            first stage model.\n",
        "        stage2_loss_type (tuple[str]): Contains the loss names used in the\n",
        "            second stage model.\n",
        "        input_with_ones (bool): Whether to concatenate an extra ones tensor in\n",
        "            input. Default: True.\n",
        "        disc_input_with_mask (bool): Whether to add mask as input in\n",
        "            discriminator. Default: False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 *args,\n",
        "                 stage1_loss_type=('loss_l1_hole', ),\n",
        "                 stage2_loss_type=('loss_l1_hole', 'loss_gan'),\n",
        "                 input_with_ones=True,\n",
        "                 disc_input_with_mask=False,\n",
        "                 **kwargs):\n",
        "        super(TwoStageInpaintor, self).__init__(*args, **kwargs)\n",
        "\n",
        "        self.stage1_loss_type = stage1_loss_type\n",
        "        self.stage2_loss_type = stage2_loss_type\n",
        "        self.input_with_ones = input_with_ones\n",
        "        self.disc_input_with_mask = disc_input_with_mask\n",
        "        self.eval_with_metrics = ('metrics' in self.test_cfg) and (\n",
        "            self.test_cfg['metrics'] is not None)\n",
        "\n",
        "    def forward_test(self,\n",
        "                     masked_img,\n",
        "                     mask,\n",
        "                     save_image=False,\n",
        "                     save_path=None,\n",
        "                     iteration=None,\n",
        "                     **kwargs):\n",
        "        \"\"\"Forward function for testing.\n",
        "\n",
        "        Args:\n",
        "            masked_img (torch.Tensor): Tensor with shape of (n, 3, h, w).\n",
        "            mask (torch.Tensor): Tensor with shape of (n, 1, h, w).\n",
        "            save_image (bool, optional): If True, results will be saved as\n",
        "                image. Defaults to False.\n",
        "            save_path (str, optional): If given a valid str, the reuslts will\n",
        "                be saved in this path. Defaults to None.\n",
        "            iteration (int, optional): Iteration number. Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            dict: Contain output results and eval metrics (if have).\n",
        "        \"\"\"\n",
        "        if self.input_with_ones:\n",
        "            tmp_ones = torch.ones_like(mask)\n",
        "            input_x = torch.cat([masked_img, tmp_ones, mask], dim=1)\n",
        "        else:\n",
        "            input_x = torch.cat([masked_img, mask], dim=1)\n",
        "        stage1_fake_res, stage2_fake_res = self.generator(input_x)\n",
        "        fake_img = stage2_fake_res * mask + masked_img * (1. - mask)\n",
        "        output = dict()\n",
        "        eval_results = {}\n",
        "        if self.eval_with_metrics:\n",
        "            gt_img = kwargs['gt_img']\n",
        "            data_dict = dict(\n",
        "                gt_img=gt_img, fake_res=stage2_fake_res, mask=mask)\n",
        "            for metric_name in self.test_cfg['metrics']:\n",
        "                if metric_name in ['ssim', 'psnr']:\n",
        "                    eval_results[metric_name] = self._eval_metrics[\n",
        "                        metric_name](tensor2img(fake_img, min_max=(-1, 1)),\n",
        "                                     tensor2img(gt_img, min_max=(-1, 1)))\n",
        "                else:\n",
        "                    eval_results[metric_name] = self._eval_metrics[\n",
        "                        metric_name]()(data_dict).item()\n",
        "            output['eval_results'] = eval_results\n",
        "        else:\n",
        "            output['stage1_fake_res'] = stage1_fake_res\n",
        "            output['stage2_fake_res'] = stage2_fake_res\n",
        "            output['fake_res'] = stage2_fake_res\n",
        "            output['fake_img'] = fake_img\n",
        "\n",
        "        output['meta'] = None if 'meta' not in kwargs else kwargs['meta'][0]\n",
        "\n",
        "        if save_image:\n",
        "            assert save_image and save_path is not None, (\n",
        "                'Save path should be given')\n",
        "            assert output['meta'] is not None, (\n",
        "                'Meta information should be given to save image.')\n",
        "\n",
        "            tmp_filename = output['meta']['gt_img_path']\n",
        "            filestem = Path(tmp_filename).stem\n",
        "            if iteration is not None:\n",
        "                filename = f'{filestem}_{iteration}.png'\n",
        "            else:\n",
        "                filename = f'{filestem}.png'\n",
        "            mmcv.mkdir_or_exist(save_path)\n",
        "            img_list = [kwargs['gt_img']] if 'gt_img' in kwargs else []\n",
        "            img_list.extend([\n",
        "                masked_img,\n",
        "                mask.expand_as(masked_img), stage1_fake_res, stage2_fake_res,\n",
        "                fake_img\n",
        "            ])\n",
        "            img = torch.cat(img_list, dim=3).cpu()\n",
        "            self.save_visualization(img, osp.join(save_path, filename))\n",
        "            output['save_img_path'] = osp.abspath(\n",
        "                osp.join(save_path, filename))\n",
        "\n",
        "        return output\n",
        "\n",
        "    def save_visualization(self, img, filename):\n",
        "        \"\"\"Save visualization results.\n",
        "\n",
        "        Args:\n",
        "            img (torch.Tensor): Tensor with shape of (n, 3, h, w).\n",
        "            filename (str): Path to save visualization.\n",
        "        \"\"\"\n",
        "        if self.test_cfg.get('img_rerange', True):\n",
        "            img = (img + 1) / 2\n",
        "        if self.test_cfg.get('img_bgr2rgb', True):\n",
        "            img = img[:, [2, 1, 0], ...]\n",
        "        save_image(img, filename, nrow=1, padding=0)\n",
        "\n",
        "    def two_stage_loss(self, stage1_data, stage2_data, data_batch):\n",
        "        \"\"\"Calculate two-stage loss.\n",
        "\n",
        "        Args:\n",
        "            stage1_data (dict): Contain stage1 results.\n",
        "            stage2_data (dict): Contain stage2 results.\n",
        "            data_batch (dict): Contain data needed to calculate loss.\n",
        "\n",
        "        Returns:\n",
        "            dict: Contain losses with name.\n",
        "        \"\"\"\n",
        "        gt = data_batch['gt_img']\n",
        "        mask = data_batch['mask']\n",
        "        masked_img = data_batch['masked_img']\n",
        "\n",
        "        loss = dict()\n",
        "        results = dict(\n",
        "            gt_img=gt.cpu(), mask=mask.cpu(), masked_img=masked_img.cpu())\n",
        "        # calculate losses for stage1\n",
        "        if self.stage1_loss_type is not None:\n",
        "            fake_res = stage1_data['fake_res']\n",
        "            fake_img = stage1_data['fake_img']\n",
        "            for type_key in self.stage1_loss_type:\n",
        "                tmp_loss = self.calculate_loss_with_type(\n",
        "                    type_key, fake_res, fake_img, gt, mask, prefix='stage1_')\n",
        "                loss.update(tmp_loss)\n",
        "\n",
        "        results.update(\n",
        "            dict(\n",
        "                stage1_fake_res=stage1_data['fake_res'].cpu(),\n",
        "                stage1_fake_img=stage1_data['fake_img'].cpu()))\n",
        "\n",
        "        if self.stage2_loss_type is not None:\n",
        "            fake_res = stage2_data['fake_res']\n",
        "            fake_img = stage2_data['fake_img']\n",
        "            for type_key in self.stage2_loss_type:\n",
        "                tmp_loss = self.calculate_loss_with_type(\n",
        "                    type_key, fake_res, fake_img, gt, mask, prefix='stage2_')\n",
        "                loss.update(tmp_loss)\n",
        "        results.update(\n",
        "            dict(\n",
        "                stage2_fake_res=stage2_data['fake_res'].cpu(),\n",
        "                stage2_fake_img=stage2_data['fake_img'].cpu()))\n",
        "\n",
        "        return results, loss\n",
        "\n",
        "    def calculate_loss_with_type(self,\n",
        "                                 loss_type,\n",
        "                                 fake_res,\n",
        "                                 fake_img,\n",
        "                                 gt,\n",
        "                                 mask,\n",
        "                                 prefix='stage1_'):\n",
        "        \"\"\"Calculate multiple types of losses.\n",
        "\n",
        "        Args:\n",
        "            loss_type (str): Type of the loss.\n",
        "            fake_res (torch.Tensor): Direct results from model.\n",
        "            fake_img (torch.Tensor): Composited results from model.\n",
        "            gt (torch.Tensor): Ground-truth tensor.\n",
        "            mask (torch.Tensor): Mask tensor.\n",
        "            prefix (str, optional): Prefix for loss name.\n",
        "                Defaults to 'stage1_'.\n",
        "\n",
        "        Returns:\n",
        "            dict: Contain loss value with its name.\n",
        "        \"\"\"\n",
        "        loss_dict = dict()\n",
        "        if loss_type == 'loss_gan':\n",
        "            if self.disc_input_with_mask:\n",
        "                disc_input_x = torch.cat([fake_img, mask], dim=1)\n",
        "            else:\n",
        "                disc_input_x = fake_img\n",
        "            g_fake_pred = self.disc(disc_input_x)\n",
        "            #############################################################\n",
        "            #loss_g_fake = self.loss_gan(g_fake_pred, True, is_disc=False)\n",
        "            #loss_g_fake = (DiffAugment(g_fake_pred, policy=policy)) #DiffAug\n",
        "\n",
        "            #alternativ:\n",
        "            g_fake_pred = DiffAugment(g_fake_pred, policy=policy)\n",
        "            loss_g_fake = self.loss_gan(g_fake_pred, True, is_disc=False)\n",
        "            ##############################################################\n",
        "            loss_dict[prefix + 'loss_g_fake'] = loss_g_fake\n",
        "        elif 'percep' in loss_type:\n",
        "            loss_pecep, loss_style = self.loss_percep(fake_img, gt)\n",
        "            if loss_pecep is not None:\n",
        "                loss_dict[prefix + loss_type] = loss_pecep\n",
        "            if loss_style is not None:\n",
        "                loss_dict[prefix + loss_type[:-6] + 'style'] = loss_style\n",
        "        elif 'tv' in loss_type:\n",
        "            loss_tv = self.loss_tv(fake_img, mask=mask)\n",
        "            loss_dict[prefix + loss_type] = loss_tv\n",
        "        elif 'l1' in loss_type:\n",
        "            weight = 1. - mask if 'valid' in loss_type else mask\n",
        "            loss_l1 = getattr(self, loss_type)(fake_res, gt, weight=weight)\n",
        "            loss_dict[prefix + loss_type] = loss_l1\n",
        "        else:\n",
        "            raise NotImplementedError(\n",
        "                f'Please check your loss type {loss_type}'\n",
        "                f' and the config dict in init function. '\n",
        "                f'We cannot find the related loss function.')\n",
        "\n",
        "        return loss_dict\n",
        "\n",
        "    def train_step(self, data_batch, optimizer):\n",
        "        \"\"\"Train step function.\n",
        "\n",
        "        In this function, the inpaintor will finish the train step following\n",
        "        the pipeline:\n",
        "\n",
        "            1. get fake res/image\n",
        "            2. optimize discriminator (if have)\n",
        "            3. optimize generator\n",
        "\n",
        "        If `self.train_cfg.disc_step > 1`, the train step will contain multiple\n",
        "        iterations for optimizing discriminator with different input data and\n",
        "        only one iteration for optimizing gerator after `disc_step` iterations\n",
        "        for discriminator.\n",
        "\n",
        "        Args:\n",
        "            data_batch (torch.Tensor): Batch of data as input.\n",
        "            optimizer (dict[torch.optim.Optimizer]): Dict with optimizers for\n",
        "                generator and discriminator (if have).\n",
        "\n",
        "        Returns:\n",
        "            dict: Dict with loss, information for logger, the number of \\\n",
        "                samples and results for visualization.\n",
        "        \"\"\"\n",
        "        log_vars = {}\n",
        "\n",
        "        gt_img = data_batch['gt_img']\n",
        "        mask = data_batch['mask']\n",
        "        masked_img = data_batch['masked_img']\n",
        "\n",
        "        # get common output from encdec\n",
        "        if self.input_with_ones:\n",
        "            tmp_ones = torch.ones_like(mask)\n",
        "            input_x = torch.cat([masked_img, tmp_ones, mask], dim=1)\n",
        "        else:\n",
        "            input_x = torch.cat([masked_img, mask], dim=1)\n",
        "        stage1_fake_res, stage2_fake_res = self.generator(input_x)\n",
        "        stage1_fake_img = masked_img * (1. - mask) + stage1_fake_res * mask\n",
        "        stage2_fake_img = masked_img * (1. - mask) + stage2_fake_res * mask\n",
        "\n",
        "        # discriminator training step\n",
        "        # In this version, we only use the results from the second stage to\n",
        "        # train discriminators, which is a commonly used setting. This can be\n",
        "        # easily modified to your custom training schedule.\n",
        "        if self.train_cfg.disc_step > 0:\n",
        "            set_requires_grad(self.disc, True)\n",
        "            if self.disc_input_with_mask:\n",
        "                disc_input_x = torch.cat([stage2_fake_img.detach(), mask],\n",
        "                                         dim=1)\n",
        "            else:\n",
        "                disc_input_x = stage2_fake_img.detach()\n",
        "            disc_losses = self.forward_train_d(disc_input_x, False, is_disc=True)\n",
        "            loss_disc, log_vars_d = self.parse_losses(disc_losses)\n",
        "            log_vars.update(log_vars_d)\n",
        "            optimizer['disc'].zero_grad()\n",
        "            loss_disc.backward()\n",
        "\n",
        "            if self.disc_input_with_mask:\n",
        "                disc_input_x = torch.cat([gt_img, mask], dim=1)\n",
        "            else:\n",
        "                disc_input_x = gt_img\n",
        "            disc_losses = self.forward_train_d(disc_input_x, True, is_disc=True)\n",
        "            loss_disc, log_vars_d = self.parse_losses(disc_losses)\n",
        "            log_vars.update(log_vars_d)\n",
        "            loss_disc.backward()\n",
        "\n",
        "            if self.with_gp_loss:\n",
        "                # gradient penalty loss should not be used with mask as input\n",
        "                assert not self.disc_input_with_mask\n",
        "                loss_d_gp = self.loss_gp(self.disc, gt_img, stage2_fake_img, mask=mask)\n",
        "                loss_disc, log_vars_d = self.parse_losses(dict(loss_gp=loss_d_gp))\n",
        "                log_vars.update(log_vars_d)\n",
        "                loss_disc.backward()\n",
        "\n",
        "            optimizer['disc'].step()\n",
        "\n",
        "            self.disc_step_count = (self.disc_step_count +\n",
        "                                    1) % self.train_cfg.disc_step\n",
        "            if self.disc_step_count != 0:\n",
        "                # results contain the data for visualization\n",
        "                results = dict(\n",
        "                    gt_img=gt_img.cpu(),\n",
        "                    masked_img=masked_img.cpu(),\n",
        "                    fake_res=stage2_fake_res.cpu(),\n",
        "                    fake_img=stage2_fake_img.cpu())\n",
        "                outputs = dict(\n",
        "                    log_vars=log_vars,\n",
        "                    num_samples=len(data_batch['gt_img'].data),\n",
        "                    results=results)\n",
        "\n",
        "                return outputs\n",
        "\n",
        "        # prepare stage1 results and stage2 results dict for calculating losses\n",
        "        stage1_results = dict(\n",
        "            fake_res=stage1_fake_res, fake_img=stage1_fake_img)\n",
        "        stage2_results = dict(\n",
        "            fake_res=stage2_fake_res, fake_img=stage2_fake_img)\n",
        "\n",
        "        # generator (encdec) and refiner training step, results contain the\n",
        "        # data for visualization\n",
        "        if self.with_gan:\n",
        "            set_requires_grad(self.disc, False)\n",
        "        results, two_stage_losses = self.two_stage_loss(stage1_results, stage2_results, data_batch)\n",
        "        loss_two_stage, log_vars_two_stage = self.parse_losses(two_stage_losses)\n",
        "        log_vars.update(log_vars_two_stage)\n",
        "        optimizer['generator'].zero_grad()\n",
        "        loss_two_stage.backward()\n",
        "        optimizer['generator'].step()\n",
        "\n",
        "        outputs = dict(\n",
        "            log_vars=log_vars,\n",
        "            num_samples=len(data_batch['gt_img'].data),\n",
        "            results=results)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# generator\n",
        "two_stage.py / train_step()\n",
        "v\n",
        "two_stage.py / two_stage_loss()\n",
        "v\n",
        "two_stage.py / calculate_loss_with_type()\n",
        "v\n",
        "two_stage.py / loss_g_fake = self.loss_gan(g_fake_pred, True, is_disc=False)\n",
        "\n",
        "# discriminator\n",
        "two_stage.py / train_step()\n",
        "v\n",
        "two_stage.py -> one_stage.py / forward_train_d()\n",
        "v\n",
        "one_stage.py / loss = dict(real_loss=loss_) if is_real else dict(fake_loss=loss_)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX1GN-VgpeGB"
      },
      "source": [
        "[Experimental] Custom loss functions for deepfill v2: HFENLoss (high frequency error norm), ElasticLoss, RelativeL1, L1CosineSim, ClipL1, FFTloss, OFLoss (Overflow loss), GPLoss (Gradient Profile (GP) loss), CPLoss (Color Profile (CP) loss) and Contextual_Loss. Currently no weight value in config and can be added in the ```two_stage.py``` file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLc8bnP6v4-s",
        "cellView": "form"
      },
      "source": [
        "#@title two_stage.py diffaug + new loss (two_stage.py)\n",
        "%%writefile /content/Colab-mmediting/mmedit/models/inpaintors/two_stage.py\n",
        "\n",
        "# victorca25's loss implementations\n",
        "# https://github.com/victorca25/BasicSR/blob/dev2/codes/models/modules/loss.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numbers\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "import torchvision.models.vgg as vgg\n",
        "from collections import OrderedDict\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "################################################################################################################################################################################################\n",
        "# VGG MODEL\n",
        "# models.modules.architectures.perceptual\n",
        "\n",
        "vgg_layer19 = {\n",
        "    'conv_1_1': 0, 'conv_1_2': 2, 'pool_1': 4, 'conv_2_1': 5, 'conv_2_2': 7, 'pool_2': 9, 'conv_3_1': 10, 'conv_3_2': 12, 'conv_3_3': 14, 'conv_3_4': 16, 'pool_3': 18, 'conv_4_1': 19, 'conv_4_2': 21, 'conv_4_3': 23, 'conv_4_4': 25, 'pool_4': 27, 'conv_5_1': 28, 'conv_5_2': 30, 'conv_5_3': 32, 'conv_5_4': 34, 'pool_5': 36\n",
        "}\n",
        "vgg_layer_inv19 = {\n",
        "    0: 'conv_1_1', 2: 'conv_1_2', 4: 'pool_1', 5: 'conv_2_1', 7: 'conv_2_2', 9: 'pool_2', 10: 'conv_3_1', 12: 'conv_3_2', 14: 'conv_3_3', 16: 'conv_3_4', 18: 'pool_3', 19: 'conv_4_1', 21: 'conv_4_2', 23: 'conv_4_3', 25: 'conv_4_4', 27: 'pool_4', 28: 'conv_5_1', 30: 'conv_5_2', 32: 'conv_5_3', 34: 'conv_5_4', 36: 'pool_5'\n",
        "}\n",
        "# VGG 16 layers to listen to\n",
        "vgg_layer16 = {\n",
        "    'conv_1_1': 0, 'conv_1_2': 2, 'pool_1': 4, 'conv_2_1': 5, 'conv_2_2': 7, 'pool_2': 9, 'conv_3_1': 10, 'conv_3_2': 12, 'conv_3_3': 14, 'pool_3': 16, 'conv_4_1': 17, 'conv_4_2': 19, 'conv_4_3': 21, 'pool_4': 23, 'conv_5_1': 24, 'conv_5_2': 26, 'conv_5_3': 28, 'pool_5': 30\n",
        "}\n",
        "vgg_layer_inv16 = {\n",
        "    0: 'conv_1_1', 2: 'conv_1_2', 4: 'pool_1', 5: 'conv_2_1', 7: 'conv_2_2', 9: 'pool_2', 10: 'conv_3_1', 12: 'conv_3_2', 14: 'conv_3_3', 16: 'pool_3', 17: 'conv_4_1', 19: 'conv_4_2', 21: 'conv_4_3', 23: 'pool_4', 24: 'conv_5_1', 26: 'conv_5_2', 28: 'conv_5_3', 30: 'pool_5'\n",
        "}\n",
        "\n",
        "class VGG_Model(nn.Module):\n",
        "    \"\"\"\n",
        "        A VGG model with listerners in the layers. \n",
        "        Will return a dictionary of outputs that correspond to the \n",
        "        layers set in \"listen_list\".\n",
        "    \"\"\"\n",
        "    def __init__(self, listen_list=None, net='vgg19', use_input_norm=True, z_norm=False):\n",
        "        super(VGG_Model, self).__init__()\n",
        "        #vgg = vgg16(pretrained=True)\n",
        "        if net == 'vgg19':\n",
        "            vgg_net = vgg.vgg19(pretrained=True)\n",
        "            vgg_layer = vgg_layer19\n",
        "            self.vgg_layer_inv = vgg_layer_inv19\n",
        "        elif net == 'vgg16':\n",
        "            vgg_net = vgg.vgg16(pretrained=True)\n",
        "            vgg_layer = vgg_layer16\n",
        "            self.vgg_layer_inv = vgg_layer_inv16\n",
        "        self.vgg_model = vgg_net.features\n",
        "        self.use_input_norm = use_input_norm\n",
        "        # image normalization\n",
        "        if self.use_input_norm:\n",
        "            if z_norm: # if input in range [-1,1]\n",
        "                mean = torch.tensor(\n",
        "                    [[[0.485-1]], [[0.456-1]], [[0.406-1]]], requires_grad=False)\n",
        "                std = torch.tensor(\n",
        "                    [[[0.229*2]], [[0.224*2]], [[0.225*2]]], requires_grad=False)\n",
        "            else: # input in range [0,1]\n",
        "                mean = torch.tensor(\n",
        "                    [[[0.485]], [[0.456]], [[0.406]]], requires_grad=False)\n",
        "                std = torch.tensor(\n",
        "                    [[[0.229]], [[0.224]], [[0.225]]], requires_grad=False)\n",
        "            self.register_buffer('mean', mean)\n",
        "            self.register_buffer('std', std)\n",
        "\n",
        "        vgg_dict = vgg_net.state_dict()\n",
        "        vgg_f_dict = self.vgg_model.state_dict()\n",
        "        vgg_dict = {k: v for k, v in vgg_dict.items() if k in vgg_f_dict}\n",
        "        vgg_f_dict.update(vgg_dict)\n",
        "        # no grad\n",
        "        for p in self.vgg_model.parameters():\n",
        "            p.requires_grad = False\n",
        "        if listen_list == []:\n",
        "            self.listen = []\n",
        "        else:\n",
        "            self.listen = set()\n",
        "            for layer in listen_list:\n",
        "                self.listen.add(vgg_layer[layer])\n",
        "        self.features = OrderedDict()\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_input_norm:\n",
        "            x = (x - self.mean.detach()) / self.std.detach()\n",
        "\n",
        "        for index, layer in enumerate(self.vgg_model):\n",
        "            x = layer(x)\n",
        "            if index in self.listen:\n",
        "                self.features[self.vgg_layer_inv[index]] = x\n",
        "        return self.features\n",
        "\n",
        "################################################################################################################################################################################################\n",
        "# from dataops.filters import *\n",
        "# codes/dataops/filters.py\n",
        "\n",
        "'''\n",
        "    Multiple image filters used by different functions. Can also be used as augmentations.\n",
        "'''\n",
        "\n",
        "import numbers\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#from dataops.common import denorm\n",
        "\n",
        "def denorm(x, min_max=(-1.0, 1.0)):\n",
        "    '''\n",
        "        Denormalize from [-1,1] range to [0,1]\n",
        "        formula: xi' = (xi - mu)/sigma\n",
        "        Example: \"out = (x + 1.0) / 2.0\" for denorm \n",
        "            range (-1,1) to (0,1)\n",
        "        for use with proper act in Generator output (ie. tanh)\n",
        "    '''\n",
        "    out = (x - min_max[0]) / (min_max[1] - min_max[0])\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        return out.clamp(0, 1)\n",
        "    elif isinstance(x, np.ndarray):\n",
        "        return np.clip(out, 0, 1)\n",
        "    else:\n",
        "        raise TypeError(\"Got unexpected object type, expected torch.Tensor or \\\n",
        "        np.ndarray\")\n",
        "\n",
        "\n",
        "def get_kernel_size(sigma = 6):\n",
        "    '''\n",
        "        Get optimal gaussian kernel size according to sigma * 6 criterion \n",
        "        (must return an int)\n",
        "        Alternative from Matlab: kernel_size=2*np.ceil(3*sigma)+1\n",
        "        https://stackoverflow.com/questions/3149279/optimal-sigma-for-gaussian-filtering-of-an-image\n",
        "    '''\n",
        "    kernel_size = np.ceil(sigma*6)\n",
        "    return kernel_size\n",
        "\n",
        "def get_kernel_sigma(kernel_size = 5):\n",
        "    '''\n",
        "        Get optimal gaussian kernel sigma (variance) according to kernel_size/6 \n",
        "        Alternative from Matlab: sigma = (kernel_size-1)/6\n",
        "    '''\n",
        "    return kernel_size/6.0\n",
        "\n",
        "def get_kernel_mean(kernel_size = 5):\n",
        "    '''\n",
        "        Get gaussian kernel mean\n",
        "    '''\n",
        "    return (kernel_size - 1) / 2.0\n",
        "\n",
        "def kernel_conv_w(kernel, channels: int =3):\n",
        "    '''\n",
        "        Reshape a H*W kernel to 2d depthwise convolutional \n",
        "            weight (for loading in a Conv2D)\n",
        "    '''\n",
        "\n",
        "    # Dynamic window expansion. expand() does not copy memory, needs contiguous()\n",
        "    kernel = kernel.expand(channels, 1, *kernel.size()).contiguous()\n",
        "    return kernel\n",
        "\n",
        "#@torch.jit.script\n",
        "def get_gaussian_kernel1d(kernel_size: int,\n",
        "                sigma: float = 1.5, \n",
        "                #channel: int = None,\n",
        "                force_even: bool = False) -> torch.Tensor:\n",
        "    r\"\"\"Function that returns 1-D Gaussian filter kernel coefficients.\n",
        "    Args:\n",
        "        kernel_size (int): filter/window size. It should be odd and positive.\n",
        "        sigma (float): gaussian standard deviation, sigma of normal distribution\n",
        "        force_even (bool): overrides requirement for odd kernel size.\n",
        "    Returns:\n",
        "        torch.Tensor: 1D tensor with 1D gaussian filter coefficients.\n",
        "    Shape:\n",
        "        - Output: :math:`(\\text{kernel_size})`\n",
        "    Examples::\n",
        "        >>> get_gaussian_kernel1d(3, 2.5)\n",
        "        tensor([0.3243, 0.3513, 0.3243])\n",
        "        >>> get_gaussian_kernel1d(5, 1.5)\n",
        "        tensor([0.1201, 0.2339, 0.2921, 0.2339, 0.1201])\n",
        "    \"\"\"\n",
        "        \n",
        "    if (not isinstance(kernel_size, int) or (\n",
        "            (kernel_size % 2 == 0) and not force_even) or (\n",
        "            kernel_size <= 0)):\n",
        "        raise TypeError(\n",
        "            \"kernel_size must be an odd positive integer. \"\n",
        "            \"Got {}\".format(kernel_size)\n",
        "        )\n",
        "\n",
        "    if kernel_size % 2 == 0:\n",
        "        x = torch.arange(kernel_size).float() - kernel_size // 2    \n",
        "        x = x + 0.5\n",
        "        gauss = torch.exp((-x.pow(2.0) / float(2 * sigma ** 2)))\n",
        "    else: #much faster\n",
        "        gauss = torch.Tensor([np.exp(-(x - kernel_size//2)**2/float(2*sigma**2)) for x in range(kernel_size)])\n",
        "\n",
        "    gauss /= gauss.sum()\n",
        "    \n",
        "    return gauss\n",
        "\n",
        "#To get the kernel coefficients\n",
        "def get_gaussian_kernel2d(\n",
        "        #kernel_size: Tuple[int, int],\n",
        "        kernel_size,\n",
        "        #sigma: Tuple[float, float],\n",
        "        sigma,\n",
        "        force_even: bool = False) -> torch.Tensor:\n",
        "    r\"\"\"Function that returns Gaussian filter matrix coefficients.\n",
        "         Modified with a faster kernel creation if the kernel size\n",
        "         is odd. \n",
        "    Args:\n",
        "        kernel_size (Tuple[int, int]): filter (window) sizes in the x and y \n",
        "         direction. Sizes should be odd and positive, unless force_even is\n",
        "         used.\n",
        "        sigma (Tuple[int, int]): gaussian standard deviation in the x and y\n",
        "         direction.\n",
        "        force_even (bool): overrides requirement for odd kernel size.\n",
        "    Returns:\n",
        "        Tensor: 2D tensor with gaussian filter matrix coefficients.\n",
        "    Shape:\n",
        "        - Output: :math:`(\\text{kernel_size}_x, \\text{kernel_size}_y)`\n",
        "    Examples::\n",
        "        >>> get_gaussian_kernel2d((3, 3), (1.5, 1.5))\n",
        "        tensor([[0.0947, 0.1183, 0.0947],\n",
        "                [0.1183, 0.1478, 0.1183],\n",
        "                [0.0947, 0.1183, 0.0947]])\n",
        "        >>> get_gaussian_kernel2d((3, 5), (1.5, 1.5))\n",
        "        tensor([[0.0370, 0.0720, 0.0899, 0.0720, 0.0370],\n",
        "                [0.0462, 0.0899, 0.1123, 0.0899, 0.0462],\n",
        "                [0.0370, 0.0720, 0.0899, 0.0720, 0.0370]])\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(kernel_size, (int, float)): \n",
        "        kernel_size = (kernel_size, kernel_size)\n",
        "\n",
        "    if isinstance(sigma, (int, float)): \n",
        "        sigma = (sigma, sigma)\n",
        "\n",
        "    if not isinstance(kernel_size, tuple) or len(kernel_size) != 2:\n",
        "        raise TypeError(\n",
        "            \"kernel_size must be a tuple of length two. Got {}\".format(\n",
        "                kernel_size\n",
        "            )\n",
        "        )\n",
        "    if not isinstance(sigma, tuple) or len(sigma) != 2:\n",
        "        raise TypeError(\n",
        "            \"sigma must be a tuple of length two. Got {}\".format(sigma)\n",
        "        )\n",
        "    ksize_x, ksize_y = kernel_size\n",
        "    sigma_x, sigma_y = sigma\n",
        "    kernel_x: torch.Tensor = get_gaussian_kernel1d(ksize_x, sigma_x, force_even)\n",
        "    kernel_y: torch.Tensor = get_gaussian_kernel1d(ksize_y, sigma_y, force_even)\n",
        "    \n",
        "    kernel_2d: torch.Tensor = torch.matmul(\n",
        "        kernel_x.unsqueeze(-1), kernel_y.unsqueeze(-1).t()\n",
        "    )\n",
        "    \n",
        "    return kernel_2d\n",
        "\n",
        "def get_gaussian_kernel(kernel_size=5, sigma=3, dim=2):\n",
        "    '''\n",
        "        This function can generate gaussian kernels in any dimension,\n",
        "            but its 3 times slower than get_gaussian_kernel2d()\n",
        "    Arguments:\n",
        "        kernel_size (Tuple[int, int]): filter sizes in the x and y direction.\n",
        "            Sizes should be odd and positive.\n",
        "        sigma (Tuple[int, int]): gaussian standard deviation in the x and y\n",
        "            direction.\n",
        "        dim: the image dimension (2D=2, 3D=3, etc)\n",
        "    Returns:\n",
        "        Tensor: tensor with gaussian filter matrix coefficients.\n",
        "    '''\n",
        "\n",
        "    if isinstance(kernel_size, numbers.Number):\n",
        "        kernel_size = [kernel_size] * dim\n",
        "    if isinstance(sigma, numbers.Number):\n",
        "        sigma = [sigma] * dim\n",
        "\n",
        "    kernel = 1\n",
        "    meshgrids = torch.meshgrid(\n",
        "        [\n",
        "            torch.arange(size, dtype=torch.float32)\n",
        "            for size in kernel_size\n",
        "        ]\n",
        "    )\n",
        "    for size, std, mgrid in zip(kernel_size, sigma, meshgrids):\n",
        "        mean = (size - 1) / 2\n",
        "        kernel *= 1 / (std * math.sqrt(2 * math.pi)) * \\\n",
        "                  torch.exp(-((mgrid - mean) / std) ** 2 / 2)\n",
        "\n",
        "    kernel = kernel / torch.sum(kernel)    \n",
        "    return kernel\n",
        "\n",
        "#TODO: could be modified to generate kernels in different dimensions\n",
        "def get_box_kernel(kernel_size: int = 5, dim=2):\n",
        "    if isinstance(kernel_size, numbers.Number):\n",
        "        kernel_size = [kernel_size] * dim\n",
        "\n",
        "    kx: float=  float(kernel_size[0])\n",
        "    ky: float=  float(kernel_size[1])\n",
        "    box_kernel = torch.Tensor(np.ones((kx, ky)) / (kx*ky))\n",
        "\n",
        "    return box_kernel\n",
        "\n",
        "\n",
        "\n",
        "#TODO: Can change HFEN to use either LoG, DoG or XDoG\n",
        "def get_log_kernel_5x5():\n",
        "    '''\n",
        "    This is a precomputed LoG kernel that has already been convolved with\n",
        "    Gaussian, for edge detection. \n",
        "    \n",
        "    http://fourier.eng.hmc.edu/e161/lectures/gradient/node8.html\n",
        "    http://homepages.inf.ed.ac.uk/rbf/HIPR2/log.htm\n",
        "    https://academic.mu.edu/phys/matthysd/web226/Lab02.htm\n",
        "    The 2-D LoG can be approximated by a 5 by 5 convolution kernel such as:\n",
        "    weight_log = torch.Tensor([\n",
        "                    [0, 0, 1, 0, 0],\n",
        "                    [0, 1, 2, 1, 0],\n",
        "                    [1, 2, -16, 2, 1],\n",
        "                    [0, 1, 2, 1, 0],\n",
        "                    [0, 0, 1, 0, 0]\n",
        "                ])\n",
        "    This is an approximate to the LoG kernel with kernel size 5 and optimal \n",
        "    sigma ~6 (0.590155...).\n",
        "    '''\n",
        "    return torch.Tensor([\n",
        "                [0, 0, 1, 0, 0],\n",
        "                [0, 1, 2, 1, 0],\n",
        "                [1, 2, -16, 2, 1],\n",
        "                [0, 1, 2, 1, 0],\n",
        "                [0, 0, 1, 0, 0]\n",
        "            ])\n",
        "\n",
        "#dim is the image dimension (2D=2, 3D=3, etc), but for now the final_kernel is hardcoded to 2D images\n",
        "#Not sure if it would make sense in higher dimensions\n",
        "#Note: Kornia suggests their laplacian kernel can also be used to generate LoG kernel: \n",
        "# https://torchgeometry.readthedocs.io/en/latest/_modules/kornia/filters/laplacian.html\n",
        "def get_log_kernel2d(kernel_size=5, sigma=None, dim=2): #sigma=0.6; kernel_size=5\n",
        "    \n",
        "    #either kernel_size or sigma are required:\n",
        "    if not kernel_size and sigma:\n",
        "        kernel_size = get_kernel_size(sigma)\n",
        "        kernel_size = [kernel_size] * dim #note: should it be [kernel_size] or [kernel_size-1]? look below \n",
        "    elif kernel_size and not sigma:\n",
        "        sigma = get_kernel_sigma(kernel_size)\n",
        "        sigma = [sigma] * dim\n",
        "\n",
        "    if isinstance(kernel_size, numbers.Number):\n",
        "        kernel_size = [kernel_size-1] * dim\n",
        "    if isinstance(sigma, numbers.Number):\n",
        "        sigma = [sigma] * dim\n",
        "\n",
        "    grids = torch.meshgrid([torch.arange(-size//2,size//2+1,1) for size in kernel_size])\n",
        "\n",
        "    kernel = 1\n",
        "    for size, std, mgrid in zip(kernel_size, sigma, grids):\n",
        "        kernel *= torch.exp(-(mgrid**2/(2.*std**2)))\n",
        "    \n",
        "    #TODO: For now hardcoded to 2 dimensions, test to make it work in any dimension\n",
        "    final_kernel = (kernel) * ((grids[0]**2 + grids[1]**2) - (2*sigma[0]*sigma[1])) * (1/((2*math.pi)*(sigma[0]**2)*(sigma[1]**2)))\n",
        "    \n",
        "    #TODO: Test if normalization has to be negative (the inverted peak should not make a difference)\n",
        "    final_kernel = -final_kernel / torch.sum(final_kernel)\n",
        "    \n",
        "    return final_kernel\n",
        "\n",
        "def get_log_kernel(kernel_size: int = 5, sigma: float = None, dim: int = 2):\n",
        "    '''\n",
        "        Returns a Laplacian of Gaussian (LoG) kernel. If the kernel is known, use it,\n",
        "        else, generate a kernel with the parameters provided (slower)\n",
        "    '''\n",
        "    if kernel_size ==5 and not sigma and dim == 2: \n",
        "        return get_log_kernel_5x5()\n",
        "    else:\n",
        "        return get_log_kernel2d(kernel_size, sigma, dim)\n",
        "\n",
        "#TODO: use\n",
        "# Implementation of binarize operation (for edge detectors)\n",
        "def binarize(bin_img, threshold):\n",
        "  #bin_img = img > threshold\n",
        "  bin_img[bin_img < threshold] = 0.\n",
        "  return bin_img\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_laplacian_kernel_3x3(alt=False) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "        Utility function that returns a laplacian kernel of 3x3\n",
        "            https://academic.mu.edu/phys/matthysd/web226/Lab02.htm\n",
        "            http://homepages.inf.ed.ac.uk/rbf/HIPR2/log.htm\n",
        "        \n",
        "        This is called a negative Laplacian because the central peak is negative. \n",
        "        It is just as appropriate to reverse the signs of the elements, using \n",
        "        -1s and a +4, to get a positive Laplacian. It doesn't matter:\n",
        "        laplacian_kernel = torch.Tensor([\n",
        "                                    [0,  -1, 0],\n",
        "                                    [-1, 4, -1],\n",
        "                                    [0,  -1, 0]\n",
        "                                ])\n",
        "        Alternative Laplacian kernel as produced by Kornia (this is positive Laplacian,\n",
        "        like: https://kornia.readthedocs.io/en/latest/filters.html\n",
        "        laplacian_kernel = torch.Tensor([\n",
        "                                    [-1, -1, -1],\n",
        "                                    [-1,  8, -1],\n",
        "                                    [-1, -1, -1]\n",
        "                                ])\n",
        "    \"\"\"\n",
        "    if alt:\n",
        "        return torch.tensor([\n",
        "                    [-1, -1, -1],\n",
        "                    [-1,  8, -1],\n",
        "                    [-1, -1, -1]\n",
        "                ])\n",
        "    else:\n",
        "        return torch.tensor([\n",
        "                    [0, 1, 0],\n",
        "                    [1,-4, 1],\n",
        "                    [0, 1, 0],\n",
        "                ])\n",
        "\n",
        "def get_gradient_kernel_3x3() -> torch.Tensor:\n",
        "    \"\"\"\n",
        "        Utility function that returns a gradient kernel of 3x3\n",
        "            in x direction (transpose for y direction)\n",
        "            kernel_gradient_v = [[0, -1, 0], \n",
        "                                 [0, 0, 0], \n",
        "                                 [0, 1, 0]]\n",
        "            kernel_gradient_h = [[0, 0, 0], \n",
        "                                 [-1, 0, 1], \n",
        "                                 [0, 0, 0]]\n",
        "    \"\"\"\n",
        "    return torch.tensor([\n",
        "                   [0, 0, 0], \n",
        "                   [-1, 0, 1], \n",
        "                   [0, 0, 0],\n",
        "            ])\n",
        "\n",
        "def get_scharr_kernel_3x3() -> torch.Tensor:\n",
        "    \"\"\"\n",
        "        Utility function that returns a scharr kernel of 3x3\n",
        "            in x direction (transpose for y direction)\n",
        "    \"\"\"\n",
        "    return torch.tensor([\n",
        "                   [-3, 0, 3],\n",
        "                   [-10,0,10],\n",
        "                   [-3, 0, 3],\n",
        "    ])\n",
        "\n",
        "def get_prewitt_kernel_3x3() -> torch.Tensor:\n",
        "    \"\"\"\n",
        "        Utility function that returns a prewitt kernel of 3x3\n",
        "            in x direction (transpose for y direction).\n",
        "        \n",
        "        Prewitt in x direction: This mask is called the \n",
        "            (vertical) Prewitt Edge Detector\n",
        "            prewitt_x= np.array([[-1, 0, 1],\n",
        "                                [-1, 0, 1],\n",
        "                                [-1, 0, 1]])\n",
        "        \n",
        "        Prewitt in y direction: This mask is called the \n",
        "            (horizontal) Prewitt Edge Detector\n",
        "            prewitt_y= np.array([[-1,-1,-1],\n",
        "                                 [0, 0, 0],\n",
        "                                 [1, 1, 1]])\n",
        "        Note that a Prewitt operator is a 1D box filter convolved with \n",
        "            a derivative operator \n",
        "            finite_diff = [-1, 0, 1]\n",
        "            simple_box = [1, 1, 1]\n",
        "    \"\"\"\n",
        "    return torch.tensor([\n",
        "                   [-1, 0, 1],\n",
        "                   [-1, 0, 1],\n",
        "                   [-1, 0, 1],\n",
        "    ])\n",
        "\n",
        "#https://github.com/kornia/kornia/blob/master/kornia/filters/kernels.py\n",
        "def get_sobel_kernel_3x3() -> torch.Tensor:\n",
        "    \"\"\"Utility function that returns a sobel kernel of 3x3\n",
        "        sobel in x direction\n",
        "            sobel_x= np.array([[-1, 0, 1],\n",
        "                               [-2, 0, 2],\n",
        "                               [-1, 0, 1]])\n",
        "        sobel in y direction\n",
        "            sobel_y= np.array([[-1,-2,-1],\n",
        "                               [0, 0, 0],\n",
        "                               [1, 2, 1]])\n",
        "        \n",
        "        Note that a Sobel operator is a [1 2 1] filter convolved with \n",
        "            a derivative operator.\n",
        "            finite_diff = [1, 2, 1]\n",
        "            simple_box = [1, 1, 1]\n",
        "    \"\"\"\n",
        "    return torch.tensor([\n",
        "        [-1., 0., 1.],\n",
        "        [-2., 0., 2.],\n",
        "        [-1., 0., 1.],\n",
        "    ])\n",
        "\n",
        "#https://towardsdatascience.com/implement-canny-edge-detection-from-scratch-with-pytorch-a1cccfa58bed\n",
        "def get_sobel_kernel_2d(kernel_size=3):\n",
        "    # get range\n",
        "    range = torch.linspace(-(kernel_size // 2), kernel_size // 2, kernel_size)\n",
        "    # compute a grid the numerator and the axis-distances\n",
        "    y, x = torch.meshgrid(range, range)\n",
        "    #Note: x is edge detector in x, y is edge detector in y, if not dividing by den\n",
        "    den = (x ** 2 + y ** 2)\n",
        "    #den[:, kernel_size // 2] = 1  # avoid division by zero at the center of den\n",
        "    den[kernel_size // 2, kernel_size // 2] = 1  # avoid division by zero at the center of den\n",
        "    #sobel_2D = x / den #produces kernel in range (0,1)\n",
        "    sobel_2D = 2*x / den #produces same kernel as kornia\n",
        "    return sobel_2D\n",
        "\n",
        "def get_sobel_kernel(kernel_size=3):\n",
        "    '''\n",
        "    Sobel kernel\n",
        "        https://en.wikipedia.org/wiki/Sobel_operator\n",
        "    Note: using the Sobel filters needs two kernels, one in X axis and one in Y \n",
        "        axis (which is the transpose of X), to get the gradients in both directions.\n",
        "        The same kernel can be used in both cases.\n",
        "    '''\n",
        "    if kernel_size==3:\n",
        "        return get_sobel_kernel_3x3()\n",
        "    else:\n",
        "        return get_sobel_kernel_2d(kernel_size)\n",
        "\n",
        "\n",
        "\n",
        "#To apply the 1D filter in X and Y axis (For SSIM)\n",
        "#@torch.jit.script\n",
        "def apply_1Dfilter(input, win, use_padding: bool=False):  \n",
        "    r\"\"\" Apply 1-D kernel to input in X and Y axes.\n",
        "         Separable filters like the Gaussian blur can be applied to \n",
        "         a two-dimensional image as two independent one-dimensional \n",
        "         calculations, so a 2-dimensional convolution operation can \n",
        "         be separated into two 1-dimensional filters. This reduces \n",
        "         the cost of computing the operator.\n",
        "           https://en.wikipedia.org/wiki/Separable_filter\n",
        "    Args:\n",
        "        input (torch.Tensor): a batch of tensors to be filtered\n",
        "        window (torch.Tensor): 1-D gauss kernel\n",
        "        use_padding: padding image before conv\n",
        "    Returns:\n",
        "        torch.Tensor: filtered tensors\n",
        "    \"\"\"\n",
        "    #N, C, H, W = input.shape\n",
        "    C = input.shape[1]\n",
        "    \n",
        "    padding = 0\n",
        "    if use_padding:\n",
        "        window_size = win.shape[3]\n",
        "        padding = window_size // 2\n",
        "\n",
        "    #same 1D filter for both axes    \n",
        "    out = F.conv2d(input, win, stride=1, padding=(0, padding), groups=C)\n",
        "    out = F.conv2d(out, win.transpose(2, 3), stride=1, padding=(padding, 0), groups=C)\n",
        "    return out\n",
        "\n",
        "#convenient alias\n",
        "apply_gaussian_filter = apply_1Dfilter\n",
        "\n",
        "\n",
        "\n",
        "#TODO: use this in the initialization of class FilterX, so it can be used on \n",
        "# forward with an image (LoG, Gaussian, etc)\n",
        "def load_filter(kernel, kernel_size=3, in_channels=3, out_channels=3, \n",
        "                stride=1, padding=True, groups=3, dim: int =2, \n",
        "                requires_grad=False):\n",
        "    '''\n",
        "        Loads a kernel's coefficients into a Conv layer that \n",
        "            can be used to convolve an image with, by default, \n",
        "            for depthwise convolution\n",
        "        Can use nn.Conv1d, nn.Conv2d or nn.Conv3d, depending on\n",
        "            the dimension set in dim (1,2,3)\n",
        "        #From Pytorch Conv2D:\n",
        "            https://pytorch.org/docs/master/_modules/torch/nn/modules/conv.html#Conv2d\n",
        "            When `groups == in_channels` and `out_channels == K * in_channels`,\n",
        "            where `K` is a positive integer, this operation is also termed in\n",
        "            literature as depthwise convolution.\n",
        "             At groups= :attr:`in_channels`, each input channel is convolved with\n",
        "             its own set of filters, of size:\n",
        "             :math:`\\left\\lfloor\\frac{out\\_channels}{in\\_channels}\\right\\rfloor`.\n",
        "    '''\n",
        "\n",
        "    '''#TODO: check if this is necessary, probably not\n",
        "    if isinstance(kernel_size, numbers.Number):\n",
        "        kernel_size = [kernel_size] * dim\n",
        "    '''\n",
        "\n",
        "    # Reshape to 2d depthwise convolutional weight\n",
        "    kernel = kernel_conv_w(kernel, in_channels)\n",
        "    assert(len(kernel.shape)==4 and kernel.shape[0]==in_channels)\n",
        "\n",
        "    if padding:\n",
        "        pad = compute_padding(kernel_size)\n",
        "    else:\n",
        "        pad = 0\n",
        "    \n",
        "    # create filter as convolutional layer\n",
        "    if dim == 1:\n",
        "        conv = nn.Conv1d\n",
        "    elif dim == 2:\n",
        "        conv = nn.Conv2d\n",
        "    elif dim == 3:\n",
        "        conv = nn.Conv3d\n",
        "    else:\n",
        "        raise RuntimeError(\n",
        "            'Only 1, 2 and 3 dimensions are supported for convolution. \\\n",
        "            Received {}.'.format(dim)\n",
        "        )\n",
        "\n",
        "    filter = conv(in_channels=in_channels, out_channels=out_channels,\n",
        "                        kernel_size=kernel_size, stride=stride, padding=padding, \n",
        "                        groups=groups, bias=False)\n",
        "    filter.weight.data = kernel\n",
        "    filter.weight.requires_grad = requires_grad\n",
        "    return filter\n",
        "\n",
        "\n",
        "def compute_padding(kernel_size):\n",
        "    '''\n",
        "        Computes padding tuple. For square kernels, pad can be an\n",
        "         int, else, a tuple with an element for each dimension\n",
        "    '''\n",
        "    # 4 or 6 ints:  (padding_left, padding_right, padding_top, padding_bottom)\n",
        "    if isinstance(kernel_size, int):\n",
        "        return kernel_size//2\n",
        "    elif isinstance(kernel_size, list):\n",
        "        computed = [k // 2 for k in kernel_size]\n",
        "\n",
        "        out_padding = []\n",
        "\n",
        "        for i in range(len(kernel_size)):\n",
        "            computed_tmp = computed[-(i + 1)]\n",
        "            # for even kernels we need to do asymetric padding\n",
        "            if kernel_size[i] % 2 == 0:\n",
        "                padding = computed_tmp - 1\n",
        "            else:\n",
        "                padding = computed_tmp\n",
        "            out_padding.append(padding)\n",
        "            out_padding.append(computed_tmp)\n",
        "        return out_padding\n",
        "\n",
        "def normalize_kernel2d(input: torch.Tensor) -> torch.Tensor:\n",
        "    r\"\"\"Normalizes kernel.\n",
        "    \"\"\"\n",
        "    if len(input.size()) < 2:\n",
        "        raise TypeError(\"input should be at least 2D tensor. Got {}\"\n",
        "                        .format(input.size()))\n",
        "    norm: torch.Tensor = input.abs().sum(dim=-1).sum(dim=-1)\n",
        "    return input / (norm.unsqueeze(-1).unsqueeze(-1))\n",
        "\n",
        "def filter2D(input: torch.Tensor, kernel: torch.Tensor,\n",
        "             border_type: str = 'reflect', \n",
        "             dim: int =2,\n",
        "             normalized: bool = False) -> torch.Tensor:\n",
        "    r\"\"\"Function that convolves a tensor with a kernel.\n",
        "    The function applies a given kernel to a tensor. The kernel is applied\n",
        "    independently at each depth channel of the tensor. Before applying the\n",
        "    kernel, the function applies padding according to the specified mode so\n",
        "    that the output remains in the same shape.\n",
        "    Args:\n",
        "        input (torch.Tensor): the input tensor with shape of\n",
        "          :math:`(B, C, H, W)`.\n",
        "        kernel (torch.Tensor): the kernel to be convolved with the input\n",
        "          tensor. The kernel shape must be :math:`(1, kH, kW)`.\n",
        "        border_type (str): the padding mode to be applied before convolving.\n",
        "          The expected modes are: ``'constant'``, ``'reflect'``,\n",
        "          ``'replicate'`` or ``'circular'``. Default: ``'reflect'``.\n",
        "        normalized (bool): If True, kernel will be L1 normalized.\n",
        "    Return:\n",
        "        torch.Tensor: the convolved tensor of same size and numbers of channels\n",
        "        as the input.\n",
        "    \"\"\"\n",
        "    if not isinstance(input, torch.Tensor):\n",
        "        raise TypeError(\"Input type is not a torch.Tensor. Got {}\"\n",
        "                        .format(type(input)))\n",
        "\n",
        "    if not isinstance(kernel, torch.Tensor):\n",
        "        raise TypeError(\"Input kernel type is not a torch.Tensor. Got {}\"\n",
        "                        .format(type(kernel)))\n",
        "\n",
        "    if not isinstance(border_type, str):\n",
        "        raise TypeError(\"Input border_type is not string. Got {}\"\n",
        "                        .format(type(kernel)))\n",
        "\n",
        "    #if not len(input.shape) == 4:\n",
        "        #raise ValueError(\"Invalid input shape, we expect BxCxHxW. Got: {}\"\n",
        "                         #.format(input.shape))\n",
        "\n",
        "    #if not len(kernel.shape) == 3:\n",
        "        #raise ValueError(\"Invalid kernel shape, we expect 1xHxW. Got: {}\"\n",
        "                         #.format(kernel.shape))\n",
        "\n",
        "    borders_list: List[str] = ['constant', 'reflect', 'replicate', 'circular']\n",
        "    if border_type not in borders_list:\n",
        "        raise ValueError(\"Invalid border_type, we expect the following: {0}.\"\n",
        "                         \"Got: {1}\".format(borders_list, border_type))\n",
        "\n",
        "    # prepare kernel\n",
        "    b, c, h, w = input.shape\n",
        "    tmp_kernel: torch.Tensor = kernel.unsqueeze(0).to(input.device).to(input.dtype)\n",
        "    if normalized:\n",
        "        tmp_kernel = normalize_kernel2d(tmp_kernel) \n",
        "    # pad the input tensor\n",
        "    height, width = tmp_kernel.shape[-2:]\n",
        "    padding_shape: List[int] = compute_padding((height, width))\n",
        "    input_pad: torch.Tensor = F.pad(input, padding_shape, mode=border_type)\n",
        "    b, c, hp, wp = input_pad.shape\n",
        "\n",
        "    tmp_kernel = tmp_kernel.expand(c, -1, -1, -1)\n",
        "\n",
        "    # convolve the tensor with the kernel.\n",
        "    if dim == 1:\n",
        "        conv = F.conv1d\n",
        "    elif dim == 2:\n",
        "        conv = F.conv2d\n",
        "        #TODO: this needs a review, the final sizes don't match with .view(b, c, h, w), (they are larger).\n",
        "            # using .view(b, c, -1, w) results in an output, but it's 3 times larger than it should be\n",
        "        '''\n",
        "        # if kernel_numel > 81 this is a faster algo\n",
        "        kernel_numel: int = height * width #kernel_numel = torch.numel(tmp_kernel[-1:])\n",
        "        if kernel_numel > 81:\n",
        "            return conv(input_pad.reshape(b * c, 1, hp, wp), tmp_kernel, padding=0, stride=1).view(b, c, h, w)\n",
        "        '''\n",
        "    elif dim == 3:\n",
        "        conv = F.conv3d\n",
        "    else:\n",
        "        raise RuntimeError(\n",
        "            'Only 1, 2 and 3 dimensions are supported. Received {}.'.format(dim)\n",
        "        )\n",
        "\n",
        "    return conv(input_pad, tmp_kernel, groups=c, padding=0, stride=1)\n",
        "\n",
        "\n",
        "#TODO: make one class to receive any arbitrary kernel and others that are specific (like gaussian, etc)\n",
        "#class FilterX(nn.Module):\n",
        "  #def __init__(self, ..., kernel_type, dim: int=2):\n",
        "      #r\"\"\"\n",
        "      #Args:\n",
        "          #argument: ...\n",
        "      #\"\"\"\n",
        "      #super(filterXd, self).__init__()\n",
        "      #Here receive an pre-made kernel of any type, load as tensor or as\n",
        "      #convXd layer (class or functional)\n",
        "      # self.filter = load_filter(kernel=kernel, kernel_size=kernel_size, \n",
        "                #in_channels=image_channels, out_channels=image_channels, stride=stride, \n",
        "                #padding=pad, groups=image_channels)\n",
        "  #def forward:\n",
        "      #This would apply the filter that was initialized\n",
        "    \n",
        "\n",
        "\n",
        "class FilterLow(nn.Module):\n",
        "    def __init__(self, recursions=1, kernel_size=9, stride=1, padding=True, \n",
        "                image_channels=3, include_pad=True, filter_type=None):\n",
        "        super(FilterLow, self).__init__()\n",
        "        \n",
        "        if padding:\n",
        "            pad = compute_padding(kernel_size)\n",
        "        else:\n",
        "            pad = 0\n",
        "        \n",
        "        if filter_type == 'gaussian':\n",
        "            sigma = get_kernel_sigma(kernel_size)\n",
        "            kernel = get_gaussian_kernel2d(kernel_size=kernel_size, sigma=sigma)\n",
        "            self.filter = load_filter(kernel=kernel, kernel_size=kernel_size, \n",
        "                    in_channels=image_channels, stride=stride, padding=pad)\n",
        "        #elif filter_type == '': #TODO... box? (the same as average) What else?\n",
        "        else:\n",
        "            self.filter = nn.AvgPool2d(kernel_size=kernel_size, stride=stride, \n",
        "                    padding=pad, count_include_pad=include_pad)\n",
        "        self.recursions = recursions\n",
        "\n",
        "    def forward(self, img):\n",
        "        for i in range(self.recursions):\n",
        "            img = self.filter(img)\n",
        "        return img\n",
        "\n",
        "\n",
        "class FilterHigh(nn.Module):\n",
        "    def __init__(self, recursions=1, kernel_size=9, stride=1, include_pad=True, \n",
        "            image_channels=3, normalize=True, filter_type=None, kernel=None):\n",
        "        super(FilterHigh, self).__init__()\n",
        "        \n",
        "        # if is standard freq. separator, will use the same LPF to remove LF from image\n",
        "        if filter_type=='gaussian' or filter_type=='average':\n",
        "            self.type = 'separator'\n",
        "            self.filter_low = FilterLow(recursions=1, kernel_size=kernel_size, stride=stride, \n",
        "                image_channels=image_channels, include_pad=include_pad, filter_type=filter_type)\n",
        "        # otherwise, can use any independent filter\n",
        "        else: #load any other filter for the high pass\n",
        "            self.type = 'independent'\n",
        "            #kernel and kernel_size should be provided. Options for edge detectors:\n",
        "            # In both dimensions: get_log_kernel, get_laplacian_kernel_3x3 \n",
        "            # and get_sobel_kernel\n",
        "            # Single dimension: get_prewitt_kernel_3x3, get_scharr_kernel_3x3 \n",
        "            # get_gradient_kernel_3x3 \n",
        "            if include_pad:\n",
        "                pad = compute_padding(kernel_size)\n",
        "            else:\n",
        "                pad = 0\n",
        "            self.filter_low = load_filter(kernel=kernel, kernel_size=kernel_size, \n",
        "                in_channels=image_channels, out_channels=image_channels, stride=stride, \n",
        "                padding=pad, groups=image_channels)\n",
        "        self.recursions = recursions\n",
        "        self.normalize = normalize\n",
        "\n",
        "    def forward(self, img):\n",
        "        if self.type == 'separator':\n",
        "            if self.recursions > 1:\n",
        "                for i in range(self.recursions - 1):\n",
        "                    img = self.filter_low(img)\n",
        "            img = img - self.filter_low(img)\n",
        "        elif self.type == 'independent':\n",
        "            img = self.filter_low(img)\n",
        "        if self.normalize:\n",
        "            return denorm(img)\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "#TODO: check how similar getting the gradient with get_gradient_kernel_3x3 is from the alternative displacing the image\n",
        "#ref from TF: https://github.com/tensorflow/tensorflow/blob/4386a6640c9fb65503750c37714971031f3dc1fd/tensorflow/python/ops/image_ops_impl.py#L3423\n",
        "def get_image_gradients(image):\n",
        "    \"\"\"Returns image gradients (dy, dx) for each color channel.\n",
        "    Both output tensors have the same shape as the input: [b, c, h, w]. \n",
        "    Places the gradient [I(x+1,y) - I(x,y)] on the base pixel (x, y). \n",
        "    That means that dy will always have zeros in the last row,\n",
        "    and dx will always have zeros in the last column.\n",
        "    This can be used to implement the anisotropic 2-D version of the \n",
        "    Total Variation formula:\n",
        "        https://en.wikipedia.org/wiki/Total_variation_denoising\n",
        "    (anisotropic is using l1, isotropic is using l2 norm)\n",
        "    \n",
        "    Arguments:\n",
        "        image: Tensor with shape [b, c, h, w].\n",
        "    Returns:\n",
        "        Pair of tensors (dy, dx) holding the vertical and horizontal image\n",
        "        gradients (1-step finite difference).  \n",
        "    Raises:\n",
        "      ValueError: If `image` is not a 3D image or 4D tensor.\n",
        "    \"\"\"\n",
        "    \n",
        "    image_shape = image.shape\n",
        "      \n",
        "    if len(image_shape) == 3:\n",
        "        # The input is a single image with shape [height, width, channels].\n",
        "        # Calculate the difference of neighboring pixel-values.\n",
        "        # The images are shifted one pixel along the height and width by slicing.\n",
        "        dx = image[:, 1:, :] - image[:, :-1, :] #pixel_dif2, f_v_1-f_v_2\n",
        "        dy = image[1:, :, :] - image[:-1, :, :] #pixel_dif1, f_h_1-f_h_2\n",
        "\n",
        "    elif len(image_shape) == 4:    \n",
        "        # Return tensors with same size as original image\n",
        "        #adds one pixel pad to the right and removes one pixel from the left\n",
        "        right = F.pad(image, [0, 1, 0, 0])[..., :, 1:]\n",
        "        #adds one pixel pad to the bottom and removes one pixel from the top\n",
        "        bottom = F.pad(image, [0, 0, 0, 1])[..., 1:, :] \n",
        "\n",
        "        #right and bottom have the same dimensions as image\n",
        "        dx, dy = right - image, bottom - image \n",
        "        \n",
        "        #this is required because otherwise results in the last column and row having \n",
        "        # the original pixels from the image\n",
        "        dx[:, :, :, -1] = 0 # dx will always have zeros in the last column, right-left\n",
        "        dy[:, :, -1, :] = 0 # dy will always have zeros in the last row,    bottom-top\n",
        "    else:\n",
        "      raise ValueError(\n",
        "          'image_gradients expects a 3D [h, w, c] or 4D tensor '\n",
        "          '[batch_size, c, h, w], not %s.', image_shape)\n",
        "\n",
        "    return dy, dx\n",
        "\n",
        "\n",
        "def get_4dim_image_gradients(image):\n",
        "    # Return tensors with same size as original image\n",
        "    # Place the gradient [I(x+1,y) - I(x,y)] on the base pixel (x, y).\n",
        "    right = F.pad(image, [0, 1, 0, 0])[..., :, 1:] #adds one pixel pad to the right and removes one pixel from the left\n",
        "    bottom = F.pad(image, [0, 0, 0, 1])[..., 1:, :] #adds one pixel pad to the bottom and removes one pixel from the top\n",
        "    botright = F.pad(image, [0, 1, 0, 1])[..., 1:, 1:] #displaces in diagonal direction\n",
        "\n",
        "    dx, dy = right - image, bottom - image #right and bottom have the same dimensions as image\n",
        "    dn, dp = botright - image, right - bottom\n",
        "    #dp is positive diagonal (bottom left to top right)\n",
        "    #dn is negative diagonal (top left to bottom right)\n",
        "    \n",
        "    #this is required because otherwise results in the last column and row having \n",
        "    # the original pixels from the image\n",
        "    dx[:, :, :, -1] = 0 # dx will always have zeros in the last column, right-left\n",
        "    dy[:, :, -1, :] = 0 # dy will always have zeros in the last row,    bottom-top\n",
        "    dp[:, :, -1, :] = 0 # dp will always have zeros in the last row\n",
        "\n",
        "    return dy, dx, dp, dn\n",
        "\n",
        "#TODO: #https://towardsdatascience.com/implement-canny-edge-detection-from-scratch-with-pytorch-a1cccfa58bed\n",
        "#TODO: https://link.springer.com/article/10.1007/s11220-020-00281-8\n",
        "def grad_orientation(grad_y, grad_x):\n",
        "    go = torch.atan(grad_y / grad_x)\n",
        "    go = go * (360 / np.pi) + 180 # convert to degree\n",
        "    go = torch.round(go / 45) * 45  # keep a split by 45\n",
        "    return go\n",
        "################################################################################################################################################################################################\n",
        "# from dataops.colors import *\n",
        "# codes/dataops/colors.py\n",
        "\n",
        "'''\n",
        "Functions for color operations on tensors.\n",
        "If needed, there are more conversions that can be used:\n",
        "https://github.com/kornia/kornia/tree/master/kornia/color\n",
        "https://github.com/R08UST/Color_Conversion_pytorch/blob/master/differentiable_color_conversion/basic_op.py\n",
        "'''\n",
        "\n",
        "\n",
        "import torch\n",
        "import math\n",
        "import cv2\n",
        "\n",
        "def bgr_to_rgb(image: torch.Tensor) -> torch.Tensor:\n",
        "    # flip image channels\n",
        "    out: torch.Tensor = image.flip(-3) #https://github.com/pytorch/pytorch/issues/229\n",
        "    #out: torch.Tensor = image[[2, 1, 0], :, :] #RGB to BGR #may be faster\n",
        "    return out\n",
        "\n",
        "def rgb_to_bgr(image: torch.Tensor) -> torch.Tensor:\n",
        "    #same operation as bgr_to_rgb(), flip image channels\n",
        "    return bgr_to_rgb(image)\n",
        "\n",
        "def bgra_to_rgba(image: torch.Tensor) -> torch.Tensor:\n",
        "    out: torch.Tensor = image[[2, 1, 0, 3], :, :]\n",
        "    return out\n",
        "\n",
        "def rgba_to_bgra(image: torch.Tensor) -> torch.Tensor:\n",
        "    #same operation as bgra_to_rgba(), flip image channels\n",
        "    return bgra_to_rgba(image)\n",
        "\n",
        "def rgb_to_grayscale(input: torch.Tensor) -> torch.Tensor:\n",
        "    r, g, b = torch.chunk(input, chunks=3, dim=-3)\n",
        "    gray: torch.Tensor = 0.299 * r + 0.587 * g + 0.114 * b\n",
        "    #gray = rgb_to_yuv(input,consts='y')\n",
        "    return gray\n",
        "\n",
        "def bgr_to_grayscale(input: torch.Tensor) -> torch.Tensor:\n",
        "    input_rgb = bgr_to_rgb(input)\n",
        "    gray: torch.Tensor = rgb_to_grayscale(input_rgb)\n",
        "    #gray = rgb_to_yuv(input_rgb,consts='y')\n",
        "    return gray\n",
        "\n",
        "def grayscale_to_rgb(input: torch.Tensor) -> torch.Tensor:\n",
        "    #repeat the gray image to the three channels\n",
        "    rgb: torch.Tensor = input.repeat(3, *[1] * (input.dim() - 1))\n",
        "    return rgb\n",
        "\n",
        "def grayscale_to_bgr(input: torch.Tensor) -> torch.Tensor:\n",
        "    return grayscale_to_rgb(input)\n",
        "\n",
        "def rgb_to_ycbcr(input: torch.Tensor, consts='yuv'):\n",
        "    return rgb_to_yuv(input, consts == 'ycbcr')\n",
        "\n",
        "def rgb_to_yuv(input: torch.Tensor, consts='yuv'):\n",
        "    \"\"\"Converts one or more images from RGB to YUV.\n",
        "    Outputs a tensor of the same shape as the `input` image tensor, containing the YUV\n",
        "    value of the pixels.\n",
        "    The output is only well defined if the value in images are in [0,1].\n",
        "    YCbCr is often confused with the YUV color space, and typically the terms YCbCr \n",
        "    and YUV are used interchangeably, leading to some confusion. The main difference \n",
        "    is that YUV is analog and YCbCr is digital: https://en.wikipedia.org/wiki/YCbCr\n",
        "    Args:\n",
        "      input: 2-D or higher rank. Image data to convert. Last dimension must be\n",
        "        size 3. (Could add additional channels, ie, AlphaRGB = AlphaYUV)\n",
        "      consts: YUV constant parameters to use. BT.601 or BT.709. Could add YCbCr\n",
        "        https://en.wikipedia.org/wiki/YUV\n",
        "    Returns:\n",
        "      images: images tensor with the same shape as `input`.\n",
        "    \"\"\"\n",
        "    \n",
        "    #channels = input.shape[0]\n",
        "    \n",
        "    if consts == 'BT.709': # HDTV YUV\n",
        "        Wr = 0.2126\n",
        "        Wb = 0.0722\n",
        "        Wg = 1 - Wr - Wb #0.7152\n",
        "        Uc = 0.539\n",
        "        Vc = 0.635\n",
        "        delta: float = 0.5 #128 if image range in [0,255]\n",
        "    elif consts == 'ycbcr': # Alt. BT.601 from Kornia YCbCr values, from JPEG conversion\n",
        "        Wr = 0.299\n",
        "        Wb = 0.114\n",
        "        Wg = 1 - Wr - Wb #0.587\n",
        "        Uc = 0.564 #(b-y) #cb\n",
        "        Vc = 0.713 #(r-y) #cr\n",
        "        delta: float = .5 #128 if image range in [0,255]\n",
        "    elif consts == 'yuvK': # Alt. yuv from Kornia YUV values: https://github.com/kornia/kornia/blob/master/kornia/color/yuv.py\n",
        "        Wr = 0.299\n",
        "        Wb = 0.114\n",
        "        Wg = 1 - Wr - Wb #0.587\n",
        "        Ur = -0.147\n",
        "        Ug = -0.289\n",
        "        Ub = 0.436\n",
        "        Vr = 0.615\n",
        "        Vg = -0.515\n",
        "        Vb = -0.100\n",
        "        #delta: float = 0.0\n",
        "    elif consts == 'y': #returns only Y channel, same as rgb_to_grayscale()\n",
        "        #Note: torchvision uses ITU-R 601-2: Wr = 0.2989, Wg = 0.5870, Wb = 0.1140\n",
        "        Wr = 0.299\n",
        "        Wb = 0.114\n",
        "        Wg = 1 - Wr - Wb #0.587\n",
        "    else: # Default to 'BT.601', SDTV YUV\n",
        "        Wr = 0.299\n",
        "        Wb = 0.114\n",
        "        Wg = 1 - Wr - Wb #0.587\n",
        "        Uc = 0.493 #0.492\n",
        "        Vc = 0.877\n",
        "        delta: float = 0.5 #128 if image range in [0,255]\n",
        "\n",
        "    r: torch.Tensor = input[..., 0, :, :]\n",
        "    g: torch.Tensor = input[..., 1, :, :]\n",
        "    b: torch.Tensor = input[..., 2, :, :]\n",
        "    #TODO\n",
        "    #r, g, b = torch.chunk(input, chunks=3, dim=-3) #Alt. Which one is faster? Appear to be the same. Differentiable? Kornia uses both in different places\n",
        "\n",
        "    if consts == 'y':\n",
        "        y: torch.Tensor = Wr * r + Wg * g + Wb * b\n",
        "        #(0.2989 * input[0] + 0.5870 * input[1] + 0.1140 * input[2]).to(img.dtype)\n",
        "        return y\n",
        "    elif consts == 'yuvK':\n",
        "        y: torch.Tensor = Wr * r + Wg * g + Wb * b\n",
        "        u: torch.Tensor = Ur * r + Ug * g + Ub * b\n",
        "        v: torch.Tensor = Vr * r + Vg * g + Vb * b\n",
        "    else: #if consts == 'ycbcr' or consts == 'yuv' or consts == 'BT.709':\n",
        "        y: torch.Tensor = Wr * r + Wg * g + Wb * b\n",
        "        u: torch.Tensor = (b - y) * Uc + delta #cb\n",
        "        v: torch.Tensor = (r - y) * Vc + delta #cr\n",
        "\n",
        "    if consts == 'uv': #returns only UV channels\n",
        "        return torch.stack((u, v), -2) \n",
        "    else:\n",
        "        return torch.stack((y, u, v), -3) \n",
        "\n",
        "def ycbcr_to_rgb(input: torch.Tensor):\n",
        "    return yuv_to_rgb(input, consts == 'ycbcr')\n",
        "\n",
        "def yuv_to_rgb(input: torch.Tensor, consts='yuv') -> torch.Tensor:\n",
        "    if consts == 'yuvK': # Alt. yuv from Kornia YUV values: https://github.com/kornia/kornia/blob/master/kornia/color/yuv.py\n",
        "        Wr = 1.14 #1.402\n",
        "        Wb = 2.029 #1.772\n",
        "        Wgu = 0.396 #.344136\n",
        "        Wgv = 0.581 #.714136\n",
        "        delta: float = 0.0\n",
        "    elif consts == 'yuv' or consts == 'ycbcr': # BT.601 from Kornia YCbCr values, from JPEG conversion\n",
        "        Wr = 1.403 #1.402\n",
        "        Wb = 1.773 #1.772\n",
        "        Wgu = .344 #.344136\n",
        "        Wgv = .714 #.714136\n",
        "        delta: float = .5 #128 if image range in [0,255]\n",
        "    \n",
        "    #Note: https://github.com/R08UST/Color_Conversion_pytorch/blob/75150c5fbfb283ae3adb85c565aab729105bbb66/differentiable_color_conversion/basic_op.py#L65 has u and v flipped\n",
        "    y: torch.Tensor = input[..., 0, :, :]\n",
        "    u: torch.Tensor = input[..., 1, :, :] #cb\n",
        "    v: torch.Tensor = input[..., 2, :, :] #cr\n",
        "    #TODO\n",
        "    #y, u, v = torch.chunk(input, chunks=3, dim=-3) #Alt. Which one is faster? Appear to be the same. Differentiable? Kornia uses both in different places\n",
        "\n",
        "    u_shifted: torch.Tensor = u - delta #cb\n",
        "    v_shifted: torch.Tensor = v - delta #cr\n",
        "\n",
        "    r: torch.Tensor = y + Wr * v_shifted\n",
        "    g: torch.Tensor = y - Wgv * v_shifted - Wgu * u_shifted\n",
        "    b: torch.Tensor = y + Wb * u_shifted\n",
        "    return torch.stack((r, g, b), -3) \n",
        "\n",
        "#Not tested:\n",
        "def rgb2srgb(imgs):\n",
        "    return torch.where(imgs<=0.04045,imgs/12.92,torch.pow((imgs+0.055)/1.055,2.4))\n",
        "\n",
        "#Not tested:\n",
        "def srgb2rgb(imgs):\n",
        "    return torch.where(imgs<=0.0031308,imgs*12.92,1.055*torch.pow((imgs),1/2.4)-0.055)\n",
        "################################################################################################################################################################################################\n",
        "# from dataops.common import norm, denorm\n",
        "# dnorm shon copy pasted\n",
        "\n",
        "def norm(x): \n",
        "    #Normalize (z-norm) from [0,1] range to [-1,1]\n",
        "    out = (x - 0.5) * 2.0\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        return out.clamp(-1, 1)\n",
        "    elif isinstance(x, np.ndarray):\n",
        "        return np.clip(out, -1, 1)\n",
        "    else:\n",
        "        raise TypeError(\"Got unexpected object type, expected torch.Tensor or \\\n",
        "        np.ndarray\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# https://github.com/victorca25/BasicSR/blob/dev2/codes/models/modules/architectures/discriminators.py\n",
        "\n",
        "class PixelDiscriminator(nn.Module):\n",
        "    \"\"\"Defines a 1x1 PatchGAN discriminator (pixelGAN)\"\"\"\n",
        "\n",
        "    def __init__(self, input_nc, ndf=64, norm_layer=nn.BatchNorm2d):\n",
        "        \"\"\"Construct a 1x1 PatchGAN discriminator\n",
        "        Parameters:\n",
        "            input_nc (int)  -- the number of channels in input images\n",
        "            ndf (int)       -- the number of filters in the last conv layer\n",
        "            norm_layer      -- normalization layer\n",
        "        \"\"\"\n",
        "        super(PixelDiscriminator, self).__init__()\n",
        "        '''\n",
        "        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
        "        else:\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\n",
        "        '''\n",
        "        use_bias = False\n",
        "\n",
        "        self.net = [\n",
        "            nn.Conv2d(input_nc, ndf, kernel_size=1, stride=1, padding=0),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(ndf, ndf * 2, kernel_size=1, stride=1, padding=0, bias=use_bias),\n",
        "            norm_layer(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Conv2d(ndf * 2, 1, kernel_size=1, stride=1, padding=0, bias=use_bias)]\n",
        "\n",
        "        self.net = nn.Sequential(*self.net)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"Standard forward.\"\"\"\n",
        "        return self.net(input)\n",
        "\n",
        "class NLayerDiscriminator(nn.Module):\n",
        "    r\"\"\"\n",
        "    PatchGAN discriminator\n",
        "    https://arxiv.org/pdf/1611.07004v3.pdf\n",
        "    https://arxiv.org/pdf/1803.07422.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False, getIntermFeat=False):\n",
        "        \"\"\"Construct a PatchGAN discriminator\n",
        "        Parameters:\n",
        "            input_nc (int)  -- the number of channels in input images\n",
        "            ndf (int)       -- the number of filters in the last conv layer\n",
        "            n_layers (int)  -- the number of conv layers in the discriminator\n",
        "            norm_layer      -- normalization layer\n",
        "        \"\"\"\n",
        "        super(NLayerDiscriminator, self).__init__()\n",
        "        '''\n",
        "        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
        "        else:\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\n",
        "        '''\n",
        "        #self.getIntermFeat = getIntermFeat # not used for now\n",
        "        #use_sigmoid not used for now\n",
        "        #TODO: test if there are benefits by incorporating the use of intermediate features from pix2pixHD\n",
        "\n",
        "        use_bias = False\n",
        "        kw = 4\n",
        "        padw = 1 # int(np.ceil((kw-1.0)/2))\n",
        "\n",
        "        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]\n",
        "        nf_mult = 1\n",
        "        nf_mult_prev = 1\n",
        "        for n in range(1, n_layers):  # gradually increase the number of filters\n",
        "            nf_mult_prev = nf_mult\n",
        "            nf_mult = min(2 ** n, 8)\n",
        "            sequence += [\n",
        "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
        "                norm_layer(ndf * nf_mult),\n",
        "                nn.LeakyReLU(0.2, True)\n",
        "            ]\n",
        "\n",
        "        nf_mult_prev = nf_mult\n",
        "        nf_mult = min(2 ** n_layers, 8)\n",
        "        sequence += [\n",
        "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n",
        "            norm_layer(ndf * nf_mult),\n",
        "            nn.LeakyReLU(0.2, True)\n",
        "        ]\n",
        "\n",
        "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]  # output 1 channel prediction map\n",
        "        self.model = nn.Sequential(*sequence)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Standard forward.\"\"\"\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "################################################################################################################################################################################################\n",
        "\n",
        "################################################################################################################################################################################################\n",
        "# https://github.com/victorca25/BasicSR/blob/dev2/codes/models/modules/loss.py\n",
        "\n",
        "class CharbonnierLoss(nn.Module):\n",
        "    \"\"\"Charbonnier Loss (L1)\"\"\"\n",
        "    def __init__(self, eps=1e-6):\n",
        "        super(CharbonnierLoss, self).__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        b, c, h, w = y.size()\n",
        "        loss = torch.sum(torch.sqrt((x - y).pow(2) + self.eps**2))\n",
        "        return loss/(c*b*h*w)\n",
        "    \n",
        "\n",
        "# Define GAN loss: [vanilla | lsgan | wgan-gp | srpgan/nsgan | hinge]\n",
        "# https://tuatini.me/creating-and-shipping-deep-learning-models-into-production/\n",
        "class GANLoss(nn.Module):\n",
        "    r\"\"\"\n",
        "    Adversarial loss\n",
        "    https://arxiv.org/abs/1711.10337\n",
        "    \"\"\"\n",
        "    def __init__(self, gan_type, real_label_val=1.0, fake_label_val=0.0):\n",
        "        super(GANLoss, self).__init__()\n",
        "        self.gan_type = gan_type.lower()\n",
        "        self.real_label_val = real_label_val\n",
        "        self.fake_label_val = fake_label_val\n",
        "\n",
        "        if self.gan_type == 'vanilla':\n",
        "            self.loss = nn.BCEWithLogitsLoss()\n",
        "        elif self.gan_type == 'lsgan':\n",
        "            self.loss = nn.MSELoss()\n",
        "        elif self.gan_type == 'srpgan' or self.gan_type == 'nsgan':\n",
        "            self.loss = nn.BCELoss()\n",
        "        elif self.gan_type == 'hinge':\n",
        "            self.loss = nn.ReLU()\n",
        "        elif self.gan_type == 'wgan-gp':\n",
        "\n",
        "            def wgan_loss(input, target):\n",
        "                # target is boolean\n",
        "                return -1 * input.mean() if target else input.mean()\n",
        "\n",
        "            self.loss = wgan_loss\n",
        "        else:\n",
        "            raise NotImplementedError('GAN type [{:s}] is not found'.format(self.gan_type))\n",
        "\n",
        "    def get_target_label(self, input, target_is_real):\n",
        "        if self.gan_type == 'wgan-gp':\n",
        "            return target_is_real\n",
        "        if target_is_real:\n",
        "            return torch.empty_like(input).fill_(self.real_label_val) #torch.ones_like(d_sr_out)\n",
        "        else:\n",
        "            return torch.empty_like(input).fill_(self.fake_label_val) #torch.zeros_like(d_sr_out)\n",
        "\n",
        "    def forward(self, input, target_is_real, is_disc = None):\n",
        "        if self.gan_type == 'hinge': #TODO: test\n",
        "            if is_disc:\n",
        "                input = -input if target_is_real else input\n",
        "                return self.loss(1 + input).mean()\n",
        "            else:\n",
        "                return (-input).mean()\n",
        "        else:\n",
        "            target_label = self.get_target_label(input, target_is_real)\n",
        "            loss = self.loss(input, target_label)\n",
        "            return loss\n",
        "\n",
        "\n",
        "class GradientPenaltyLoss(nn.Module):\n",
        "    def __init__(self, device=torch.device('cpu')):\n",
        "        super(GradientPenaltyLoss, self).__init__()\n",
        "        self.register_buffer('grad_outputs', torch.Tensor())\n",
        "        self.grad_outputs = self.grad_outputs.to(device)\n",
        "\n",
        "    def get_grad_outputs(self, input):\n",
        "        if self.grad_outputs.size() != input.size():\n",
        "            self.grad_outputs.resize_(input.size()).fill_(1.0)\n",
        "        return self.grad_outputs\n",
        "\n",
        "    def forward(self, interp, interp_crit):\n",
        "        grad_outputs = self.get_grad_outputs(interp_crit)\n",
        "        grad_interp = torch.autograd.grad(outputs=interp_crit, inputs=interp, \\\n",
        "            grad_outputs=grad_outputs, create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
        "        grad_interp = grad_interp.view(grad_interp.size(0), -1)\n",
        "        grad_interp_norm = grad_interp.norm(2, dim=1)\n",
        "\n",
        "        loss = ((grad_interp_norm - 1)**2).mean()\n",
        "        return loss\n",
        "\n",
        "\n",
        "class HFENLoss(nn.Module): # Edge loss with pre_smooth\n",
        "    \"\"\"Calculates high frequency error norm (HFEN) between target and \n",
        "     prediction used to quantify the quality of reconstruction of edges \n",
        "     and fine features. \n",
        "     \n",
        "     Uses a rotationally symmetric LoG (Laplacian of Gaussian) filter to \n",
        "     capture edges. The original filter kernel is of size 1515 pixels, \n",
        "     and has a standard deviation of 1.5 pixels.\n",
        "     ks = 2 * int(truncate * sigma + 0.5) + 1, so use truncate=4.5\n",
        "     \n",
        "     HFEN is computed as the norm of the result obtained by LoG filtering the \n",
        "     difference between the reconstructed and reference images.\n",
        "    [1]: Ravishankar and Bresler: MR Image Reconstruction From Highly\n",
        "    Undersampled k-Space Data by Dictionary Learning, 2011\n",
        "        https://ieeexplore.ieee.org/document/5617283\n",
        "    [2]: Han et al: Image Reconstruction Using Analysis Model Prior, 2016\n",
        "        https://www.hindawi.com/journals/cmmm/2016/7571934/\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    img1 : torch.Tensor or torch.autograd.Variable\n",
        "        Predicted image\n",
        "    img2 : torch.Tensor or torch.autograd.Variable\n",
        "        Target image\n",
        "    norm: if true, follows [2], who define a normalized version of HFEN.\n",
        "        If using RelativeL1 criterion, it's already normalized. \n",
        "    \"\"\"\n",
        "    def __init__(self, loss_f=None, kernel='log', kernel_size=15, sigma = 2.5, norm = False): #1.4 ~ 1.5\n",
        "        super(HFENLoss, self).__init__()\n",
        "        # can use different criteria\n",
        "        self.criterion = loss_f\n",
        "        self.norm = norm\n",
        "        #can use different kernels like DoG instead:\n",
        "        if kernel == 'dog':\n",
        "            kernel = get_dog_kernel(kernel_size, sigma)\n",
        "        else:\n",
        "            kernel = get_log_kernel(kernel_size, sigma)\n",
        "        self.filter = load_filter(kernel=kernel, kernel_size=kernel_size)\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "        self.filter.to(img1.device)\n",
        "        # HFEN loss\n",
        "        log1 = self.filter(img1)\n",
        "        log2 = self.filter(img2)\n",
        "        hfen_loss = self.criterion(log1, log2)\n",
        "        if self.norm:\n",
        "            hfen_loss /= img2.norm()\n",
        "        return hfen_loss\n",
        "\n",
        "\n",
        "class TVLoss(nn.Module):\n",
        "    def __init__(self, tv_type='tv', p = 1):\n",
        "        super(TVLoss, self).__init__()\n",
        "        assert p in [1, 2]\n",
        "        self.p = p\n",
        "        self.tv_type = tv_type\n",
        "\n",
        "    def forward(self, x):\n",
        "        img_shape = x.shape\n",
        "        if len(img_shape) == 3 or len(img_shape) == 4:\n",
        "            if self.tv_type == 'dtv':\n",
        "                dy, dx, dp, dn  = get_4dim_image_gradients(x)\n",
        "\n",
        "                if len(dy.shape) == 3:\n",
        "                    # Sum for all axis. (None is an alias for all axis.)\n",
        "                    reduce_axes = None\n",
        "                    batch_size = 1\n",
        "                elif len(dy.shape) == 4:\n",
        "                    # Only sum for the last 3 axis.\n",
        "                    # This results in a 1-D tensor with the total variation for each image.\n",
        "                    reduce_axes = (-3, -2, -1)\n",
        "                    batch_size = x.size()[0]\n",
        "                #Compute the element-wise magnitude of a vector array\n",
        "                # Calculates the TV for each image in the batch\n",
        "                # Calculate the total variation by taking the absolute value of the\n",
        "                # pixel-differences and summing over the appropriate axis.\n",
        "                if self.p == 1:\n",
        "                    loss = (dy.abs().sum(dim=reduce_axes) + dx.abs().sum(dim=reduce_axes) + dp.abs().sum(dim=reduce_axes) + dn.abs().sum(dim=reduce_axes)) # Calculates the TV loss for each image in the batch\n",
        "                elif self.p == 2:\n",
        "                    loss = torch.pow(dy,2).sum(dim=reduce_axes) + torch.pow(dx,2).sum(dim=reduce_axes) + torch.pow(dp,2).sum(dim=reduce_axes) + torch.pow(dn,2).sum(dim=reduce_axes)\n",
        "                # calculate the scalar loss-value for tv loss\n",
        "                loss = loss.sum()/(2.0*batch_size) # averages the TV loss all the images in the batch (note: the division is not in TF version, only the sum reduction)\n",
        "                return loss\n",
        "            else: #'tv'\n",
        "                dy, dx  = get_image_gradients(x)\n",
        "\n",
        "                if len(dy.shape) == 3:\n",
        "                    # Sum for all axis. (None is an alias for all axis.)\n",
        "                    reduce_axes = None\n",
        "                    batch_size = 1\n",
        "                elif len(dy.shape) == 4:\n",
        "                    # Only sum for the last 3 axis.\n",
        "                    # This results in a 1-D tensor with the total variation for each image.\n",
        "                    reduce_axes = (-3, -2, -1)\n",
        "                    batch_size = x.size()[0]\n",
        "                #Compute the element-wise magnitude of a vector array\n",
        "                # Calculates the TV for each image in the batch\n",
        "                # Calculate the total variation by taking the absolute value of the\n",
        "                # pixel-differences and summing over the appropriate axis.\n",
        "                if self.p == 1:\n",
        "                    loss = dy.abs().sum(dim=reduce_axes) + dx.abs().sum(dim=reduce_axes)\n",
        "                elif self.p == 2:\n",
        "                    loss = torch.pow(dy,2).sum(dim=reduce_axes) + torch.pow(dx,2).sum(dim=reduce_axes)\n",
        "                # calculate the scalar loss-value for tv loss\n",
        "                loss = loss.sum()/batch_size # averages the TV loss all the images in the batch (note: the division is not in TF version, only the sum reduction)\n",
        "                return loss\n",
        "        else:\n",
        "            raise ValueError(\"Expected input tensor to be of ndim 3 or 4, but got \" + str(len(img_shape)))\n",
        "    \n",
        "\n",
        "class GradientLoss(nn.Module):\n",
        "    def __init__(self, loss_f = None, reduction='mean', gradientdir='2d'): #2d or 4d\n",
        "        super(GradientLoss, self).__init__()\n",
        "        self.criterion = loss_f\n",
        "        self.gradientdir = gradientdir\n",
        "    \n",
        "    def forward(self, input, target):\n",
        "        if self.gradientdir == '4d':\n",
        "            inputdy, inputdx, inputdp, inputdn = get_4dim_image_gradients(input)\n",
        "            targetdy, targetdx, targetdp, targetdn = get_4dim_image_gradients(target) \n",
        "            return (self.criterion(inputdx, targetdx) + self.criterion(inputdy, targetdy) + \\\n",
        "                    self.criterion(inputdp, targetdp) + self.criterion(inputdn, targetdn))/4\n",
        "        else: #'2d'\n",
        "            inputdy, inputdx = get_image_gradients(input)\n",
        "            targetdy, targetdx = get_image_gradients(target) \n",
        "            return (self.criterion(inputdx, targetdx) + self.criterion(inputdy, targetdy))/2\n",
        "\n",
        "\n",
        "class ElasticLoss(nn.Module):\n",
        "    def __init__(self, a=0.2, reduction='mean'): #a=0.5 default\n",
        "        super(ElasticLoss, self).__init__()\n",
        "        self.alpha = torch.FloatTensor([a, 1 - a]) #.to('cuda:0')\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        if not isinstance(input, tuple):\n",
        "            input = (input,)\n",
        "\n",
        "        for i in range(len(input)):\n",
        "            l2 = F.mse_loss(input[i].squeeze(), target.squeeze(), reduction=self.reduction).mul(self.alpha[0])\n",
        "            l1 = F.l1_loss(input[i].squeeze(), target.squeeze(), reduction=self.reduction).mul(self.alpha[1])\n",
        "            loss = l1 + l2\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "#TODO: change to RelativeNorm and set criterion as an input argument, could be any basic criterion\n",
        "class RelativeL1(nn.Module):\n",
        "    '''\n",
        "    Comparing to the regular L1, introducing the division by |c|+epsilon \n",
        "    better models the human vision systems sensitivity to variations\n",
        "    in the dark areas. (where epsilon = 0.01, to prevent values of 0 in the\n",
        "    denominator)\n",
        "    '''\n",
        "    def __init__(self, eps=.01, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.criterion = torch.nn.L1Loss(reduction=reduction)\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        base = target + self.eps\n",
        "        return self.criterion(input/base, target/base)\n",
        "\n",
        "\n",
        "class L1CosineSim(nn.Module):\n",
        "    '''\n",
        "    https://github.com/dmarnerides/hdr-expandnet/blob/master/train.py\n",
        "    Can be used to replace L1 pixel loss, but includes a cosine similarity term \n",
        "    to ensure color correctness of the RGB vectors of each pixel.\n",
        "    lambda is a constant factor that adjusts the contribution of the cosine similarity term\n",
        "    It provides improved color stability, especially for low luminance values, which\n",
        "    are frequent in HDR images, since slight variations in any of theRGB components of these \n",
        "    low values do not contribute much totheL1loss, but they may however cause noticeable \n",
        "    color shifts. More in the paper: https://arxiv.org/pdf/1803.02266.pdf\n",
        "    '''\n",
        "    def __init__(self, loss_lambda=5, reduction='mean'):\n",
        "        super(L1CosineSim, self).__init__()\n",
        "        self.similarity = torch.nn.CosineSimilarity(dim=1, eps=1e-20)\n",
        "        self.l1_loss = nn.L1Loss(reduction=reduction)\n",
        "        self.loss_lambda = loss_lambda\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        cosine_term = (1 - self.similarity(x, y)).mean()\n",
        "        return self.l1_loss(x, y) + self.loss_lambda * cosine_term\n",
        "\n",
        "\n",
        "class ClipL1(nn.Module):\n",
        "    '''\n",
        "    Clip L1 loss\n",
        "    From: https://github.com/HolmesShuan/AIM2020-Real-Super-Resolution/\n",
        "    ClipL1 Loss combines Clip function and L1 loss. self.clip_min sets the \n",
        "    gradients of well-trained pixels to zeros and clip_max works as a noise filter.\n",
        "    data range [0, 255]: (clip_min=0.0, clip_max=10.0), \n",
        "    for [0,1] set clip_min to 1/255=0.003921.\n",
        "    '''\n",
        "    def __init__(self, clip_min=0.0, clip_max=10.0):\n",
        "        super(ClipL1, self).__init__()\n",
        "        self.clip_max = clip_max\n",
        "        self.clip_min = clip_min\n",
        "\n",
        "    def forward(self, sr, hr):\n",
        "        loss = torch.mean(torch.clamp(torch.abs(sr-hr), self.clip_min, self.clip_max))\n",
        "        return loss\n",
        "\n",
        "\n",
        "# Frequency loss \n",
        "# https://github.com/lj1995-computer-vision/Trident-Dehazing-Network/blob/master/loss/fft.py\n",
        "class FFTloss(torch.nn.Module):\n",
        "    def __init__(self, loss_f = torch.nn.L1Loss, reduction='mean'):\n",
        "        super(FFTloss, self).__init__()\n",
        "        self.criterion = loss_f(reduction=reduction)\n",
        "\n",
        "    def forward(self, img1, img2):\n",
        "        zeros=torch.zeros(img1.size()).to(img1.device)\n",
        "        return self.criterion(torch.fft(torch.stack((img1,zeros),-1),2),torch.fft(torch.stack((img2,zeros),-1),2))\n",
        "\n",
        "\n",
        "class OFLoss(torch.nn.Module):\n",
        "    '''\n",
        "    Overflow loss\n",
        "    Only use if the image range is in [0,1]. (This solves the SPL brightness problem\n",
        "    and can be useful in other cases as well)\n",
        "    https://github.com/lj1995-computer-vision/Trident-Dehazing-Network/blob/master/loss/brelu.py\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(OFLoss, self).__init__()\n",
        "\n",
        "    def forward(self, img1):\n",
        "        img_clamp = img1.clamp(0,1)\n",
        "        b,c,h,w = img1.shape\n",
        "        return torch.log((img1 - img_clamp).abs() + 1).sum()/b/c/h/w\n",
        "\n",
        "\n",
        "#TODO: testing\n",
        "# Color loss \n",
        "class ColorLoss(torch.nn.Module):\n",
        "    def __init__(self, loss_f = torch.nn.L1Loss, reduction='mean', ds_f=None):\n",
        "        super(ColorLoss, self).__init__()\n",
        "        self.ds_f = ds_f\n",
        "        self.criterion = loss_f\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        input_uv = rgb_to_yuv(self.ds_f(input), consts='uv')\n",
        "        target_uv = rgb_to_yuv(self.ds_f(target), consts='uv')\n",
        "        return self.criterion(input_uv, target_uv)\n",
        "\n",
        "#TODO: testing\n",
        "# Averaging Downscale loss \n",
        "class AverageLoss(torch.nn.Module):\n",
        "    def __init__(self, loss_f = torch.nn.L1Loss, reduction='mean', ds_f=None):\n",
        "        super(AverageLoss, self).__init__()\n",
        "        self.ds_f = ds_f\n",
        "        self.criterion = loss_f\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        input_uv = rgb_to_yuv(self.ds_f(input), consts='uv')\n",
        "        target_uv = rgb_to_yuv(self.ds_f(target), consts='uv')\n",
        "        return self.criterion(input_uv, target_uv)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################\n",
        "# Spatial Profile Loss\n",
        "########################\n",
        "\n",
        "class GPLoss(nn.Module):\n",
        "    '''\n",
        "    https://github.com/ssarfraz/SPL/blob/master/SPL_Loss/\n",
        "    Gradient Profile (GP) loss\n",
        "    The image gradients in each channel can easily be computed \n",
        "    by simple 1-pixel shifted image differences from itself. \n",
        "    '''\n",
        "    def __init__(self, trace=False, spl_denorm=False):\n",
        "        super(GPLoss, self).__init__()\n",
        "        self.spl_denorm = spl_denorm\n",
        "        if trace == True: # Alternate behavior: use the complete calculation with SPL_ComputeWithTrace()\n",
        "            self.trace = SPL_ComputeWithTrace()\n",
        "        else: # Default behavior: use the more efficient SPLoss()\n",
        "            self.trace = SPLoss()\n",
        "\n",
        "    def __call__(self, input, reference):\n",
        "        ## Use \"spl_denorm\" when reading a [-1,1] input, but you want to compute the loss over a [0,1] range\n",
        "        # Note: only rgb_to_yuv() requires image in the [0,1], so this denorm is optional, depending on the net\n",
        "        if self.spl_denorm == True:\n",
        "            input = denorm(input)\n",
        "            reference = denorm(reference)\n",
        "        input_h, input_v = get_image_gradients(input)\n",
        "        ref_h, ref_v = get_image_gradients(reference)\n",
        "\n",
        "        trace_v = self.trace(input_v,ref_v)\n",
        "        trace_h = self.trace(input_h,ref_h)\n",
        "        return trace_v + trace_h\n",
        "\n",
        "class CPLoss(nn.Module):\n",
        "    '''\n",
        "    Color Profile (CP) loss\n",
        "    '''\n",
        "    def __init__(self, rgb=True, yuv=True, yuvgrad=True, trace=False, spl_denorm=False, yuv_denorm=False):\n",
        "        super(CPLoss, self).__init__()\n",
        "        self.rgb = rgb\n",
        "        self.yuv = yuv\n",
        "        self.yuvgrad = yuvgrad\n",
        "        self.spl_denorm = spl_denorm\n",
        "        self.yuv_denorm = yuv_denorm\n",
        "        \n",
        "        if trace == True: # Alternate behavior: use the complete calculation with SPL_ComputeWithTrace()\n",
        "            self.trace = SPL_ComputeWithTrace()\n",
        "            self.trace_YUV = SPL_ComputeWithTrace()\n",
        "        else: # Default behavior: use the more efficient SPLoss()\n",
        "            self.trace = SPLoss()\n",
        "            self.trace_YUV = SPLoss()\n",
        "\n",
        "    def __call__(self, input, reference):\n",
        "        ## Use \"spl_denorm\" when reading a [-1,1] input, but you want to compute the loss over a [0,1] range\n",
        "        # self.spl_denorm=False when your inputs and outputs are in [0,1] range already\n",
        "        # Note: only rgb_to_yuv() requires image in the [0,1], so this denorm is optional, depending on the net\n",
        "        if self.spl_denorm:\n",
        "            input = denorm(input)\n",
        "            reference = denorm(reference)\n",
        "        total_loss= 0\n",
        "        if self.rgb:\n",
        "            total_loss += self.trace(input,reference)\n",
        "        if self.yuv:\n",
        "            # rgb_to_yuv() needs images in [0,1] range to work\n",
        "            if not self.spl_denorm and self.yuv_denorm:\n",
        "                input = denorm(input)\n",
        "                reference = denorm(reference)\n",
        "            input_yuv = rgb_to_yuv(input)\n",
        "            reference_yuv = rgb_to_yuv(reference)\n",
        "            total_loss += self.trace(input_yuv,reference_yuv)\n",
        "        if self.yuvgrad:\n",
        "            input_h, input_v = get_image_gradients(input_yuv)\n",
        "            ref_h, ref_v = get_image_gradients(reference_yuv)\n",
        "\n",
        "            total_loss +=  self.trace(input_v,ref_v)\n",
        "            total_loss +=  self.trace(input_h,ref_h)\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "## Spatial Profile Loss (SPL) with trace\n",
        "class SPL_ComputeWithTrace(nn.Module):\n",
        "    \"\"\"\n",
        "    Spatial Profile Loss (SPL)\n",
        "    Both loss versions equate to the cosine similarity of rows/columns. \n",
        "    'SPL_ComputeWithTrace()' uses the trace (sum over the diagonal) of matrix multiplication \n",
        "    of L2-normalized input/target rows/columns.\n",
        "    Slow implementation of the trace loss using the same formula as stated in the paper. \n",
        "    In principle, we compute the loss between a source and target image by considering such \n",
        "    pattern differences along the image x and y-directions. Considering a row or a column \n",
        "    spatial profile of an image as a vector, we can compute the similarity between them in \n",
        "    this induced vector space. Formally, this similarity is measured over each image channel c.\n",
        "    The first term computes similarity among row profiles and the second among column profiles \n",
        "    of an image pair (x, y) of size H W. These image pixels profiles are L2-normalized to \n",
        "    have a normalized cosine similarity loss.\n",
        "    \"\"\"\n",
        "    def __init__(self,weight = [1.,1.,1.]): # The variable 'weight' was originally intended to weigh color channels differently. In our experiments, we found that an equal weight between all channels gives the best results. As such, this variable is a leftover from that time and can be removed.\n",
        "        super(SPL_ComputeWithTrace, self).__init__()\n",
        "        self.weight = weight\n",
        "\n",
        "    def __call__(self, input, reference):\n",
        "        a = 0\n",
        "        b = 0\n",
        "        for i in range(input.shape[0]):\n",
        "            for j in range(input.shape[1]):\n",
        "                a += torch.trace(torch.matmul(F.normalize(input[i,j,:,:],p=2,dim=1),torch.t(F.normalize(reference[i,j,:,:],p=2,dim=1))))/input.shape[2]*self.weight[j]\n",
        "                b += torch.trace(torch.matmul(torch.t(F.normalize(input[i,j,:,:],p=2,dim=0)),F.normalize(reference[i,j,:,:],p=2,dim=0)))/input.shape[3]*self.weight[j]\n",
        "        a = -torch.sum(a)/input.shape[0]\n",
        "        b = -torch.sum(b)/input.shape[0]\n",
        "        return a+b\n",
        "\n",
        "## Spatial Profile Loss (SPL) without trace, prefered\n",
        "class SPLoss(nn.Module):\n",
        "    ''' \n",
        "    Spatial Profile Loss (SPL)\n",
        "    'SPLoss()' L2-normalizes the rows/columns, performs piece-wise multiplication \n",
        "    of the two tensors and then sums along the corresponding axes. This variant \n",
        "    needs less operations since it can be performed batchwise.\n",
        "    Note: SPLoss() makes image results too bright, when using images in the [0,1] \n",
        "    range and no activation as output of the Generator.\n",
        "    SPL_ComputeWithTrace() does not have this problem, but results are very blurry. \n",
        "    Adding the Overflow Loss fixes this problem.\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(SPLoss, self).__init__()\n",
        "        #self.weight = weight\n",
        "\n",
        "    def __call__(self, input, reference):\n",
        "        a = torch.sum(torch.sum(F.normalize(input, p=2, dim=2) * F.normalize(reference, p=2, dim=2),dim=2, keepdim=True))\n",
        "        b = torch.sum(torch.sum(F.normalize(input, p=2, dim=3) * F.normalize(reference, p=2, dim=3),dim=3, keepdim=True))\n",
        "        return -(a + b) / (input.size(2) * input.size(0))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################\n",
        "# Contextual Loss\n",
        "########################\n",
        "\n",
        "DIS_TYPES = ['cosine', 'l1', 'l2']\n",
        "\n",
        "class Contextual_Loss(nn.Module):\n",
        "    '''\n",
        "    Contextual loss for unaligned images (https://arxiv.org/abs/1803.02077)\n",
        "    https://github.com/roimehrez/contextualLoss\n",
        "    https://github.com/S-aiueo32/contextual_loss_pytorch\n",
        "    https://github.com/z-bingo/Contextual-Loss-PyTorch\n",
        "    layers_weights: is a dict, e.g., {'conv_1_1': 1.0, 'conv_3_2': 1.0}\n",
        "    crop_quarter: boolean\n",
        "    '''\n",
        "    def __init__(self, layers_weights, crop_quarter=False, max_1d_size=100, \n",
        "            distance_type: str = 'cosine', b=1.0, band_width=0.5, \n",
        "            use_vgg: bool = True, net: str = 'vgg19', calc_type: str =  'regular'):\n",
        "        super(Contextual_Loss, self).__init__()\n",
        "\n",
        "        assert band_width > 0, 'band_width parameter must be positive.'\n",
        "        assert distance_type in DIS_TYPES,\\\n",
        "            f'select a distance type from {DIS_TYPES}.'\n",
        "\n",
        "        listen_list = []\n",
        "        self.layers_weights = {}\n",
        "        try:\n",
        "            listen_list = layers_weights.keys()\n",
        "            self.layers_weights = layers_weights\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        self.crop_quarter = crop_quarter\n",
        "        self.distanceType = distance_type\n",
        "        self.max_1d_size = max_1d_size\n",
        "        self.b = b\n",
        "        self.band_width = band_width #self.h = h, #sigma\n",
        "        \n",
        "        if use_vgg:\n",
        "            self.vgg_model = VGG_Model(listen_list=listen_list, net=net)\n",
        "\n",
        "        if calc_type == 'bilateral':\n",
        "            self.calculate_loss = self.bilateral_CX_Loss\n",
        "        elif calc_type == 'symetric':\n",
        "            self.calculate_loss = self.symetric_CX_Loss\n",
        "        else: #if calc_type == 'regular':\n",
        "            self.calculate_loss = self.calculate_CX_Loss\n",
        "\n",
        "    def forward(self, images, gt):\n",
        "        device = images.device\n",
        "        \n",
        "        if hasattr(self, 'vgg_model'):\n",
        "            assert images.shape[1] == 3 and gt.shape[1] == 3,\\\n",
        "                'VGG model takes 3 channel images.'\n",
        "            \n",
        "            loss = 0\n",
        "            vgg_images = self.vgg_model(images)\n",
        "            vgg_images = {k: v.clone().to(device) for k, v in vgg_images.items()}\n",
        "            vgg_gt = self.vgg_model(gt)\n",
        "            vgg_gt = {k: v.to(device) for k, v in vgg_gt.items()}\n",
        "\n",
        "            for key in self.layers_weights.keys():\n",
        "                if self.crop_quarter:\n",
        "                    vgg_images[key] = self._crop_quarters(vgg_images[key])\n",
        "                    vgg_gt[key] = self._crop_quarters(vgg_gt[key])\n",
        "\n",
        "                N, C, H, W = vgg_images[key].size()\n",
        "                if H*W > self.max_1d_size**2:\n",
        "                    vgg_images[key] = self._random_pooling(vgg_images[key], output_1d_size=self.max_1d_size)\n",
        "                    vgg_gt[key] = self._random_pooling(vgg_gt[key], output_1d_size=self.max_1d_size)\n",
        "\n",
        "                loss_t = self.calculate_loss(vgg_images[key], vgg_gt[key])\n",
        "                loss += loss_t * self.layers_weights[key]\n",
        "                # del vgg_images[key], vgg_gt[key]\n",
        "        #TODO: without VGG it runs, but results are not looking right\n",
        "        else:\n",
        "            if self.crop_quarter:\n",
        "                images = self._crop_quarters(images)\n",
        "                gt = self._crop_quarters(gt)\n",
        "\n",
        "            N, C, H, W = images.size()\n",
        "            if H*W > self.max_1d_size**2:\n",
        "                images = self._random_pooling(images, output_1d_size=self.max_1d_size)\n",
        "                gt = self._random_pooling(gt, output_1d_size=self.max_1d_size)\n",
        "\n",
        "            loss = self.calculate_loss(images, gt)\n",
        "        return loss\n",
        "\n",
        "    @staticmethod\n",
        "    def _random_sampling(tensor, n, indices):\n",
        "        N, C, H, W = tensor.size()\n",
        "        S = H * W\n",
        "        tensor = tensor.view(N, C, S)\n",
        "        device=tensor.device\n",
        "        if indices is None:\n",
        "            indices = torch.randperm(S)[:n].contiguous().type_as(tensor).long()\n",
        "            indices = indices.clamp(indices.min(), tensor.shape[-1]-1) #max = indices.max()-1\n",
        "            indices = indices.view(1, 1, -1).expand(N, C, -1)\n",
        "        indices = indices.to(device)\n",
        "\n",
        "        res = torch.gather(tensor, index=indices, dim=-1)\n",
        "        return res, indices\n",
        "\n",
        "    @staticmethod\n",
        "    def _random_pooling(feats, output_1d_size=100):\n",
        "        single_input = type(feats) is torch.Tensor\n",
        "\n",
        "        if single_input:\n",
        "            feats = [feats]\n",
        "\n",
        "        N, C, H, W = feats[0].size()\n",
        "        feats_sample, indices = Contextual_Loss._random_sampling(feats[0], output_1d_size**2, None)\n",
        "        res = [feats_sample]\n",
        "\n",
        "        for i in range(1, len(feats)):\n",
        "            feats_sample, _ = Contextual_Loss._random_sampling(feats[i], -1, indices)\n",
        "            res.append(feats_sample)\n",
        "\n",
        "        res = [feats_sample.view(N, C, output_1d_size, output_1d_size) for feats_sample in res]\n",
        "\n",
        "        if single_input:\n",
        "            return res[0]\n",
        "        return res\n",
        "\n",
        "    @staticmethod\n",
        "    def _crop_quarters(feature_tensor):\n",
        "        N, fC, fH, fW = feature_tensor.size()\n",
        "        quarters_list = []\n",
        "        quarters_list.append(feature_tensor[..., 0:round(fH / 2), 0:round(fW / 2)])\n",
        "        quarters_list.append(feature_tensor[..., 0:round(fH / 2), round(fW / 2):])\n",
        "        quarters_list.append(feature_tensor[..., round(fH / 2):, 0:round(fW / 2)])\n",
        "        quarters_list.append(feature_tensor[..., round(fH / 2):, round(fW / 2):])\n",
        "\n",
        "        feature_tensor = torch.cat(quarters_list, dim=0)\n",
        "        return feature_tensor\n",
        "\n",
        "    @staticmethod\n",
        "    def _create_using_L2(I_features, T_features):\n",
        "        \"\"\"\n",
        "        Calculating the distance between each feature of I and T\n",
        "        :param I_features:\n",
        "        :param T_features:\n",
        "        :return: raw_distance: [N, C, H, W, H*W], each element of which is the distance between I and T at each position\n",
        "        \"\"\"\n",
        "        assert I_features.size() == T_features.size()\n",
        "        N, C, H, W = I_features.size()\n",
        "\n",
        "        Ivecs = I_features.view(N, C, -1)\n",
        "        Tvecs = T_features.view(N, C, -1)\n",
        "        #\n",
        "        square_I = torch.sum(Ivecs*Ivecs, dim=1, keepdim=False)\n",
        "        square_T = torch.sum(Tvecs*Tvecs, dim=1, keepdim=False)\n",
        "        # raw_distance\n",
        "        raw_distance = []\n",
        "        for i in range(N):\n",
        "            Ivec, Tvec, s_I, s_T = Ivecs[i, ...], Tvecs[i, ...], square_I[i, ...], square_T[i, ...]\n",
        "            # matrix multiplication\n",
        "            AB = Ivec.permute(1, 0) @ Tvec\n",
        "            dist = s_I.view(-1, 1) + s_T.view(1, -1) - 2*AB\n",
        "            raw_distance.append(dist.view(1, H, W, H*W))\n",
        "        raw_distance = torch.cat(raw_distance, dim=0)\n",
        "        raw_distance = torch.clamp(raw_distance, 0.0)\n",
        "        return raw_distance\n",
        "\n",
        "    @staticmethod\n",
        "    def _create_using_L1(I_features, T_features):\n",
        "        assert I_features.size() == T_features.size()\n",
        "        N, C, H, W = I_features.size()\n",
        "\n",
        "        Ivecs = I_features.view(N, C, -1)\n",
        "        Tvecs = T_features.view(N, C, -1)\n",
        "\n",
        "        raw_distance = []\n",
        "        for i in range(N):\n",
        "            Ivec, Tvec = Ivecs[i, ...], Tvecs[i, ...]\n",
        "            dist = torch.sum(\n",
        "                torch.abs(Ivec.view(C, -1, 1) - Tvec.view(C, 1, -1)), dim=0, keepdim=False\n",
        "            )\n",
        "            raw_distance.append(dist.view(1, H, W, H*W))\n",
        "        raw_distance = torch.cat(raw_distance, dim=0)\n",
        "        return raw_distance\n",
        "\n",
        "    @staticmethod\n",
        "    def _create_using_dotP(I_features, T_features):\n",
        "        assert I_features.size() == T_features.size()\n",
        "        # prepare feature before calculating cosine distance\n",
        "        # mean shifting by channel-wise mean of `y`.\n",
        "        mean_T = T_features.mean(dim=(0, 2, 3), keepdim=True)        \n",
        "        I_features = I_features - mean_T\n",
        "        T_features = T_features - mean_T\n",
        "\n",
        "        # L2 channelwise normalization\n",
        "        I_features = F.normalize(I_features, p=2, dim=1)\n",
        "        T_features = F.normalize(T_features, p=2, dim=1)\n",
        "        \n",
        "        N, C, H, W = I_features.size()\n",
        "        cosine_dist = []\n",
        "        # work seperatly for each example in dim 1\n",
        "        for i in range(N):\n",
        "            # channel-wise vectorization\n",
        "            T_features_i = T_features[i].view(1, 1, C, H*W).permute(3, 2, 0, 1).contiguous() # 1CHW --> 11CP, with P=H*W\n",
        "            I_features_i = I_features[i].unsqueeze(0)\n",
        "            dist = F.conv2d(I_features_i, T_features_i).permute(0, 2, 3, 1).contiguous()\n",
        "            #cosine_dist.append(dist) # back to 1CHW\n",
        "            #TODO: temporary hack to workaround AMP bug:\n",
        "            cosine_dist.append(dist.to(torch.float32)) # back to 1CHW\n",
        "        cosine_dist = torch.cat(cosine_dist, dim=0)\n",
        "        cosine_dist = (1 - cosine_dist) / 2\n",
        "        cosine_dist = cosine_dist.clamp(min=0.0)\n",
        "\n",
        "        return cosine_dist\n",
        "\n",
        "    #compute_relative_distance\n",
        "    @staticmethod\n",
        "    def _calculate_relative_distance(raw_distance, epsilon=1e-5):\n",
        "        \"\"\"\n",
        "        Normalizing the distances first as Eq. (2) in paper\n",
        "        :param raw_distance:\n",
        "        :param epsilon:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        div = torch.min(raw_distance, dim=-1, keepdim=True)[0]\n",
        "        relative_dist = raw_distance / (div + epsilon) # Eq 2\n",
        "        return relative_dist\n",
        "\n",
        "    def symetric_CX_Loss(self, I_features, T_features):\n",
        "        loss = (self.calculate_CX_Loss(T_features, I_features) + self.calculate_CX_Loss(I_features, T_features)) / 2\n",
        "        return loss #score\n",
        "\n",
        "    def bilateral_CX_Loss(self, I_features, T_features, weight_sp: float = 0.1):\n",
        "        def compute_meshgrid(shape):\n",
        "            N, C, H, W = shape\n",
        "            rows = torch.arange(0, H, dtype=torch.float32) / (H + 1)\n",
        "            cols = torch.arange(0, W, dtype=torch.float32) / (W + 1)\n",
        "\n",
        "            feature_grid = torch.meshgrid(rows, cols)\n",
        "            feature_grid = torch.stack(feature_grid).unsqueeze(0)\n",
        "            feature_grid = torch.cat([feature_grid for _ in range(N)], dim=0)\n",
        "\n",
        "            return feature_grid\n",
        "\n",
        "        # spatial loss\n",
        "        grid = compute_meshgrid(I_features.shape).to(T_features.device)\n",
        "        raw_distance = Contextual_Loss._create_using_L2(grid, grid) # calculate raw distance\n",
        "        dist_tilde = Contextual_Loss._calculate_relative_distance(raw_distance)\n",
        "        exp_distance = torch.exp((self.b - dist_tilde) / self.band_width) # Eq(3)\n",
        "        cx_sp = exp_distance / torch.sum(exp_distance, dim=-1, keepdim=True) # Eq(4)\n",
        "\n",
        "        # feature loss\n",
        "        # calculate raw distances\n",
        "        if self.distanceType == 'l1':\n",
        "            raw_distance = Contextual_Loss._create_using_L1(I_features, T_features)\n",
        "        elif self.distanceType == 'l2':\n",
        "            raw_distance = Contextual_Loss._create_using_L2(I_features, T_features)\n",
        "        else: # self.distanceType == 'cosine':\n",
        "            raw_distance = Contextual_Loss._create_using_dotP(I_features, T_features)\n",
        "        dist_tilde = Contextual_Loss._calculate_relative_distance(raw_distance)\n",
        "        exp_distance = torch.exp((self.b - dist_tilde) / self.band_width) # Eq(3)\n",
        "        cx_feat = exp_distance / torch.sum(exp_distance, dim=-1, keepdim=True) # Eq(4)\n",
        "\n",
        "        # combined loss\n",
        "        cx_combine = (1. - weight_sp) * cx_feat + weight_sp * cx_sp\n",
        "        k_max_NC, _ = torch.max(cx_combine, dim=2, keepdim=True)\n",
        "        cx = k_max_NC.mean(dim=1)\n",
        "        cx_loss = torch.mean(-torch.log(cx + 1e-5))\n",
        "        return cx_loss\n",
        "\n",
        "    def calculate_CX_Loss(self, I_features, T_features):\n",
        "        device = I_features.device\n",
        "        T_features = T_features.to(device)\n",
        "\n",
        "        if torch.sum(torch.isnan(I_features)) == torch.numel(I_features) or torch.sum(torch.isinf(I_features)) == torch.numel(I_features):\n",
        "            print(I_features)\n",
        "            raise ValueError('NaN or Inf in I_features')\n",
        "        if torch.sum(torch.isnan(T_features)) == torch.numel(T_features) or torch.sum(\n",
        "                torch.isinf(T_features)) == torch.numel(T_features):\n",
        "            print(T_features)\n",
        "            raise ValueError('NaN or Inf in T_features')\n",
        "\n",
        "        # calculate raw distances\n",
        "        if self.distanceType == 'l1':\n",
        "            raw_distance = Contextual_Loss._create_using_L1(I_features, T_features)\n",
        "        elif self.distanceType == 'l2':\n",
        "            raw_distance = Contextual_Loss._create_using_L2(I_features, T_features)\n",
        "        else: # self.distanceType == 'cosine':\n",
        "            raw_distance = Contextual_Loss._create_using_dotP(I_features, T_features)\n",
        "        if torch.sum(torch.isnan(raw_distance)) == torch.numel(raw_distance) or torch.sum(\n",
        "                torch.isinf(raw_distance)) == torch.numel(raw_distance):\n",
        "            print(raw_distance)\n",
        "            raise ValueError('NaN or Inf in raw_distance')\n",
        "\n",
        "        # normalizing the distances\n",
        "        relative_distance = Contextual_Loss._calculate_relative_distance(raw_distance)\n",
        "        if torch.sum(torch.isnan(relative_distance)) == torch.numel(relative_distance) or torch.sum(\n",
        "                torch.isinf(relative_distance)) == torch.numel(relative_distance):\n",
        "            print(relative_distance)\n",
        "            raise ValueError('NaN or Inf in relative_distance')\n",
        "        del raw_distance\n",
        "\n",
        "        #compute_sim()\n",
        "        # where h>0 is a band-width parameter\n",
        "        exp_distance = torch.exp((self.b - relative_distance) / self.band_width) # Eq(3)\n",
        "        if torch.sum(torch.isnan(exp_distance)) == torch.numel(exp_distance) or torch.sum(\n",
        "                torch.isinf(exp_distance)) == torch.numel(exp_distance):\n",
        "            print(exp_distance)\n",
        "            raise ValueError('NaN or Inf in exp_distance')\n",
        "        del relative_distance\n",
        "        \n",
        "        # Similarity\n",
        "        contextual_sim = exp_distance / torch.sum(exp_distance, dim=-1, keepdim=True) # Eq(4)\n",
        "        if torch.sum(torch.isnan(contextual_sim)) == torch.numel(contextual_sim) or torch.sum(\n",
        "                torch.isinf(contextual_sim)) == torch.numel(contextual_sim):\n",
        "            print(contextual_sim)\n",
        "            raise ValueError('NaN or Inf in contextual_sim')\n",
        "        del exp_distance\n",
        "        \n",
        "        #contextual_loss()\n",
        "        max_gt_sim = torch.max(torch.max(contextual_sim, dim=1)[0], dim=1)[0] # Eq(1)\n",
        "        del contextual_sim\n",
        "        CS = torch.mean(max_gt_sim, dim=1)\n",
        "        CX_loss = torch.mean(-torch.log(CS)) # Eq(5)\n",
        "        if torch.isnan(CX_loss):\n",
        "            raise ValueError('NaN in computing CX_loss')\n",
        "        return CX_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#two_stage.py\n",
        "\n",
        "# Differentiable Augmentation for Data-Efficient GAN Training\n",
        "# Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han\n",
        "# https://arxiv.org/pdf/2006.10738\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "policy = 'color,translation,cutout'\n",
        "\n",
        "def DiffAugment(x, policy='', channels_first=True):\n",
        "    if policy:\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 3, 1, 2)\n",
        "        for p in policy.split(','):\n",
        "            for f in AUGMENT_FNS[p]:\n",
        "                x = f(x)\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 2, 3, 1)\n",
        "        x = x.contiguous()\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_brightness(x):\n",
        "    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_saturation(x):\n",
        "    x_mean = x.mean(dim=1, keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_contrast(x):\n",
        "    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_translation(x, ratio=0.125):\n",
        "    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
        "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
        "    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
        "    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_cutout(x, ratio=0.5):\n",
        "    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
        "    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
        "    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
        "    mask[grid_batch, grid_x, grid_y] = 0\n",
        "    x = x * mask.unsqueeze(1)\n",
        "    return x\n",
        "\n",
        "\n",
        "AUGMENT_FNS = {\n",
        "    'color': [rand_brightness, rand_saturation, rand_contrast],\n",
        "    'translation': [rand_translation],\n",
        "    'cutout': [rand_cutout],\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import os.path as osp\n",
        "from pathlib import Path\n",
        "\n",
        "import mmcv\n",
        "import torch\n",
        "from mmedit.core import tensor2img\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from ..common.model_utils import set_requires_grad\n",
        "from ..registry import MODELS\n",
        "from .one_stage import OneStageInpaintor\n",
        "\n",
        "#from DiffAugment_pytorch import DiffAugment\n",
        "\n",
        "@MODELS.register_module()\n",
        "class TwoStageInpaintor(OneStageInpaintor):\n",
        "    \"\"\"Two-Stage Inpaintor.\n",
        "\n",
        "    Currently, we support these loss types in each of two stage inpaintors:\n",
        "    ['loss_gan', 'loss_l1_hole', 'loss_l1_valid', 'loss_composed_percep',\\\n",
        "     'loss_out_percep', 'loss_tv']\n",
        "    The `stage1_loss_type` and `stage2_loss_type` should be chosen from these\n",
        "    loss types.\n",
        "\n",
        "    Args:\n",
        "        stage1_loss_type (tuple[str]): Contains the loss names used in the\n",
        "            first stage model.\n",
        "        stage2_loss_type (tuple[str]): Contains the loss names used in the\n",
        "            second stage model.\n",
        "        input_with_ones (bool): Whether to concatenate an extra ones tensor in\n",
        "            input. Default: True.\n",
        "        disc_input_with_mask (bool): Whether to add mask as input in\n",
        "            discriminator. Default: False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 *args,\n",
        "                 stage1_loss_type=('loss_l1_hole', ),\n",
        "                 stage2_loss_type=('loss_l1_hole', 'loss_gan'),\n",
        "                 input_with_ones=True,\n",
        "                 disc_input_with_mask=False,\n",
        "                 **kwargs):\n",
        "        super(TwoStageInpaintor, self).__init__(*args, **kwargs)\n",
        "\n",
        "        self.stage1_loss_type = stage1_loss_type\n",
        "        self.stage2_loss_type = stage2_loss_type\n",
        "        self.input_with_ones = input_with_ones\n",
        "        self.disc_input_with_mask = disc_input_with_mask\n",
        "        self.eval_with_metrics = ('metrics' in self.test_cfg) and (\n",
        "            self.test_cfg['metrics'] is not None)\n",
        "\n",
        "        # new loss\n",
        "        \n",
        "        # #l_hfen_type = CharbonnierLoss() # nn.L1Loss(), nn.MSELoss(), CharbonnierLoss(), ElasticLoss(), RelativeL1(), L1CosineSim()\n",
        "        l_hfen_type = L1CosineSim()\n",
        "        self.HFENLoss = HFENLoss(loss_f=l_hfen_type, kernel='log', kernel_size=15, sigma = 2.5, norm = False)\n",
        "\n",
        "        self.ElasticLoss = ElasticLoss(a=0.2, reduction='mean')\n",
        "\n",
        "        self.RelativeL1 = RelativeL1(eps=.01, reduction='mean')\n",
        "\n",
        "        self.L1CosineSim = L1CosineSim(loss_lambda=5, reduction='mean')\n",
        "\n",
        "        self.ClipL1 = ClipL1(clip_min=0.0, clip_max=10.0)\n",
        "\n",
        "        self.FFTloss = FFTloss(loss_f = torch.nn.L1Loss, reduction='mean')\n",
        "\n",
        "        self.OFLoss = OFLoss()\n",
        "\n",
        "        self.GPLoss = GPLoss(trace=False, spl_denorm=False)\n",
        "\n",
        "        self.CPLoss = CPLoss(rgb=True, yuv=True, yuvgrad=True, trace=False, spl_denorm=False, yuv_denorm=False)\n",
        "\n",
        "        layers_weights = {'conv_1_1': 1.0, 'conv_3_2': 1.0}\n",
        "        self.Contextual_Loss = Contextual_Loss(layers_weights, crop_quarter=False, max_1d_size=100, \n",
        "            distance_type = 'cosine', b=1.0, band_width=0.5, \n",
        "            use_vgg = True, net = 'vgg19', calc_type = 'regular')\n",
        "\n",
        "\n",
        "\n",
        "    def forward_test(self,\n",
        "                     masked_img,\n",
        "                     mask,\n",
        "                     save_image=False,\n",
        "                     save_path=None,\n",
        "                     iteration=None,\n",
        "                     **kwargs):\n",
        "        \"\"\"Forward function for testing.\n",
        "\n",
        "        Args:\n",
        "            masked_img (torch.Tensor): Tensor with shape of (n, 3, h, w).\n",
        "            mask (torch.Tensor): Tensor with shape of (n, 1, h, w).\n",
        "            save_image (bool, optional): If True, results will be saved as\n",
        "                image. Defaults to False.\n",
        "            save_path (str, optional): If given a valid str, the reuslts will\n",
        "                be saved in this path. Defaults to None.\n",
        "            iteration (int, optional): Iteration number. Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            dict: Contain output results and eval metrics (if have).\n",
        "        \"\"\"\n",
        "        if self.input_with_ones:\n",
        "            tmp_ones = torch.ones_like(mask)\n",
        "            input_x = torch.cat([masked_img, tmp_ones, mask], dim=1)\n",
        "        else:\n",
        "            input_x = torch.cat([masked_img, mask], dim=1)\n",
        "        stage1_fake_res, stage2_fake_res = self.generator(input_x)\n",
        "        fake_img = stage2_fake_res * mask + masked_img * (1. - mask)\n",
        "        output = dict()\n",
        "        eval_results = {}\n",
        "        if self.eval_with_metrics:\n",
        "            gt_img = kwargs['gt_img']\n",
        "            data_dict = dict(\n",
        "                gt_img=gt_img, fake_res=stage2_fake_res, mask=mask)\n",
        "            for metric_name in self.test_cfg['metrics']:\n",
        "                if metric_name in ['ssim', 'psnr']:\n",
        "                    eval_results[metric_name] = self._eval_metrics[\n",
        "                        metric_name](tensor2img(fake_img, min_max=(-1, 1)),\n",
        "                                     tensor2img(gt_img, min_max=(-1, 1)))\n",
        "                else:\n",
        "                    eval_results[metric_name] = self._eval_metrics[\n",
        "                        metric_name]()(data_dict).item()\n",
        "            output['eval_results'] = eval_results\n",
        "        else:\n",
        "            output['stage1_fake_res'] = stage1_fake_res\n",
        "            output['stage2_fake_res'] = stage2_fake_res\n",
        "            output['fake_res'] = stage2_fake_res\n",
        "            output['fake_img'] = fake_img\n",
        "\n",
        "        output['meta'] = None if 'meta' not in kwargs else kwargs['meta'][0]\n",
        "\n",
        "        if save_image:\n",
        "            assert save_image and save_path is not None, (\n",
        "                'Save path should be given')\n",
        "            assert output['meta'] is not None, (\n",
        "                'Meta information should be given to save image.')\n",
        "\n",
        "            tmp_filename = output['meta']['gt_img_path']\n",
        "            filestem = Path(tmp_filename).stem\n",
        "            if iteration is not None:\n",
        "                filename = f'{filestem}_{iteration}.png'\n",
        "            else:\n",
        "                filename = f'{filestem}.png'\n",
        "            mmcv.mkdir_or_exist(save_path)\n",
        "            img_list = [kwargs['gt_img']] if 'gt_img' in kwargs else []\n",
        "            img_list.extend([\n",
        "                masked_img,\n",
        "                mask.expand_as(masked_img), stage1_fake_res, stage2_fake_res,\n",
        "                fake_img\n",
        "            ])\n",
        "            img = torch.cat(img_list, dim=3).cpu()\n",
        "            self.save_visualization(img, osp.join(save_path, filename))\n",
        "            output['save_img_path'] = osp.abspath(\n",
        "                osp.join(save_path, filename))\n",
        "\n",
        "        return output\n",
        "\n",
        "    def save_visualization(self, img, filename):\n",
        "        \"\"\"Save visualization results.\n",
        "\n",
        "        Args:\n",
        "            img (torch.Tensor): Tensor with shape of (n, 3, h, w).\n",
        "            filename (str): Path to save visualization.\n",
        "        \"\"\"\n",
        "        if self.test_cfg.get('img_rerange', True):\n",
        "            img = (img + 1) / 2\n",
        "        if self.test_cfg.get('img_bgr2rgb', True):\n",
        "            img = img[:, [2, 1, 0], ...]\n",
        "        save_image(img, filename, nrow=1, padding=0)\n",
        "\n",
        "    def two_stage_loss(self, stage1_data, stage2_data, data_batch):\n",
        "        \"\"\"Calculate two-stage loss.\n",
        "\n",
        "        Args:\n",
        "            stage1_data (dict): Contain stage1 results.\n",
        "            stage2_data (dict): Contain stage2 results.\n",
        "            data_batch (dict): Contain data needed to calculate loss.\n",
        "\n",
        "        Returns:\n",
        "            dict: Contain losses with name.\n",
        "        \"\"\"\n",
        "        gt = data_batch['gt_img']\n",
        "        mask = data_batch['mask']\n",
        "        masked_img = data_batch['masked_img']\n",
        "\n",
        "        loss = dict()\n",
        "        results = dict(\n",
        "            gt_img=gt.cpu(), mask=mask.cpu(), masked_img=masked_img.cpu())\n",
        "        # calculate losses for stage1\n",
        "        if self.stage1_loss_type is not None:\n",
        "            fake_res = stage1_data['fake_res']\n",
        "            fake_img = stage1_data['fake_img']\n",
        "            for type_key in self.stage1_loss_type:\n",
        "                tmp_loss = self.calculate_loss_with_type(\n",
        "                    type_key, fake_res, fake_img, gt, mask, prefix='stage1_')\n",
        "                loss.update(tmp_loss)\n",
        "\n",
        "        results.update(\n",
        "            dict(\n",
        "                stage1_fake_res=stage1_data['fake_res'].cpu(),\n",
        "                stage1_fake_img=stage1_data['fake_img'].cpu()))\n",
        "\n",
        "        if self.stage2_loss_type is not None:\n",
        "            fake_res = stage2_data['fake_res']\n",
        "            fake_img = stage2_data['fake_img']\n",
        "            for type_key in self.stage2_loss_type:\n",
        "                tmp_loss = self.calculate_loss_with_type(\n",
        "                    type_key, fake_res, fake_img, gt, mask, prefix='stage2_')\n",
        "                loss.update(tmp_loss)\n",
        "        results.update(\n",
        "            dict(\n",
        "                stage2_fake_res=stage2_data['fake_res'].cpu(),\n",
        "                stage2_fake_img=stage2_data['fake_img'].cpu()))\n",
        "\n",
        "        return results, loss\n",
        "\n",
        "    def calculate_loss_with_type(self,\n",
        "                                 loss_type,\n",
        "                                 fake_res,\n",
        "                                 fake_img,\n",
        "                                 gt,\n",
        "                                 mask,\n",
        "                                 prefix='stage1_'):\n",
        "        \"\"\"Calculate multiple types of losses.\n",
        "\n",
        "        Args:\n",
        "            loss_type (str): Type of the loss.\n",
        "            fake_res (torch.Tensor): Direct results from model.\n",
        "            fake_img (torch.Tensor): Composited results from model.\n",
        "            gt (torch.Tensor): Ground-truth tensor.\n",
        "            mask (torch.Tensor): Mask tensor.\n",
        "            prefix (str, optional): Prefix for loss name.\n",
        "                Defaults to 'stage1_'.\n",
        "\n",
        "        Returns:\n",
        "            dict: Contain loss value with its name.\n",
        "        \"\"\"\n",
        "        loss_dict = dict()\n",
        "        if loss_type == 'loss_gan':\n",
        "            if self.disc_input_with_mask:\n",
        "                disc_input_x = torch.cat([fake_img, mask], dim=1)\n",
        "            else:\n",
        "                disc_input_x = fake_img\n",
        "            g_fake_pred = self.disc(disc_input_x)\n",
        "            #############################################################\n",
        "            #loss_g_fake = self.loss_gan(g_fake_pred, True, is_disc=False)\n",
        "            #loss_g_fake = (DiffAugment(g_fake_pred, policy=policy)) #DiffAug\n",
        "\n",
        "            #alternativ:\n",
        "            g_fake_pred = DiffAugment(g_fake_pred, policy=policy)\n",
        "            loss_g_fake = self.loss_gan(g_fake_pred, True, is_disc=False)\n",
        "            ##############################################################\n",
        "            loss_dict[prefix + 'loss_g_fake'] = loss_g_fake\n",
        "        elif 'percep' in loss_type:\n",
        "            loss_pecep, loss_style = self.loss_percep(fake_img, gt)\n",
        "            if loss_pecep is not None:\n",
        "                loss_dict[prefix + loss_type] = loss_pecep\n",
        "            if loss_style is not None:\n",
        "                loss_dict[prefix + loss_type[:-6] + 'style'] = loss_style\n",
        "        elif 'tv' in loss_type:\n",
        "            loss_tv = self.loss_tv(fake_img, mask=mask)\n",
        "            loss_dict[prefix + loss_type] = loss_tv\n",
        "        elif 'l1' in loss_type:\n",
        "            weight = 1. - mask if 'valid' in loss_type else mask\n",
        "            loss_l1 = getattr(self, loss_type)(fake_res, gt, weight=weight)\n",
        "            loss_dict[prefix + loss_type] = loss_l1\n",
        "        # new\n",
        "        elif 'HFEN' in loss_type:\n",
        "            loss_hfen = self.HFENLoss(fake_img, gt)\n",
        "            loss_dict[prefix + loss_type] = loss_hfen\n",
        "        elif 'Elastic' in loss_type:\n",
        "            loss_elastic = self.ElasticLoss(fake_img, gt)\n",
        "            loss_dict[prefix + loss_type] = loss_elastic\n",
        "        elif 'RelativeL1' in loss_type:\n",
        "            loss_relativel1 = self.RelativeL1(fake_img, gt)\n",
        "            loss_dict[prefix + loss_type] = loss_relativel1\n",
        "        elif 'L1CosineSim' in loss_type:\n",
        "            loss_l1cosinesim = self.L1CosineSim(fake_img, gt)\n",
        "            loss_dict[prefix + loss_type] = loss_l1cosinesim\n",
        "        elif 'ClipL1' in loss_type:\n",
        "            loss_clipl1 = self.ClipL1(fake_img, gt)\n",
        "            loss_dict[prefix + loss_type] = loss_clipl1\n",
        "        elif 'FFT' in loss_type:\n",
        "            loss_fft = self.FFTloss(fake_img, gt)\n",
        "            loss_dict[prefix + loss_type] = loss_fft\n",
        "        elif 'OF' in loss_type:\n",
        "            loss_of = self.FFTloss(fake_img)\n",
        "            loss_dict[prefix + loss_type] = loss_of\n",
        "        elif 'GP' in loss_type:\n",
        "            loss_gp = self.FFTloss(fake_img, gt)\n",
        "            loss_dict[prefix + loss_type] = loss_gp\n",
        "        elif 'CP' in loss_type:\n",
        "            loss_cp = self.FFTloss(fake_img, gt)\n",
        "            loss_dict[prefix + loss_type] = loss_cp\n",
        "        else:\n",
        "            raise NotImplementedError(\n",
        "                f'Please check your loss type {loss_type}'\n",
        "                f' and the config dict in init function. '\n",
        "                f'We cannot find the related loss function.')\n",
        "\n",
        "        return loss_dict\n",
        "\n",
        "    def train_step(self, data_batch, optimizer):\n",
        "        \"\"\"Train step function.\n",
        "\n",
        "        In this function, the inpaintor will finish the train step following\n",
        "        the pipeline:\n",
        "\n",
        "            1. get fake res/image\n",
        "            2. optimize discriminator (if have)\n",
        "            3. optimize generator\n",
        "\n",
        "        If `self.train_cfg.disc_step > 1`, the train step will contain multiple\n",
        "        iterations for optimizing discriminator with different input data and\n",
        "        only one iteration for optimizing gerator after `disc_step` iterations\n",
        "        for discriminator.\n",
        "\n",
        "        Args:\n",
        "            data_batch (torch.Tensor): Batch of data as input.\n",
        "            optimizer (dict[torch.optim.Optimizer]): Dict with optimizers for\n",
        "                generator and discriminator (if have).\n",
        "\n",
        "        Returns:\n",
        "            dict: Dict with loss, information for logger, the number of \\\n",
        "                samples and results for visualization.\n",
        "        \"\"\"\n",
        "        log_vars = {}\n",
        "\n",
        "        gt_img = data_batch['gt_img']\n",
        "        mask = data_batch['mask']\n",
        "        masked_img = data_batch['masked_img']\n",
        "\n",
        "        # get common output from encdec\n",
        "        if self.input_with_ones:\n",
        "            tmp_ones = torch.ones_like(mask)\n",
        "            input_x = torch.cat([masked_img, tmp_ones, mask], dim=1)\n",
        "        else:\n",
        "            input_x = torch.cat([masked_img, mask], dim=1)\n",
        "        stage1_fake_res, stage2_fake_res = self.generator(input_x)\n",
        "        stage1_fake_img = masked_img * (1. - mask) + stage1_fake_res * mask\n",
        "        stage2_fake_img = masked_img * (1. - mask) + stage2_fake_res * mask\n",
        "\n",
        "        # discriminator training step\n",
        "        # In this version, we only use the results from the second stage to\n",
        "        # train discriminators, which is a commonly used setting. This can be\n",
        "        # easily modified to your custom training schedule.\n",
        "        if self.train_cfg.disc_step > 0:\n",
        "            set_requires_grad(self.disc, True)\n",
        "            if self.disc_input_with_mask:\n",
        "                disc_input_x = torch.cat([stage2_fake_img.detach(), mask],\n",
        "                                         dim=1)\n",
        "            else:\n",
        "                disc_input_x = stage2_fake_img.detach()\n",
        "            disc_losses = self.forward_train_d(disc_input_x, False, is_disc=True)\n",
        "            loss_disc, log_vars_d = self.parse_losses(disc_losses)\n",
        "            log_vars.update(log_vars_d)\n",
        "            optimizer['disc'].zero_grad()\n",
        "            loss_disc.backward()\n",
        "\n",
        "            if self.disc_input_with_mask:\n",
        "                disc_input_x = torch.cat([gt_img, mask], dim=1)\n",
        "            else:\n",
        "                disc_input_x = gt_img\n",
        "            disc_losses = self.forward_train_d(disc_input_x, True, is_disc=True)\n",
        "            loss_disc, log_vars_d = self.parse_losses(disc_losses)\n",
        "            log_vars.update(log_vars_d)\n",
        "            loss_disc.backward()\n",
        "\n",
        "            if self.with_gp_loss:\n",
        "                # gradient penalty loss should not be used with mask as input\n",
        "                assert not self.disc_input_with_mask\n",
        "                loss_d_gp = self.loss_gp(self.disc, gt_img, stage2_fake_img, mask=mask)\n",
        "                loss_disc, log_vars_d = self.parse_losses(dict(loss_gp=loss_d_gp))\n",
        "                log_vars.update(log_vars_d)\n",
        "                loss_disc.backward()\n",
        "\n",
        "            optimizer['disc'].step()\n",
        "\n",
        "            self.disc_step_count = (self.disc_step_count +\n",
        "                                    1) % self.train_cfg.disc_step\n",
        "            if self.disc_step_count != 0:\n",
        "                # results contain the data for visualization\n",
        "                results = dict(\n",
        "                    gt_img=gt_img.cpu(),\n",
        "                    masked_img=masked_img.cpu(),\n",
        "                    fake_res=stage2_fake_res.cpu(),\n",
        "                    fake_img=stage2_fake_img.cpu())\n",
        "                outputs = dict(\n",
        "                    log_vars=log_vars,\n",
        "                    num_samples=len(data_batch['gt_img'].data),\n",
        "                    results=results)\n",
        "\n",
        "                return outputs\n",
        "\n",
        "        # prepare stage1 results and stage2 results dict for calculating losses\n",
        "        stage1_results = dict(\n",
        "            fake_res=stage1_fake_res, fake_img=stage1_fake_img)\n",
        "        stage2_results = dict(\n",
        "            fake_res=stage2_fake_res, fake_img=stage2_fake_img)\n",
        "\n",
        "        # generator (encdec) and refiner training step, results contain the\n",
        "        # data for visualization\n",
        "        if self.with_gan:\n",
        "            set_requires_grad(self.disc, False)\n",
        "        results, two_stage_losses = self.two_stage_loss(stage1_results, stage2_results, data_batch)\n",
        "        loss_two_stage, log_vars_two_stage = self.parse_losses(two_stage_losses)\n",
        "        log_vars.update(log_vars_two_stage)\n",
        "        optimizer['generator'].zero_grad()\n",
        "        loss_two_stage.backward()\n",
        "        optimizer['generator'].step()\n",
        "\n",
        "        outputs = dict(\n",
        "            log_vars=log_vars,\n",
        "            num_samples=len(data_batch['gt_img'].data),\n",
        "            results=results)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# generator\n",
        "two_stage.py / train_step()\n",
        "v\n",
        "two_stage.py / two_stage_loss()\n",
        "v\n",
        "two_stage.py / calculate_loss_with_type()\n",
        "v\n",
        "two_stage.py / loss_g_fake = self.loss_gan(g_fake_pred, True, is_disc=False)\n",
        "\n",
        "# discriminator\n",
        "two_stage.py / train_step()\n",
        "v\n",
        "two_stage.py -> one_stage.py / forward_train_d()\n",
        "v\n",
        "one_stage.py / loss = dict(real_loss=loss_) if is_real else dict(fake_loss=loss_)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10eWTHaHlZaS",
        "cellView": "form"
      },
      "source": [
        "#@title custom.py + new loss (custom.py)\n",
        "%%writefile /content/Colab-mmediting/configs/inpainting/deepfillv2/custom.py\n",
        "model = dict(\n",
        "    type='TwoStageInpaintor',\n",
        "    disc_input_with_mask=True,\n",
        "    encdec=dict(\n",
        "        type='DeepFillEncoderDecoder',\n",
        "        stage1=dict(\n",
        "            type='GLEncoderDecoder',\n",
        "            encoder=dict(\n",
        "                type='DeepFillEncoder',\n",
        "                conv_type='gated_conv',\n",
        "                channel_factor=0.75,\n",
        "                padding_mode='reflect'),\n",
        "            decoder=dict(\n",
        "                type='DeepFillDecoder',\n",
        "                conv_type='gated_conv',\n",
        "                in_channels=96,\n",
        "                channel_factor=0.75,\n",
        "                out_act_cfg=dict(type='Tanh'),\n",
        "                padding_mode='reflect'),\n",
        "            dilation_neck=dict(\n",
        "                type='GLDilationNeck',\n",
        "                in_channels=96,\n",
        "                conv_type='gated_conv',\n",
        "                act_cfg=dict(type='ELU'),\n",
        "                padding_mode='reflect')),\n",
        "        stage2=dict(\n",
        "            type='DeepFillRefiner',\n",
        "            encoder_attention=dict(\n",
        "                type='DeepFillEncoder',\n",
        "                encoder_type='stage2_attention',\n",
        "                conv_type='gated_conv',\n",
        "                channel_factor=0.75,\n",
        "                padding_mode='reflect'),\n",
        "            encoder_conv=dict(\n",
        "                type='DeepFillEncoder',\n",
        "                encoder_type='stage2_conv',\n",
        "                conv_type='gated_conv',\n",
        "                channel_factor=0.75,\n",
        "                padding_mode='reflect'),\n",
        "            dilation_neck=dict(\n",
        "                type='GLDilationNeck',\n",
        "                in_channels=96,\n",
        "                conv_type='gated_conv',\n",
        "                act_cfg=dict(type='ELU'),\n",
        "                padding_mode='reflect'),\n",
        "            contextual_attention=dict(\n",
        "                type='ContextualAttentionNeck',\n",
        "                in_channels=96,\n",
        "                conv_type='gated_conv',\n",
        "                padding_mode='reflect'),\n",
        "            decoder=dict(\n",
        "                type='DeepFillDecoder',\n",
        "                in_channels=192,\n",
        "                conv_type='gated_conv',\n",
        "                out_act_cfg=dict(type='Tanh'),\n",
        "                padding_mode='reflect'))),\n",
        "    disc=dict(\n",
        "        # 'GLDiscs', 'ModifiedVGG', 'MultiLayerDiscriminator', 'DeepFillv1Discriminators', 'PatchDiscriminator'\n",
        "        type='MultiLayerDiscriminator',\n",
        "        in_channels=4,\n",
        "        max_channels=256,\n",
        "        fc_in_channels=None,\n",
        "        num_convs=6,\n",
        "        norm_cfg=None,\n",
        "        act_cfg=dict(type='LeakyReLU', negative_slope=0.2),\n",
        "        out_act_cfg=dict(type='LeakyReLU', negative_slope=0.2),\n",
        "        with_spectral_norm=True,\n",
        "    ),\n",
        "    # loss_gan, percep, tv, l1, HFEN, Elastic, RelativeL1, L1CosineSim, ClipL1, FFT, OF, GP, CP\n",
        "    # loss_l1_hole, loss_l1_valid\n",
        "\n",
        "    # default\n",
        "    # stage1_loss_type=('loss_l1_hole', 'loss_l1_valid'),\n",
        "    # stage2_loss_type=('loss_l1_hole', 'loss_l1_valid', 'loss_gan'),\n",
        "    stage1_loss_type=('loss_l1_hole', 'loss_l1_valid'),\n",
        "    stage2_loss_type=('loss_l1_hole', 'loss_l1_valid', 'loss_gan', 'HFEN'),\n",
        "    loss_gan=dict(\n",
        "        type='GANLoss',\n",
        "        gan_type='hinge',\n",
        "        loss_weight=0.1,\n",
        "    ),\n",
        "    loss_l1_hole=dict(\n",
        "        type='L1Loss',\n",
        "        loss_weight=1.0,\n",
        "    ),\n",
        "    loss_l1_valid=dict(\n",
        "        type='L1Loss',\n",
        "        loss_weight=1.0,\n",
        "    ),\n",
        "    pretrained=None)\n",
        "\n",
        "train_cfg = dict(disc_step=1)\n",
        "test_cfg = dict(metrics=['l1', 'psnr', 'ssim'])\n",
        "\n",
        "dataset_type = 'ImgInpaintingDataset'\n",
        "input_shape = (256, 256)\n",
        "\n",
        "train_pipeline = [\n",
        "    dict(type='LoadImageFromFile', key='gt_img'),\n",
        "    dict(\n",
        "        type='LoadMask',\n",
        "        mask_mode='irregular',\n",
        "        mask_config=dict(\n",
        "            num_vertexes=(4, 10),\n",
        "            max_angle=6.0,\n",
        "            length_range=(20, 128),\n",
        "            brush_width=(10, 45),\n",
        "            area_ratio_range=(0.15, 0.65),\n",
        "            img_shape=input_shape)),\n",
        "    dict(\n",
        "        type='Crop',\n",
        "        keys=['gt_img'],\n",
        "        crop_size=(384, 384),\n",
        "        random_crop=True,\n",
        "    ),\n",
        "    dict(\n",
        "        type='Resize',\n",
        "        keys=['gt_img'],\n",
        "        scale=input_shape,\n",
        "        keep_ratio=False,\n",
        "    ),\n",
        "    dict(\n",
        "        type='Normalize',\n",
        "        keys=['gt_img'],\n",
        "        mean=[127.5] * 3,\n",
        "        std=[127.5] * 3,\n",
        "        to_rgb=False),\n",
        "    dict(type='GetMaskedImage'),\n",
        "    dict(\n",
        "        type='Collect',\n",
        "        keys=['gt_img', 'masked_img', 'mask'],\n",
        "        meta_keys=['gt_img_path']),\n",
        "    dict(type='ImageToTensor', keys=['gt_img', 'masked_img', 'mask'])\n",
        "]\n",
        "\n",
        "test_pipeline = train_pipeline\n",
        "\n",
        "data_root = '/content/data'\n",
        "\n",
        "data = dict(\n",
        "    samples_per_gpu=2,\n",
        "    workers_per_gpu=8,\n",
        "    val_samples_per_gpu=1,\n",
        "    val_workers_per_gpu=8,\n",
        "    drop_last=True,\n",
        "    train=dict(\n",
        "        type=dataset_type,\n",
        "        ann_file='/content/train/train.tflist',\n",
        "        data_prefix=data_root,\n",
        "        pipeline=train_pipeline,\n",
        "        test_mode=False),\n",
        "    val=dict(\n",
        "        type=dataset_type,\n",
        "        ann_file='/content/val/val.tflist',\n",
        "        data_prefix=data_root,\n",
        "        pipeline=test_pipeline,\n",
        "        test_mode=True),\n",
        "    test=dict(\n",
        "        type=dataset_type,\n",
        "        ann_file='/content/val/val.tflist',\n",
        "        data_prefix=data_root,\n",
        "        pipeline=test_pipeline,\n",
        "        test_mode=True))\n",
        "\n",
        "optimizers = dict(\n",
        "    generator=dict(type='Adam', lr=0.0001), disc=dict(type='Adam', lr=0.0001))\n",
        "\n",
        "lr_config = dict(policy='Fixed', by_epoch=False)\n",
        "\n",
        "checkpoint_config = dict(by_epoch=False, interval=1000)\n",
        "log_config = dict(\n",
        "    interval=100,\n",
        "    hooks=[\n",
        "        dict(type='TextLoggerHook', by_epoch=False),\n",
        "        dict(type='TensorboardLoggerHook'),\n",
        "        #dict(type='PaviLoggerHook', init_kwargs=dict(project='mmedit')) # not possible to install\n",
        "    ])\n",
        "\n",
        "visual_config = dict(\n",
        "    type='VisualizationHook',\n",
        "    output_dir='visual',\n",
        "    interval=1000,\n",
        "    res_name_list=[\n",
        "        'gt_img', 'masked_img', 'stage1_fake_res', 'stage1_fake_img',\n",
        "        'stage2_fake_res', 'stage2_fake_img', 'fake_gt_local'\n",
        "    ],\n",
        ")\n",
        "\n",
        "evaluation = dict(interval=50000)\n",
        "\n",
        "total_iters = 1000003\n",
        "dist_params = dict(backend='nccl')\n",
        "log_level = 'INFO'\n",
        "work_dir = './work_dirs/test_pggan'\n",
        "load_from = None\n",
        "resume_from = None\n",
        "workflow = [('train', 10000)]\n",
        "exp_name = 'deepfillv2_256x256_8x2_places'\n",
        "find_unused_parameters = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyYsI4ssRqp4",
        "cellView": "form"
      },
      "source": [
        "#@title Train\n",
        "%cd /content/Colab-mmediting\n",
        "!python tools/train.py configs/inpainting/deepfillv2/custom.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3rY96332zAA"
      },
      "source": [
        "# Test trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3biQzN2PzwZP"
      },
      "source": [
        "!python demo/inpainting_demo.py \"configs/inpainting/deepfillv2/custom.py\" \\\n",
        "\"/content/mmediting/work_dirs/test_pggan/iter_XXXXX.pth\" /content/image.png /content/mask.png /content/result.png"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
