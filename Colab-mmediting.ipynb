{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab-mmediting.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5REUXZI7ejq6",
        "colab_type": "text"
      },
      "source": [
        "# Colab-mmediting with [open-mmlab/mmediting](https://github.com/open-mmlab/mmediting)\n",
        "\n",
        "My fork is located in [styler00dollar/Colab-mmediting](https://github.com/styler00dollar/Colab-mmediting)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7SvGgIHiNCF",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Check GPU\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pq-pTiD6LHx3",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Install mmediting reqruirements (installing mmcv takes quite long)\n",
        "%cd /content/\n",
        "!git clone https://github.com/styler00dollar/Colab-mmediting\n",
        "%cd Colab-mmediting\n",
        "!pip install -r requirements.txt\n",
        "!pip install -v -e .  # or \"python setup.py develop\"\n",
        "!pip install tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6giPJBPbb4JU",
        "colab_type": "text"
      },
      "source": [
        "# Test inpainting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5To_7PcbdNm",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Download all models\n",
        "%cd /content/\n",
        "!mkdir /content/models/\n",
        "# deepfillv2\n",
        "!wget --no-check-certificate 'https://openmmlab.oss-accelerate.aliyuncs.com/mmediting/inpainting/deepfillv2/deepfillv2_256x256_8x2_places_20200619-10d15793.pth' -O /content/models/deepfillv2_256x256_8x2_places.pth\n",
        "!wget --no-check-certificate 'https://openmmlab.oss-accelerate.aliyuncs.com/mmediting/inpainting/deepfillv2/deepfillv2_256x256_8x2_celeba_20200619-c96e5f12.pth' -O /content/models/deepfillv2_256x256_8x2_celeba.pth\n",
        "# deepfillv1\n",
        "!wget --no-check-certificate 'https://openmmlab.oss-accelerate.aliyuncs.com/mmediting/inpainting/deepfillv1/deepfillv1_256x256_4x4_celeba_20200619-dd51a855.pth' -O /content/models/deepfillv1_256x256_4x4_celeba.pth\n",
        "!wget --no-check-certificate 'https://openmmlab.oss-accelerate.aliyuncs.com/mmediting/inpainting/deepfillv1/deepfillv1_256x256_8x2_places_20200619-c00a0e21.pth' -O /content/models/deepfillv1_256x256_8x2_places.pth\n",
        "# global_local\n",
        "!wget --no-check-certificate 'https://openmmlab.oss-accelerate.aliyuncs.com/mmediting/inpainting/global_local/gl_256x256_8x12_celeba_20200619-5af0493f.pth' -O /content/models/gl_256x256_8x12_celeba.pth\n",
        "!wget --no-check-certificate 'https://openmmlab.oss-accelerate.aliyuncs.com/mmediting/inpainting/global_local/gl_256x256_8x12_places_20200619-52a040a8.pth' -O /content/models/gl_256x256_8x12_places.pth\n",
        "# partial_conv\n",
        "!wget --no-check-certificate 'https://openmmlab.oss-accelerate.aliyuncs.com/mmediting/inpainting/pconv/pconv_256x256_stage2_4x2_places_20200619-1ffed0e8.pth' -O /content/models/pconv_places.pth\n",
        "!wget --no-check-certificate 'https://openmmlab.oss-accelerate.aliyuncs.com/mmediting/inpainting/pconv/pconv_256x256_stage2_4x2_celeba_20200619-860f8b95.pth' -O /content/models/pconv_celeba.pth\n",
        "%cd /content/Colab-mmediting"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGA8wwgbcIvt",
        "colab_type": "text"
      },
      "source": [
        "Place pictures in ```/content/image.png``` and ```/content/mask.png ```. Result will be in ```/content/result.png```.\n",
        "\n",
        "Info: The mask is a black white image and the masked area is white. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXP0lp7pg7Pv",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Connect Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "print('Google Drive connected.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfbkE2oVemqf",
        "colab_type": "text"
      },
      "source": [
        "# deepfillv2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yLpR2sBOVA-",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title deepfill v2 places\n",
        "!python demo/inpainting_demo.py configs/inpainting/deepfillv2/deepfillv2_256x256_8x2_places.py /content/models/deepfillv2_256x256_8x2_places.pth /content/image.png /content/mask.png /content/result.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKR7CROqc1lP",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title deepfill v2 celeba\n",
        "!python demo/inpainting_demo.py configs/inpainting/deepfillv2/deepfillv2_256x256_8x2_celeba.py /content/models/deepfillv2_256x256_8x2_celeba.pth /content/image.png /content/mask.png /content/result.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MJLf_sWeo7z",
        "colab_type": "text"
      },
      "source": [
        "# deepfillv1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCl0oMrGc_Sw",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title deepfill v1 places\n",
        "!python demo/inpainting_demo.py configs/inpainting/deepfillv1/deepfillv1_256x256_8x2_places.py /content/models/deepfillv1_256x256_8x2_places.pth /content/image.png /content/mask.png /content/result.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMMd4hB6dIOs",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title deepfill v1 celeba\n",
        "!python demo/inpainting_demo.py configs/inpainting/deepfillv1/deepfillv1_256x256_4x4_celeba.py /content/models/deepfillv1_256x256_4x4_celeba.pth /content/image.png /content/mask.png /content/result.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7CdL58mer_5",
        "colab_type": "text"
      },
      "source": [
        "# global_local"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBpo1ZQUdPCv",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title global_local places\n",
        "!python demo/inpainting_demo.py configs/inpainting/global_local/gl_256x256_8x12_places.py /content/models/gl_256x256_8x12_places.pth /content/image.png /content/mask.png /content/result.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrhWrbA8dXQl",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title global_local celeba\n",
        "!python demo/inpainting_demo.py configs/inpainting/global_local/gl_256x256_8x12_celeba.py /content/models/gl_256x256_8x12_celeba.pth /content/image.png /content/mask.png /content/result.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3YN4oS27F9v",
        "colab_type": "text"
      },
      "source": [
        "# partial_conv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4X_FZSaFeu8Q",
        "colab_type": "text"
      },
      "source": [
        "partial_conv (Does not accept any image dimension as input. ```250x250, 256x256, 500x500, 501x501, 502x502, 503x503, 504x504, 505x505, 506x506, 507x507, 508x508, 509x509, 510x510, 511x511, 512x512, 624x624, 750x750, 1000x1000, 1001x1001, 1000x1250, 1250x1250, 1256x1256 and 1512x1512``` seem to work. ```180x180, 192x192, 201x201, 325x325, 400x400, 425x425, 444x444, 480x480, 513x513, 514x514, 515x515, 516x516, 517x517, 518x518, 520x520, 524x524, 555x555, 569x569,  666x666, 724x724, 800x800, 812x812, 1500x1500 and 2000x2000``` does not work. I can't really see a pattern.)\n",
        "\n",
        "[Problematic code is here.](https://github.com/open-mmlab/mmediting/blob/master/mmedit/models/backbones/encoder_decoders/decoders/pconv_decoder.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJWS5f7RdwJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# resize input if needed\n",
        "import cv2\n",
        "filepath = '/content/image.png'\n",
        "size = (666,666)\n",
        "\n",
        "image = cv2.imread(filepath)\n",
        "image = cv2.resize(image, size, cv2.INTER_NEAREST)\n",
        "cv2.imwrite(filepath, image)\n",
        "filepath = '/content/mask.png'\n",
        "image = cv2.imread(filepath)\n",
        "image = cv2.resize(image, size, cv2.INTER_NEAREST)\n",
        "cv2.imwrite(filepath, image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2dCAUN9dipe",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title partial_conv 8x1 places\n",
        "!python demo/inpainting_demo.py configs/inpainting/partial_conv/pconv_256x256_stage1_8x1_places.py /content/models/pconv_places.pth /content/image.png /content/mask.png /content/result.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYUx3xlFdepC",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title partial_conv 8x1 celeba\n",
        "!python demo/inpainting_demo.py configs/inpainting/partial_conv/pconv_256x256_stage1_8x1_celeba.py /content/models/pconv_celeba.pth /content/image.png /content/mask.png /content/result.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJ2995x-e4pc",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title partial_conv 4x2 places\n",
        "!python demo/inpainting_demo.py configs/inpainting/partial_conv/pconv_256x256_stage2_4x2_places.py /content/models/pconv_places.pth /content/image.png /content/mask.png /content/result.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TglLOv2Me-OC",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title partial_conv 4x2 celeba\n",
        "!python demo/inpainting_demo.py configs/inpainting/partial_conv/pconv_256x256_stage2_4x2_celeba.py /content/models/pconv_celeba.pth /content/image.png /content/mask.png /content/result.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY9giPLf2nak",
        "colab_type": "text"
      },
      "source": [
        "# Training deepfillv2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLWkPZpTvg_8",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title create empty folders\n",
        "!mkdir /content/train/\n",
        "!mkdir /content/val/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Il9CqQQ2mun",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title custom.py (contains configuration for paths and parameter)\n",
        "%%writefile /content/Colab-mmediting/configs/inpainting/deepfillv2/custom.py\n",
        "model = dict(\n",
        "    type='TwoStageInpaintor',\n",
        "    disc_input_with_mask=True,\n",
        "    encdec=dict(\n",
        "        type='DeepFillEncoderDecoder',\n",
        "        stage1=dict(\n",
        "            type='GLEncoderDecoder',\n",
        "            encoder=dict(\n",
        "                type='DeepFillEncoder',\n",
        "                conv_type='gated_conv',\n",
        "                channel_factor=0.75,\n",
        "                padding_mode='reflect'),\n",
        "            decoder=dict(\n",
        "                type='DeepFillDecoder',\n",
        "                conv_type='gated_conv',\n",
        "                in_channels=96,\n",
        "                channel_factor=0.75,\n",
        "                out_act_cfg=dict(type='Tanh'),\n",
        "                padding_mode='reflect'),\n",
        "            dilation_neck=dict(\n",
        "                type='GLDilationNeck',\n",
        "                in_channels=96,\n",
        "                conv_type='gated_conv',\n",
        "                act_cfg=dict(type='ELU'),\n",
        "                padding_mode='reflect')),\n",
        "        stage2=dict(\n",
        "            type='DeepFillRefiner',\n",
        "            encoder_attention=dict(\n",
        "                type='DeepFillEncoder',\n",
        "                encoder_type='stage2_attention',\n",
        "                conv_type='gated_conv',\n",
        "                channel_factor=0.75,\n",
        "                padding_mode='reflect'),\n",
        "            encoder_conv=dict(\n",
        "                type='DeepFillEncoder',\n",
        "                encoder_type='stage2_conv',\n",
        "                conv_type='gated_conv',\n",
        "                channel_factor=0.75,\n",
        "                padding_mode='reflect'),\n",
        "            dilation_neck=dict(\n",
        "                type='GLDilationNeck',\n",
        "                in_channels=96,\n",
        "                conv_type='gated_conv',\n",
        "                act_cfg=dict(type='ELU'),\n",
        "                padding_mode='reflect'),\n",
        "            contextual_attention=dict(\n",
        "                type='ContextualAttentionNeck',\n",
        "                in_channels=96,\n",
        "                conv_type='gated_conv',\n",
        "                padding_mode='reflect'),\n",
        "            decoder=dict(\n",
        "                type='DeepFillDecoder',\n",
        "                in_channels=192,\n",
        "                conv_type='gated_conv',\n",
        "                out_act_cfg=dict(type='Tanh'),\n",
        "                padding_mode='reflect'))),\n",
        "    disc=dict(\n",
        "        type='MultiLayerDiscriminator',\n",
        "        in_channels=4,\n",
        "        max_channels=256,\n",
        "        fc_in_channels=None,\n",
        "        num_convs=6,\n",
        "        norm_cfg=None,\n",
        "        act_cfg=dict(type='LeakyReLU', negative_slope=0.2),\n",
        "        out_act_cfg=dict(type='LeakyReLU', negative_slope=0.2),\n",
        "        with_spectral_norm=True,\n",
        "    ),\n",
        "    stage1_loss_type=('loss_l1_hole', 'loss_l1_valid'),\n",
        "    stage2_loss_type=('loss_l1_hole', 'loss_l1_valid', 'loss_gan'),\n",
        "    loss_gan=dict(\n",
        "        type='GANLoss',\n",
        "        gan_type='hinge',\n",
        "        loss_weight=0.1,\n",
        "    ),\n",
        "    loss_l1_hole=dict(\n",
        "        type='L1Loss',\n",
        "        loss_weight=1.0,\n",
        "    ),\n",
        "    loss_l1_valid=dict(\n",
        "        type='L1Loss',\n",
        "        loss_weight=1.0,\n",
        "    ),\n",
        "    pretrained=None)\n",
        "\n",
        "train_cfg = dict(disc_step=1)\n",
        "test_cfg = dict(metrics=['l1', 'psnr', 'ssim'])\n",
        "\n",
        "dataset_type = 'ImgInpaintingDataset'\n",
        "input_shape = (256, 256)\n",
        "\n",
        "train_pipeline = [\n",
        "    dict(type='LoadImageFromFile', key='gt_img'),\n",
        "    dict(\n",
        "        type='LoadMask',\n",
        "        mask_mode='irregular',\n",
        "        mask_config=dict(\n",
        "            num_vertexes=(4, 10),\n",
        "            max_angle=6.0,\n",
        "            length_range=(20, 128),\n",
        "            brush_width=(10, 45),\n",
        "            area_ratio_range=(0.15, 0.65),\n",
        "            img_shape=input_shape)),\n",
        "    dict(\n",
        "        type='Crop',\n",
        "        keys=['gt_img'],\n",
        "        crop_size=(384, 384),\n",
        "        random_crop=True,\n",
        "    ),\n",
        "    dict(\n",
        "        type='Resize',\n",
        "        keys=['gt_img'],\n",
        "        scale=input_shape,\n",
        "        keep_ratio=False,\n",
        "    ),\n",
        "    dict(\n",
        "        type='Normalize',\n",
        "        keys=['gt_img'],\n",
        "        mean=[127.5] * 3,\n",
        "        std=[127.5] * 3,\n",
        "        to_rgb=False),\n",
        "    dict(type='GetMaskedImage'),\n",
        "    dict(\n",
        "        type='Collect',\n",
        "        keys=['gt_img', 'masked_img', 'mask'],\n",
        "        meta_keys=['gt_img_path']),\n",
        "    dict(type='ImageToTensor', keys=['gt_img', 'masked_img', 'mask'])\n",
        "]\n",
        "\n",
        "test_pipeline = train_pipeline\n",
        "\n",
        "data_root = '/content/data'\n",
        "\n",
        "data = dict(\n",
        "    samples_per_gpu=2,\n",
        "    workers_per_gpu=8,\n",
        "    val_samples_per_gpu=1,\n",
        "    val_workers_per_gpu=8,\n",
        "    drop_last=True,\n",
        "    train=dict(\n",
        "        type=dataset_type,\n",
        "        ann_file='/content/train/train.tflist',\n",
        "        data_prefix=data_root,\n",
        "        pipeline=train_pipeline,\n",
        "        test_mode=False),\n",
        "    val=dict(\n",
        "        type=dataset_type,\n",
        "        ann_file='/content/val/val.tflist',\n",
        "        data_prefix=data_root,\n",
        "        pipeline=test_pipeline,\n",
        "        test_mode=True),\n",
        "    test=dict(\n",
        "        type=dataset_type,\n",
        "        ann_file='/content/val/val.tflist',\n",
        "        data_prefix=data_root,\n",
        "        pipeline=test_pipeline,\n",
        "        test_mode=True))\n",
        "\n",
        "optimizers = dict(\n",
        "    generator=dict(type='Adam', lr=0.0001), disc=dict(type='Adam', lr=0.0001))\n",
        "\n",
        "lr_config = dict(policy='Fixed', by_epoch=False)\n",
        "\n",
        "checkpoint_config = dict(by_epoch=False, interval=1000)\n",
        "log_config = dict(\n",
        "    interval=100,\n",
        "    hooks=[\n",
        "        dict(type='TextLoggerHook', by_epoch=False),\n",
        "        dict(type='TensorboardLoggerHook'),\n",
        "        #dict(type='PaviLoggerHook', init_kwargs=dict(project='mmedit'))\n",
        "    ])\n",
        "\n",
        "visual_config = dict(\n",
        "    type='VisualizationHook',\n",
        "    output_dir='visual',\n",
        "    interval=1000,\n",
        "    res_name_list=[\n",
        "        'gt_img', 'masked_img', 'stage1_fake_res', 'stage1_fake_img',\n",
        "        'stage2_fake_res', 'stage2_fake_img', 'fake_gt_local'\n",
        "    ],\n",
        ")\n",
        "\n",
        "evaluation = dict(interval=50000)\n",
        "\n",
        "total_iters = 1000003\n",
        "dist_params = dict(backend='nccl')\n",
        "log_level = 'INFO'\n",
        "work_dir = './work_dirs/test_pggan'\n",
        "load_from = None\n",
        "resume_from = None\n",
        "workflow = [('train', 10000)]\n",
        "exp_name = 'deepfillv2_256x256_8x2_places'\n",
        "find_unused_parameters = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-dU6DgA-Nqx",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title removing env info, since it creates a problem (train.py)\n",
        "%%writefile /content/Colab-mmediting/tools/train.py\n",
        "import argparse\n",
        "import copy\n",
        "import os\n",
        "import os.path as osp\n",
        "import time\n",
        "\n",
        "import mmcv\n",
        "import torch\n",
        "from mmcv import Config\n",
        "from mmcv.runner import init_dist\n",
        "\n",
        "from mmedit import __version__\n",
        "from mmedit.apis import set_random_seed, train_model\n",
        "from mmedit.datasets import build_dataset\n",
        "from mmedit.models import build_model\n",
        "from mmedit.utils import collect_env, get_root_logger\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description='Train an editor')\n",
        "    parser.add_argument('config', help='train config file path')\n",
        "    parser.add_argument('--work-dir', help='the dir to save logs and models')\n",
        "    parser.add_argument(\n",
        "        '--resume-from', help='the checkpoint file to resume from')\n",
        "    parser.add_argument(\n",
        "        '--no-validate',\n",
        "        action='store_true',\n",
        "        help='whether not to evaluate the checkpoint during training')\n",
        "    parser.add_argument(\n",
        "        '--gpus',\n",
        "        type=int,\n",
        "        default=1,\n",
        "        help='number of gpus to use '\n",
        "        '(only applicable to non-distributed training)')\n",
        "    parser.add_argument('--seed', type=int, default=None, help='random seed')\n",
        "    parser.add_argument(\n",
        "        '--deterministic',\n",
        "        action='store_true',\n",
        "        help='whether to set deterministic options for CUDNN backend.')\n",
        "    parser.add_argument(\n",
        "        '--launcher',\n",
        "        choices=['none', 'pytorch', 'slurm', 'mpi'],\n",
        "        default='none',\n",
        "        help='job launcher')\n",
        "    parser.add_argument('--local_rank', type=int, default=0)\n",
        "    parser.add_argument(\n",
        "        '--autoscale-lr',\n",
        "        action='store_true',\n",
        "        help='automatically scale lr with the number of gpus')\n",
        "    args = parser.parse_args()\n",
        "    if 'LOCAL_RANK' not in os.environ:\n",
        "        os.environ['LOCAL_RANK'] = str(args.local_rank)\n",
        "\n",
        "    return args\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "\n",
        "    cfg = Config.fromfile(args.config)\n",
        "    # set cudnn_benchmark\n",
        "    if cfg.get('cudnn_benchmark', False):\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "    # update configs according to CLI args\n",
        "    if args.work_dir is not None:\n",
        "        cfg.work_dir = args.work_dir\n",
        "    if args.resume_from is not None:\n",
        "        cfg.resume_from = args.resume_from\n",
        "    cfg.gpus = args.gpus\n",
        "\n",
        "    if args.autoscale_lr:\n",
        "        # apply the linear scaling rule (https://arxiv.org/abs/1706.02677)\n",
        "        cfg.optimizer['lr'] = cfg.optimizer['lr'] * cfg.gpus / 8\n",
        "\n",
        "    # init distributed env first, since logger depends on the dist info.\n",
        "    if args.launcher == 'none':\n",
        "        distributed = False\n",
        "    else:\n",
        "        distributed = True\n",
        "        init_dist(args.launcher, **cfg.dist_params)\n",
        "\n",
        "    # create work_dir\n",
        "    mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
        "    # init the logger before other steps\n",
        "    timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())\n",
        "    log_file = osp.join(cfg.work_dir, f'{timestamp}.log')\n",
        "    logger = get_root_logger(log_file=log_file, log_level=cfg.log_level)\n",
        "\n",
        "    # log env info\n",
        "    #env_info_dict = collect_env()\n",
        "    #env_info = '\\n'.join([f'{k}: {v}' for k, v in env_info_dict.items()])\n",
        "    #dash_line = '-' * 60 + '\\n'\n",
        "    #logger.info('Environment info:\\n' + dash_line + env_info + '\\n' +\n",
        "    #            dash_line)\n",
        "\n",
        "    # log some basic info\n",
        "    logger.info('Distributed training: {}'.format(distributed))\n",
        "    logger.info('mmedit Version: {}'.format(__version__))\n",
        "    logger.info('Config:\\n{}'.format(cfg.text))\n",
        "\n",
        "    # set random seeds\n",
        "    if args.seed is not None:\n",
        "        logger.info('Set random seed to {}, deterministic: {}'.format(\n",
        "            args.seed, args.deterministic))\n",
        "        set_random_seed(args.seed, deterministic=args.deterministic)\n",
        "    cfg.seed = args.seed\n",
        "\n",
        "    model = build_model(\n",
        "        cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n",
        "\n",
        "    datasets = [build_dataset(cfg.data.train)]\n",
        "    if len(cfg.workflow) == 2:\n",
        "        val_dataset = copy.deepcopy(cfg.data.val)\n",
        "        val_dataset.pipeline = cfg.data.train.pipeline\n",
        "        datasets.append(build_dataset(val_dataset))\n",
        "    if cfg.checkpoint_config is not None:\n",
        "        # save version, config file content and class names in\n",
        "        # checkpoints as meta data\n",
        "        cfg.checkpoint_config.meta = dict(\n",
        "            mmedit_version=__version__,\n",
        "            config=cfg.text,\n",
        "        )\n",
        "\n",
        "    # meta information\n",
        "    meta = dict()\n",
        "    if cfg.get('exp_name', None) is None:\n",
        "        cfg['exp_name'] = osp.splitext(osp.basename(cfg.work_dir))[0]\n",
        "    meta['exp_name'] = cfg.exp_name\n",
        "    meta['mmedit Version'] = __version__\n",
        "    meta['seed'] = args.seed\n",
        "    #meta['env_info'] = env_info\n",
        "\n",
        "    # add an attribute for visualization convenience\n",
        "    train_model(\n",
        "        model,\n",
        "        datasets,\n",
        "        cfg,\n",
        "        distributed=distributed,\n",
        "        validate=(not args.no_validate),\n",
        "        timestamp=timestamp,\n",
        "        meta=meta)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQogZKR72tWD",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Train (works on pytorch 1.6, despite the claims in the official repo)\n",
        "%cd /content/Colab-mmediting\n",
        "!python tools/train.py configs/inpainting/deepfillv2/custom.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BC1jaAYRrQ_",
        "colab_type": "text"
      },
      "source": [
        "# Training deepfillv2 with Differentiable Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "negr7_QTTeLP",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title one_stage.py (adding differentiable augmentation for deepfillv2)\n",
        "%%writefile /content/Colab-mmediting/mmedit/models/inpaintors/one_stage.py\n",
        "# Differentiable Augmentation for Data-Efficient GAN Training\n",
        "# Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han\n",
        "# https://arxiv.org/pdf/2006.10738\n",
        "\n",
        "from mmcv.runner import auto_fp16 \n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "policy = 'color,translation,cutout'\n",
        "\n",
        "def DiffAugment(x, policy='', channels_first=True):\n",
        "    if policy:\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 3, 1, 2)\n",
        "        for p in policy.split(','):\n",
        "            for f in AUGMENT_FNS[p]:\n",
        "                x = f(x)\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 2, 3, 1)\n",
        "        x = x.contiguous()\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_brightness(x):\n",
        "    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_saturation(x):\n",
        "    x_mean = x.mean(dim=1, keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_contrast(x):\n",
        "    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_translation(x, ratio=0.125):\n",
        "    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
        "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
        "    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
        "    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_cutout(x, ratio=0.5):\n",
        "    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
        "    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
        "    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
        "    mask[grid_batch, grid_x, grid_y] = 0\n",
        "    x = x * mask.unsqueeze(1)\n",
        "    return x\n",
        "\n",
        "\n",
        "AUGMENT_FNS = {\n",
        "    'color': [rand_brightness, rand_saturation, rand_contrast],\n",
        "    'translation': [rand_translation],\n",
        "    'cutout': [rand_cutout],\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import os.path as osp\n",
        "from pathlib import Path\n",
        "\n",
        "import mmcv\n",
        "import torch\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from mmedit.core import L1Evaluation, psnr, ssim, tensor2img\n",
        "from ..base import BaseModel\n",
        "from ..builder import build_backbone, build_component, build_loss\n",
        "from ..common import set_requires_grad\n",
        "from ..registry import MODELS\n",
        "\n",
        "\n",
        "@MODELS.register_module()\n",
        "class OneStageInpaintor(BaseModel):\n",
        "    \"\"\"Standard one-stage inpaintor with commonly used losses.\n",
        "\n",
        "    An inpaintor must contain an encoder-decoder style generator to\n",
        "    inpaint masked regions. A discriminator will be adopted when\n",
        "    adversarial training is needed.\n",
        "\n",
        "    In this class, we provide a common interface for inpaintors.\n",
        "    For other inpaintors, only some funcs may be modified to fit the\n",
        "    input style or training schedule.\n",
        "\n",
        "    Args:\n",
        "        generator (dict): Config for encoder-decoder style generator.\n",
        "        disc (dict): Config for discriminator.\n",
        "        loss_gan (dict): Config for adversarial loss.\n",
        "        loss_gp (dict): Config for gradient penalty loss.\n",
        "        loss_disc_shift (dict): Config for discriminator shift loss.\n",
        "        loss_composed_percep (dict): Config for perceptural and style loss with\n",
        "            composed image as input.\n",
        "        loss_out_percep (dict): Config for perceptural and style loss with\n",
        "            direct output as input.\n",
        "        loss_l1_hole (dict): Config for l1 loss in the hole.\n",
        "        loss_l1_valid (dict): Config for l1 loss in the valid region.\n",
        "        loss_tv (dict): Config for total variation loss.\n",
        "        train_cfg (dict): Configs for training scheduler. `disc_step` must be\n",
        "            contained for indicates the discriminator updating steps in each\n",
        "            training step.\n",
        "        test_cfg (dict): Configs for testing scheduler.\n",
        "        pretrained (str): Path for pretrained model. Default None.\n",
        "    \"\"\"\n",
        "    _eval_metrics = dict(l1=L1Evaluation, psnr=psnr, ssim=ssim)\n",
        "\n",
        "    def __init__(self,\n",
        "                 encdec,\n",
        "                 disc=None,\n",
        "                 loss_gan=None,\n",
        "                 loss_gp=None,\n",
        "                 loss_disc_shift=None,\n",
        "                 loss_composed_percep=None,\n",
        "                 loss_out_percep=False,\n",
        "                 loss_l1_hole=None,\n",
        "                 loss_l1_valid=None,\n",
        "                 loss_tv=None,\n",
        "                 train_cfg=None,\n",
        "                 test_cfg=None,\n",
        "                 pretrained=None):\n",
        "        super(OneStageInpaintor, self).__init__()\n",
        "        self.with_l1_hole_loss = loss_l1_hole is not None\n",
        "        self.with_l1_valid_loss = loss_l1_valid is not None\n",
        "        self.with_tv_loss = loss_tv is not None\n",
        "        self.with_composed_percep_loss = loss_composed_percep is not None\n",
        "        self.with_out_percep_loss = loss_out_percep\n",
        "        self.with_gan = disc is not None and loss_gan is not None\n",
        "        self.with_gp_loss = loss_gp is not None\n",
        "        self.with_disc_shift_loss = loss_disc_shift is not None\n",
        "        self.is_train = train_cfg is not None\n",
        "        self.train_cfg = train_cfg\n",
        "        self.test_cfg = test_cfg\n",
        "        self.eval_with_metrics = ('metrics' in self.test_cfg) and (\n",
        "            self.test_cfg['metrics'] is not None)\n",
        "\n",
        "        self.generator = build_backbone(encdec)\n",
        "\n",
        "        self.fp16_enabled = False \n",
        "\n",
        "        # build loss modules\n",
        "        if self.with_gan:\n",
        "            self.disc = build_component(disc)\n",
        "            self.loss_gan = build_loss(loss_gan)\n",
        "\n",
        "        if self.with_l1_hole_loss:\n",
        "            self.loss_l1_hole = build_loss(loss_l1_hole)\n",
        "\n",
        "        if self.with_l1_valid_loss:\n",
        "            self.loss_l1_valid = build_loss(loss_l1_valid)\n",
        "\n",
        "        if self.with_composed_percep_loss:\n",
        "            self.loss_percep = build_loss(loss_composed_percep)\n",
        "\n",
        "        if self.with_gp_loss:\n",
        "            self.loss_gp = build_loss(loss_gp)\n",
        "\n",
        "        if self.with_disc_shift_loss:\n",
        "            self.loss_disc_shift = build_loss(loss_disc_shift)\n",
        "\n",
        "        if self.with_tv_loss:\n",
        "            self.loss_tv = build_loss(loss_tv)\n",
        "\n",
        "        self.disc_step_count = 0\n",
        "        self.init_weights(pretrained=pretrained)\n",
        "\n",
        "    def init_weights(self, pretrained=None):\n",
        "        \"\"\"Init weights for models.\n",
        "\n",
        "        Args:\n",
        "            pretrained (str, optional): Path for pretrained weights. If given\n",
        "                None, pretrained weights will not be loaded. Defaults to None.\n",
        "        \"\"\"\n",
        "        self.generator.init_weights(pretrained=pretrained)\n",
        "        if self.with_gan:\n",
        "            self.disc.init_weights(pretrained=pretrained)\n",
        "\n",
        "    @auto_fp16(apply_to=('masked_img', 'mask')) \n",
        "    def forward(self, masked_img, mask, test_mode=True, **kwargs):\n",
        "        \"\"\"Forward function.\n",
        "\n",
        "        Args:\n",
        "            masked_img (torch.Tensor): Image with hole as input.\n",
        "            mask (torch.Tensor): Mask as input.\n",
        "            test_mode (bool, optional): Whether use testing mode.\n",
        "                Defaults to True.\n",
        "\n",
        "        Returns:\n",
        "            dict: Dict contains output results.\n",
        "        \"\"\"\n",
        "        if not test_mode:\n",
        "            return self.forward_train(masked_img, mask, **kwargs)\n",
        "        else:\n",
        "            return self.forward_test(masked_img, mask, **kwargs)\n",
        "\n",
        "    def forward_train(self, *args, **kwargs):\n",
        "        \"\"\"Forward function for training.\n",
        "\n",
        "        In this version, we do not use this interface.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError('This interface should not be used in '\n",
        "                                  'current training schedule. Please use '\n",
        "                                  '`train_step` for training.')\n",
        "\n",
        "    def forward_train_d(self, data_batch, is_real, is_disc):\n",
        "        \"\"\"Forward function in discriminator training step.\n",
        "\n",
        "        In this function, we compute the prediction for each data batch (real\n",
        "        or fake). Meanwhile, the standard gan loss will be computed with\n",
        "        several proposed losses fro stable training.\n",
        "\n",
        "        Args:\n",
        "            data (torch.Tensor): Batch of real data or fake data.\n",
        "            is_real (bool): If True, the gan loss will regard this batch as\n",
        "                real data. Otherwise, the gan loss will regard this batch as\n",
        "                fake data.\n",
        "            is_disc (bool): If True, this function is called in discriminator\n",
        "                training step. Otherwise, this function is called in generator\n",
        "                training step. This will help us to compute different types of\n",
        "                adversarial loss, like LSGAN.\n",
        "\n",
        "        Returns:\n",
        "            dict: Contains the loss items computed in this function.\n",
        "        \"\"\"\n",
        "        pred = self.disc(data_batch)\n",
        "        ##############################################################\n",
        "        #loss_ = self.loss_gan(pred, is_real, is_disc)\n",
        "        #loss_ = (DiffAugment(pred, policy=policy)) #DiffAug\n",
        "\n",
        "        #alternativ:\n",
        "        pred = (DiffAugment(pred, policy=policy))\n",
        "        loss_ = self.loss_gan(pred, is_real, is_disc)\n",
        "\n",
        "        loss = dict(real_loss=loss_) if is_real else dict(fake_loss=loss_)\n",
        "        ##############################################################\n",
        "        if self.with_disc_shift_loss:\n",
        "            loss_d_shift = self.loss_disc_shift(loss_)\n",
        "            # 0.5 for average the fake and real data\n",
        "            loss.update(loss_disc_shift=loss_d_shift * 0.5)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def generator_loss(self, fake_res, fake_img, data_batch):\n",
        "        \"\"\"Forward function in generator training step.\n",
        "\n",
        "        In this function, we mainly compute the loss items for generator with\n",
        "        the given (fake_res, fake_img). In general, the `fake_res` is the\n",
        "        direct output of the generator and the `fake_img` is the composition of\n",
        "        direct output and ground-truth image.\n",
        "\n",
        "        Args:\n",
        "            fake_res (torch.Tensor): Direct output of the generator.\n",
        "            fake_img (torch.Tensor): Composition of `fake_res` and\n",
        "                ground-truth image.\n",
        "            data_batch (dict): Contain other elements for computing losses.\n",
        "\n",
        "        Returns:\n",
        "            tuple(dict): Dict contains the results computed within this \\\n",
        "                function for visualization and dict contains the loss items \\\n",
        "                computed in this function.\n",
        "        \"\"\"\n",
        "        gt = data_batch['gt_img']\n",
        "        mask = data_batch['mask']\n",
        "        masked_img = data_batch['masked_img']\n",
        "\n",
        "        loss = dict()\n",
        "\n",
        "        if self.with_gan:\n",
        "            g_fake_pred = self.disc(fake_img)\n",
        "            loss_g_fake = self.loss_gan(g_fake_pred, True, is_disc=False)\n",
        "            loss['loss_g_fake'] = loss_g_fake\n",
        "\n",
        "        if self.with_l1_hole_loss:\n",
        "            loss_l1_hole = self.loss_l1_hole(fake_res, gt, weight=mask)\n",
        "            loss['loss_l1_hole'] = loss_l1_hole\n",
        "\n",
        "        if self.with_l1_valid_loss:\n",
        "            loss_loss_l1_valid = self.loss_l1_valid(\n",
        "                fake_res, gt, weight=1. - mask)\n",
        "            loss['loss_l1_valid'] = loss_loss_l1_valid\n",
        "\n",
        "        if self.with_composed_percep_loss:\n",
        "            loss_pecep, loss_style = self.loss_percep(fake_img, gt)\n",
        "            if loss_pecep is not None:\n",
        "                loss['loss_composed_percep'] = loss_pecep\n",
        "            if loss_style is not None:\n",
        "                loss['loss_composed_style'] = loss_style\n",
        "\n",
        "        if self.with_out_percep_loss:\n",
        "            loss_out_percep, loss_out_style = self.loss_percep(fake_res, gt)\n",
        "            if loss_out_percep is not None:\n",
        "                loss['loss_out_percep'] = loss_out_percep\n",
        "            if loss_out_style is not None:\n",
        "                loss['loss_out_style'] = loss_out_style\n",
        "\n",
        "        if self.with_tv_loss:\n",
        "            loss_tv = self.loss_tv(fake_img, mask=mask)\n",
        "            loss['loss_tv'] = loss_tv\n",
        "\n",
        "        res = dict(\n",
        "            gt_img=gt.cpu(),\n",
        "            masked_img=masked_img.cpu(),\n",
        "            fake_res=fake_res.cpu(),\n",
        "            fake_img=fake_img.cpu())\n",
        "\n",
        "        return res, loss\n",
        "\n",
        "    def forward_test(self,\n",
        "                     masked_img,\n",
        "                     mask,\n",
        "                     save_image=False,\n",
        "                     save_path=None,\n",
        "                     iteration=None,\n",
        "                     **kwargs):\n",
        "        \"\"\"Forward function for testing.\n",
        "\n",
        "        Args:\n",
        "            masked_img (torch.Tensor): Tensor with shape of (n, 3, h, w).\n",
        "            mask (torch.Tensor): Tensor with shape of (n, 1, h, w).\n",
        "            save_image (bool, optional): If True, results will be saved as\n",
        "                image. Defaults to False.\n",
        "            save_path (str, optional): If given a valid str, the reuslts will\n",
        "                be saved in this path. Defaults to None.\n",
        "            iteration (int, optional): Iteration number. Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            dict: Contain output results and eval metrics (if have).\n",
        "        \"\"\"\n",
        "        input_x = torch.cat([masked_img, mask], dim=1)\n",
        "        fake_res = self.generator(input_x)\n",
        "        fake_img = fake_res * mask + masked_img * (1. - mask)\n",
        "\n",
        "        output = dict()\n",
        "        eval_results = {}\n",
        "        if self.eval_with_metrics:\n",
        "            gt_img = kwargs['gt_img']\n",
        "            data_dict = dict(gt_img=gt_img, fake_res=fake_res, mask=mask)\n",
        "            for metric_name in self.test_cfg['metrics']:\n",
        "                if metric_name in ['ssim', 'psnr']:\n",
        "                    eval_results[metric_name] = self._eval_metrics[\n",
        "                        metric_name](tensor2img(fake_img, min_max=(-1, 1)),\n",
        "                                     tensor2img(gt_img, min_max=(-1, 1)))\n",
        "                else:\n",
        "                    eval_results[metric_name] = self._eval_metrics[\n",
        "                        metric_name]()(data_dict).item()\n",
        "            output['eval_results'] = eval_results\n",
        "        else:\n",
        "            output['fake_res'] = fake_res\n",
        "            output['fake_img'] = fake_img\n",
        "\n",
        "        output['meta'] = None if 'meta' not in kwargs else kwargs['meta'][0]\n",
        "\n",
        "        if save_image:\n",
        "            assert save_image and save_path is not None, (\n",
        "                'Save path should been given')\n",
        "            assert output['meta'] is not None, (\n",
        "                'Meta information should be given to save image.')\n",
        "\n",
        "            tmp_filename = output['meta']['gt_img_path']\n",
        "            filestem = Path(tmp_filename).stem\n",
        "            if iteration is not None:\n",
        "                filename = f'{filestem}_{iteration}.png'\n",
        "            else:\n",
        "                filename = f'{filestem}.png'\n",
        "            mmcv.mkdir_or_exist(save_path)\n",
        "            img_list = [kwargs['gt_img']] if 'gt_img' in kwargs else []\n",
        "            img_list.extend(\n",
        "                [masked_img,\n",
        "                 mask.expand_as(masked_img), fake_res, fake_img])\n",
        "            img = torch.cat(img_list, dim=3).cpu()\n",
        "            self.save_visualization(img, osp.join(save_path, filename))\n",
        "            output['save_img_path'] = osp.abspath(\n",
        "                osp.join(save_path, filename))\n",
        "\n",
        "        return output\n",
        "\n",
        "    def save_visualization(self, img, filename):\n",
        "        \"\"\"Save visualization results.\n",
        "\n",
        "        Args:\n",
        "            img (torch.Tensor): Tensor with shape of (n, 3, h, w).\n",
        "            filename (str): Path to save visualization.\n",
        "        \"\"\"\n",
        "        if self.test_cfg.get('img_rerange', True):\n",
        "            img = (img + 1) / 2\n",
        "        if self.test_cfg.get('img_bgr2rgb', True):\n",
        "            img = img[:, [2, 1, 0], ...]\n",
        "        save_image(img, filename, nrow=1, padding=0)\n",
        "\n",
        "    def train_step(self, data_batch, optimizer):\n",
        "        \"\"\"Train step function.\n",
        "\n",
        "        In this function, the inpaintor will finish the train step following\n",
        "        the pipeline:\n",
        "\n",
        "            1. get fake res/image\n",
        "            2. optimize discriminator (if have)\n",
        "            3. optimize generator\n",
        "\n",
        "        If `self.train_cfg.disc_step > 1`, the train step will contain multiple\n",
        "        iterations for optimizing discriminator with different input data and\n",
        "        only one iteration for optimizing gerator after `disc_step` iterations\n",
        "        for discriminator.\n",
        "\n",
        "        Args:\n",
        "            data_batch (torch.Tensor): Batch of data as input.\n",
        "            optimizer (dict[torch.optim.Optimizer]): Dict with optimizers for\n",
        "                generator and discriminator (if have).\n",
        "\n",
        "        Returns:\n",
        "            dict: Dict with loss, information for logger, the number of \\\n",
        "                samples and results for visualization.\n",
        "        \"\"\"\n",
        "        log_vars = {}\n",
        "\n",
        "        gt_img = data_batch['gt_img']\n",
        "        mask = data_batch['mask']\n",
        "        masked_img = data_batch['masked_img']\n",
        "\n",
        "        # get common output from encdec\n",
        "        input_x = torch.cat([masked_img, mask], dim=1)\n",
        "        fake_res = self.generator(input_x)\n",
        "        fake_img = gt_img * (1. - mask) + fake_res * mask\n",
        "\n",
        "        # discriminator training step\n",
        "        if self.train_cfg.disc_step > 0:\n",
        "            set_requires_grad(self.disc, True)\n",
        "            disc_losses = self.forward_train_d(\n",
        "                fake_img.detach(), False, is_disc=True)\n",
        "            loss_disc, log_vars_d = self.parse_losses(disc_losses)\n",
        "            log_vars.update(log_vars_d)\n",
        "            optimizer['disc'].zero_grad()\n",
        "            loss_disc.backward()\n",
        "\n",
        "            disc_losses = self.forward_train_d(gt_img, True, is_disc=True)\n",
        "            loss_disc, log_vars_d = self.parse_losses(disc_losses)\n",
        "            log_vars.update(log_vars_d)\n",
        "            loss_disc.backward()\n",
        "\n",
        "            if self.with_gp_loss:\n",
        "                loss_d_gp = self.loss_gp(\n",
        "                    self.disc, gt_img, fake_img, mask=mask)\n",
        "                loss_disc, log_vars_d = self.parse_losses(\n",
        "                    dict(loss_gp=loss_d_gp))\n",
        "                log_vars.update(log_vars_d)\n",
        "                loss_disc.backward()\n",
        "\n",
        "            optimizer['disc'].step()\n",
        "\n",
        "            self.disc_step_count = (self.disc_step_count +\n",
        "                                    1) % self.train_cfg.disc_step\n",
        "            if self.disc_step_count != 0:\n",
        "                # results contain the data for visualization\n",
        "                results = dict(\n",
        "                    gt_img=gt_img.cpu(),\n",
        "                    masked_img=masked_img.cpu(),\n",
        "                    fake_res=fake_res.cpu(),\n",
        "                    fake_img=fake_img.cpu())\n",
        "                outputs = dict(\n",
        "                    log_vars=log_vars,\n",
        "                    num_samples=len(data_batch['gt_img'].data),\n",
        "                    results=results)\n",
        "\n",
        "                return outputs\n",
        "\n",
        "        # generator (encdec) training step, results contain the data\n",
        "        # for visualization\n",
        "        if self.with_gan:\n",
        "            set_requires_grad(self.disc, False)\n",
        "        results, g_losses = self.generator_loss(fake_res, fake_img, data_batch)\n",
        "        loss_g, log_vars_g = self.parse_losses(g_losses)\n",
        "        log_vars.update(log_vars_g)\n",
        "        optimizer['generator'].zero_grad()\n",
        "        loss_g.backward()\n",
        "        optimizer['generator'].step()\n",
        "\n",
        "        outputs = dict(\n",
        "            log_vars=log_vars,\n",
        "            num_samples=len(data_batch['gt_img'].data),\n",
        "            results=results)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def val_step(self, data_batch, **kwargs):\n",
        "        \"\"\"Forward function for evaluation.\n",
        "\n",
        "        Args:\n",
        "            data_batch (dict): Contain data for forward.\n",
        "\n",
        "        Returns:\n",
        "            dict: Contain the results from model.\n",
        "        \"\"\"\n",
        "        output = self.forward_test(**data_batch, **kwargs)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def forward_dummy(self, x):\n",
        "        \"\"\"Forward dummy function for getting flops.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor with shape of (n, c, h, w).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Results tensor with shape of (n, 3, h, w).\n",
        "        \"\"\"\n",
        "        res = self.generator(x)\n",
        "\n",
        "        return res\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWkTD21NTJKX",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title two_stage.py (adding differentiable augmentation for deepfillv2)\n",
        "%%writefile /content/Colab-mmediting/mmedit/models/inpaintors/two_stage.py\n",
        "#two_stage.py\n",
        "\n",
        "# Differentiable Augmentation for Data-Efficient GAN Training\n",
        "# Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han\n",
        "# https://arxiv.org/pdf/2006.10738\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "policy = 'color,translation,cutout'\n",
        "\n",
        "def DiffAugment(x, policy='', channels_first=True):\n",
        "    if policy:\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 3, 1, 2)\n",
        "        for p in policy.split(','):\n",
        "            for f in AUGMENT_FNS[p]:\n",
        "                x = f(x)\n",
        "        if not channels_first:\n",
        "            x = x.permute(0, 2, 3, 1)\n",
        "        x = x.contiguous()\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_brightness(x):\n",
        "    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_saturation(x):\n",
        "    x_mean = x.mean(dim=1, keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_contrast(x):\n",
        "    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n",
        "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_translation(x, ratio=0.125):\n",
        "    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
        "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
        "    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
        "    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_cutout(x, ratio=0.5):\n",
        "    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
        "    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
        "    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
        "    mask[grid_batch, grid_x, grid_y] = 0\n",
        "    x = x * mask.unsqueeze(1)\n",
        "    return x\n",
        "\n",
        "\n",
        "AUGMENT_FNS = {\n",
        "    'color': [rand_brightness, rand_saturation, rand_contrast],\n",
        "    'translation': [rand_translation],\n",
        "    'cutout': [rand_cutout],\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import os.path as osp\n",
        "from pathlib import Path\n",
        "\n",
        "import mmcv\n",
        "import torch\n",
        "from mmedit.core import tensor2img\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from ..common.model_utils import set_requires_grad\n",
        "from ..registry import MODELS\n",
        "from .one_stage import OneStageInpaintor\n",
        "\n",
        "#from DiffAugment_pytorch import DiffAugment\n",
        "\n",
        "@MODELS.register_module()\n",
        "class TwoStageInpaintor(OneStageInpaintor):\n",
        "    \"\"\"Two-Stage Inpaintor.\n",
        "\n",
        "    Currently, we support these loss types in each of two stage inpaintors:\n",
        "    ['loss_gan', 'loss_l1_hole', 'loss_l1_valid', 'loss_composed_percep',\\\n",
        "     'loss_out_percep', 'loss_tv']\n",
        "    The `stage1_loss_type` and `stage2_loss_type` should be chosen from these\n",
        "    loss types.\n",
        "\n",
        "    Args:\n",
        "        stage1_loss_type (tuple[str]): Contains the loss names used in the\n",
        "            first stage model.\n",
        "        stage2_loss_type (tuple[str]): Contains the loss names used in the\n",
        "            second stage model.\n",
        "        input_with_ones (bool): Whether to concatenate an extra ones tensor in\n",
        "            input. Default: True.\n",
        "        disc_input_with_mask (bool): Whether to add mask as input in\n",
        "            discriminator. Default: False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 *args,\n",
        "                 stage1_loss_type=('loss_l1_hole', ),\n",
        "                 stage2_loss_type=('loss_l1_hole', 'loss_gan'),\n",
        "                 input_with_ones=True,\n",
        "                 disc_input_with_mask=False,\n",
        "                 **kwargs):\n",
        "        super(TwoStageInpaintor, self).__init__(*args, **kwargs)\n",
        "\n",
        "        self.stage1_loss_type = stage1_loss_type\n",
        "        self.stage2_loss_type = stage2_loss_type\n",
        "        self.input_with_ones = input_with_ones\n",
        "        self.disc_input_with_mask = disc_input_with_mask\n",
        "        self.eval_with_metrics = ('metrics' in self.test_cfg) and (\n",
        "            self.test_cfg['metrics'] is not None)\n",
        "\n",
        "    def forward_test(self,\n",
        "                     masked_img,\n",
        "                     mask,\n",
        "                     save_image=False,\n",
        "                     save_path=None,\n",
        "                     iteration=None,\n",
        "                     **kwargs):\n",
        "        \"\"\"Forward function for testing.\n",
        "\n",
        "        Args:\n",
        "            masked_img (torch.Tensor): Tensor with shape of (n, 3, h, w).\n",
        "            mask (torch.Tensor): Tensor with shape of (n, 1, h, w).\n",
        "            save_image (bool, optional): If True, results will be saved as\n",
        "                image. Defaults to False.\n",
        "            save_path (str, optional): If given a valid str, the reuslts will\n",
        "                be saved in this path. Defaults to None.\n",
        "            iteration (int, optional): Iteration number. Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            dict: Contain output results and eval metrics (if have).\n",
        "        \"\"\"\n",
        "        if self.input_with_ones:\n",
        "            tmp_ones = torch.ones_like(mask)\n",
        "            input_x = torch.cat([masked_img, tmp_ones, mask], dim=1)\n",
        "        else:\n",
        "            input_x = torch.cat([masked_img, mask], dim=1)\n",
        "        stage1_fake_res, stage2_fake_res = self.generator(input_x)\n",
        "        fake_img = stage2_fake_res * mask + masked_img * (1. - mask)\n",
        "        output = dict()\n",
        "        eval_results = {}\n",
        "        if self.eval_with_metrics:\n",
        "            gt_img = kwargs['gt_img']\n",
        "            data_dict = dict(\n",
        "                gt_img=gt_img, fake_res=stage2_fake_res, mask=mask)\n",
        "            for metric_name in self.test_cfg['metrics']:\n",
        "                if metric_name in ['ssim', 'psnr']:\n",
        "                    eval_results[metric_name] = self._eval_metrics[\n",
        "                        metric_name](tensor2img(fake_img, min_max=(-1, 1)),\n",
        "                                     tensor2img(gt_img, min_max=(-1, 1)))\n",
        "                else:\n",
        "                    eval_results[metric_name] = self._eval_metrics[\n",
        "                        metric_name]()(data_dict).item()\n",
        "            output['eval_results'] = eval_results\n",
        "        else:\n",
        "            output['stage1_fake_res'] = stage1_fake_res\n",
        "            output['stage2_fake_res'] = stage2_fake_res\n",
        "            output['fake_res'] = stage2_fake_res\n",
        "            output['fake_img'] = fake_img\n",
        "\n",
        "        output['meta'] = None if 'meta' not in kwargs else kwargs['meta'][0]\n",
        "\n",
        "        if save_image:\n",
        "            assert save_image and save_path is not None, (\n",
        "                'Save path should be given')\n",
        "            assert output['meta'] is not None, (\n",
        "                'Meta information should be given to save image.')\n",
        "\n",
        "            tmp_filename = output['meta']['gt_img_path']\n",
        "            filestem = Path(tmp_filename).stem\n",
        "            if iteration is not None:\n",
        "                filename = f'{filestem}_{iteration}.png'\n",
        "            else:\n",
        "                filename = f'{filestem}.png'\n",
        "            mmcv.mkdir_or_exist(save_path)\n",
        "            img_list = [kwargs['gt_img']] if 'gt_img' in kwargs else []\n",
        "            img_list.extend([\n",
        "                masked_img,\n",
        "                mask.expand_as(masked_img), stage1_fake_res, stage2_fake_res,\n",
        "                fake_img\n",
        "            ])\n",
        "            img = torch.cat(img_list, dim=3).cpu()\n",
        "            self.save_visualization(img, osp.join(save_path, filename))\n",
        "            output['save_img_path'] = osp.abspath(\n",
        "                osp.join(save_path, filename))\n",
        "\n",
        "        return output\n",
        "\n",
        "    def save_visualization(self, img, filename):\n",
        "        \"\"\"Save visualization results.\n",
        "\n",
        "        Args:\n",
        "            img (torch.Tensor): Tensor with shape of (n, 3, h, w).\n",
        "            filename (str): Path to save visualization.\n",
        "        \"\"\"\n",
        "        if self.test_cfg.get('img_rerange', True):\n",
        "            img = (img + 1) / 2\n",
        "        if self.test_cfg.get('img_bgr2rgb', True):\n",
        "            img = img[:, [2, 1, 0], ...]\n",
        "        save_image(img, filename, nrow=1, padding=0)\n",
        "\n",
        "    def two_stage_loss(self, stage1_data, stage2_data, data_batch):\n",
        "        \"\"\"Calculate two-stage loss.\n",
        "\n",
        "        Args:\n",
        "            stage1_data (dict): Contain stage1 results.\n",
        "            stage2_data (dict): Contain stage2 results.\n",
        "            data_batch (dict): Contain data needed to calculate loss.\n",
        "\n",
        "        Returns:\n",
        "            dict: Contain losses with name.\n",
        "        \"\"\"\n",
        "        gt = data_batch['gt_img']\n",
        "        mask = data_batch['mask']\n",
        "        masked_img = data_batch['masked_img']\n",
        "\n",
        "        loss = dict()\n",
        "        results = dict(\n",
        "            gt_img=gt.cpu(), mask=mask.cpu(), masked_img=masked_img.cpu())\n",
        "        # calculate losses for stage1\n",
        "        if self.stage1_loss_type is not None:\n",
        "            fake_res = stage1_data['fake_res']\n",
        "            fake_img = stage1_data['fake_img']\n",
        "            for type_key in self.stage1_loss_type:\n",
        "                tmp_loss = self.calculate_loss_with_type(\n",
        "                    type_key, fake_res, fake_img, gt, mask, prefix='stage1_')\n",
        "                loss.update(tmp_loss)\n",
        "\n",
        "        results.update(\n",
        "            dict(\n",
        "                stage1_fake_res=stage1_data['fake_res'].cpu(),\n",
        "                stage1_fake_img=stage1_data['fake_img'].cpu()))\n",
        "\n",
        "        if self.stage2_loss_type is not None:\n",
        "            fake_res = stage2_data['fake_res']\n",
        "            fake_img = stage2_data['fake_img']\n",
        "            for type_key in self.stage2_loss_type:\n",
        "                tmp_loss = self.calculate_loss_with_type(\n",
        "                    type_key, fake_res, fake_img, gt, mask, prefix='stage2_')\n",
        "                loss.update(tmp_loss)\n",
        "        results.update(\n",
        "            dict(\n",
        "                stage2_fake_res=stage2_data['fake_res'].cpu(),\n",
        "                stage2_fake_img=stage2_data['fake_img'].cpu()))\n",
        "\n",
        "        return results, loss\n",
        "\n",
        "    def calculate_loss_with_type(self,\n",
        "                                 loss_type,\n",
        "                                 fake_res,\n",
        "                                 fake_img,\n",
        "                                 gt,\n",
        "                                 mask,\n",
        "                                 prefix='stage1_'):\n",
        "        \"\"\"Calculate multiple types of losses.\n",
        "\n",
        "        Args:\n",
        "            loss_type (str): Type of the loss.\n",
        "            fake_res (torch.Tensor): Direct results from model.\n",
        "            fake_img (torch.Tensor): Composited results from model.\n",
        "            gt (torch.Tensor): Ground-truth tensor.\n",
        "            mask (torch.Tensor): Mask tensor.\n",
        "            prefix (str, optional): Prefix for loss name.\n",
        "                Defaults to 'stage1_'.\n",
        "\n",
        "        Returns:\n",
        "            dict: Contain loss value with its name.\n",
        "        \"\"\"\n",
        "        loss_dict = dict()\n",
        "        if loss_type == 'loss_gan':\n",
        "            if self.disc_input_with_mask:\n",
        "                disc_input_x = torch.cat([fake_img, mask], dim=1)\n",
        "            else:\n",
        "                disc_input_x = fake_img\n",
        "            g_fake_pred = self.disc(disc_input_x)\n",
        "            #############################################################\n",
        "            #loss_g_fake = self.loss_gan(g_fake_pred, True, is_disc=False)\n",
        "            #loss_g_fake = (DiffAugment(g_fake_pred, policy=policy)) #DiffAug\n",
        "\n",
        "            #alternativ:\n",
        "            g_fake_pred = DiffAugment(g_fake_pred, policy=policy)\n",
        "            loss_g_fake = self.loss_gan(g_fake_pred, True, is_disc=False)\n",
        "            ##############################################################\n",
        "            loss_dict[prefix + 'loss_g_fake'] = loss_g_fake\n",
        "        elif 'percep' in loss_type:\n",
        "            loss_pecep, loss_style = self.loss_percep(fake_img, gt)\n",
        "            if loss_pecep is not None:\n",
        "                loss_dict[prefix + loss_type] = loss_pecep\n",
        "            if loss_style is not None:\n",
        "                loss_dict[prefix + loss_type[:-6] + 'style'] = loss_style\n",
        "        elif 'tv' in loss_type:\n",
        "            loss_tv = self.loss_tv(fake_img, mask=mask)\n",
        "            loss_dict[prefix + loss_type] = loss_tv\n",
        "        elif 'l1' in loss_type:\n",
        "            weight = 1. - mask if 'valid' in loss_type else mask\n",
        "            loss_l1 = getattr(self, loss_type)(fake_res, gt, weight=weight)\n",
        "            loss_dict[prefix + loss_type] = loss_l1\n",
        "        else:\n",
        "            raise NotImplementedError(\n",
        "                f'Please check your loss type {loss_type}'\n",
        "                f' and the config dict in init function. '\n",
        "                f'We cannot find the related loss function.')\n",
        "\n",
        "        return loss_dict\n",
        "\n",
        "    def train_step(self, data_batch, optimizer):\n",
        "        \"\"\"Train step function.\n",
        "\n",
        "        In this function, the inpaintor will finish the train step following\n",
        "        the pipeline:\n",
        "\n",
        "            1. get fake res/image\n",
        "            2. optimize discriminator (if have)\n",
        "            3. optimize generator\n",
        "\n",
        "        If `self.train_cfg.disc_step > 1`, the train step will contain multiple\n",
        "        iterations for optimizing discriminator with different input data and\n",
        "        only one iteration for optimizing gerator after `disc_step` iterations\n",
        "        for discriminator.\n",
        "\n",
        "        Args:\n",
        "            data_batch (torch.Tensor): Batch of data as input.\n",
        "            optimizer (dict[torch.optim.Optimizer]): Dict with optimizers for\n",
        "                generator and discriminator (if have).\n",
        "\n",
        "        Returns:\n",
        "            dict: Dict with loss, information for logger, the number of \\\n",
        "                samples and results for visualization.\n",
        "        \"\"\"\n",
        "        log_vars = {}\n",
        "\n",
        "        gt_img = data_batch['gt_img']\n",
        "        mask = data_batch['mask']\n",
        "        masked_img = data_batch['masked_img']\n",
        "\n",
        "        # get common output from encdec\n",
        "        if self.input_with_ones:\n",
        "            tmp_ones = torch.ones_like(mask)\n",
        "            input_x = torch.cat([masked_img, tmp_ones, mask], dim=1)\n",
        "        else:\n",
        "            input_x = torch.cat([masked_img, mask], dim=1)\n",
        "        stage1_fake_res, stage2_fake_res = self.generator(input_x)\n",
        "        stage1_fake_img = masked_img * (1. - mask) + stage1_fake_res * mask\n",
        "        stage2_fake_img = masked_img * (1. - mask) + stage2_fake_res * mask\n",
        "\n",
        "        # discriminator training step\n",
        "        # In this version, we only use the results from the second stage to\n",
        "        # train discriminators, which is a commonly used setting. This can be\n",
        "        # easily modified to your custom training schedule.\n",
        "        if self.train_cfg.disc_step > 0:\n",
        "            set_requires_grad(self.disc, True)\n",
        "            if self.disc_input_with_mask:\n",
        "                disc_input_x = torch.cat([stage2_fake_img.detach(), mask],\n",
        "                                         dim=1)\n",
        "            else:\n",
        "                disc_input_x = stage2_fake_img.detach()\n",
        "            disc_losses = self.forward_train_d(disc_input_x, False, is_disc=True)\n",
        "            loss_disc, log_vars_d = self.parse_losses(disc_losses)\n",
        "            log_vars.update(log_vars_d)\n",
        "            optimizer['disc'].zero_grad()\n",
        "            loss_disc.backward()\n",
        "\n",
        "            if self.disc_input_with_mask:\n",
        "                disc_input_x = torch.cat([gt_img, mask], dim=1)\n",
        "            else:\n",
        "                disc_input_x = gt_img\n",
        "            disc_losses = self.forward_train_d(disc_input_x, True, is_disc=True)\n",
        "            loss_disc, log_vars_d = self.parse_losses(disc_losses)\n",
        "            log_vars.update(log_vars_d)\n",
        "            loss_disc.backward()\n",
        "\n",
        "            if self.with_gp_loss:\n",
        "                # gradient penalty loss should not be used with mask as input\n",
        "                assert not self.disc_input_with_mask\n",
        "                loss_d_gp = self.loss_gp(self.disc, gt_img, stage2_fake_img, mask=mask)\n",
        "                loss_disc, log_vars_d = self.parse_losses(dict(loss_gp=loss_d_gp))\n",
        "                log_vars.update(log_vars_d)\n",
        "                loss_disc.backward()\n",
        "\n",
        "            optimizer['disc'].step()\n",
        "\n",
        "            self.disc_step_count = (self.disc_step_count +\n",
        "                                    1) % self.train_cfg.disc_step\n",
        "            if self.disc_step_count != 0:\n",
        "                # results contain the data for visualization\n",
        "                results = dict(\n",
        "                    gt_img=gt_img.cpu(),\n",
        "                    masked_img=masked_img.cpu(),\n",
        "                    fake_res=stage2_fake_res.cpu(),\n",
        "                    fake_img=stage2_fake_img.cpu())\n",
        "                outputs = dict(\n",
        "                    log_vars=log_vars,\n",
        "                    num_samples=len(data_batch['gt_img'].data),\n",
        "                    results=results)\n",
        "\n",
        "                return outputs\n",
        "\n",
        "        # prepare stage1 results and stage2 results dict for calculating losses\n",
        "        stage1_results = dict(\n",
        "            fake_res=stage1_fake_res, fake_img=stage1_fake_img)\n",
        "        stage2_results = dict(\n",
        "            fake_res=stage2_fake_res, fake_img=stage2_fake_img)\n",
        "\n",
        "        # generator (encdec) and refiner training step, results contain the\n",
        "        # data for visualization\n",
        "        if self.with_gan:\n",
        "            set_requires_grad(self.disc, False)\n",
        "        results, two_stage_losses = self.two_stage_loss(stage1_results, stage2_results, data_batch)\n",
        "        loss_two_stage, log_vars_two_stage = self.parse_losses(two_stage_losses)\n",
        "        log_vars.update(log_vars_two_stage)\n",
        "        optimizer['generator'].zero_grad()\n",
        "        loss_two_stage.backward()\n",
        "        optimizer['generator'].step()\n",
        "\n",
        "        outputs = dict(\n",
        "            log_vars=log_vars,\n",
        "            num_samples=len(data_batch['gt_img'].data),\n",
        "            results=results)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# generator\n",
        "two_stage.py / train_step()\n",
        "v\n",
        "two_stage.py / two_stage_loss()\n",
        "v\n",
        "two_stage.py / calculate_loss_with_type()\n",
        "v\n",
        "two_stage.py / loss_g_fake = self.loss_gan(g_fake_pred, True, is_disc=False)\n",
        "\n",
        "# discriminator\n",
        "two_stage.py / train_step()\n",
        "v\n",
        "two_stage.py -> one_stage.py / forward_train_d()\n",
        "v\n",
        "one_stage.py / loss = dict(real_loss=loss_) if is_real else dict(fake_loss=loss_)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyYsI4ssRqp4",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Train\n",
        "%cd /content/Colab-mmediting\n",
        "!python tools/train.py configs/inpainting/deepfillv2/custom.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3rY96332zAA",
        "colab_type": "text"
      },
      "source": [
        "# Test trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3biQzN2PzwZP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python demo/inpainting_demo.py \"configs/inpainting/deepfillv2/custom.py\" \\\n",
        "\"/content/mmediting/work_dirs/test_pggan/iter_XXXXX.pth\" /content/image.png /content/mask.png /content/result.png"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
